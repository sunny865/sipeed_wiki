{"/news/maixhub/new_maixhub.html": {"title": "新版 MaixHub 正式上线啦！", "content": "---\ntitle: 新版 MaixHub 正式上线啦！\nkeywords: 在线训练, 模型平台, 单片机运行模型, 云端训练, 模型库, model zoo\ndate: 2022-09-04\ntags: maixhub, 模型训练\ncover: ./images/deploy.gif\n---\n\n\n\n先看看效果， 一个检测小鸭子（可同时检测多个相同或者不同目标），另外一个是表情分类模型：\n\n![deploy](./images/deploy.gif)\n\n![face_emotion](./images/face_emotion.gif)\n\n<!-- more -->\n\n> 原文： https://neucrack.com/p/444 , 持续更新\n> 视频： [基于K210的情绪识别与可视化](https://www.bilibili.com/video/BV1Xe4y1D7ne?spm_id_from=333.337.search-card.all.click&vd_source=6c974e13f53439d17d6a092a499df304)\n\n## 新版 [MaixHub](https://maixhub.com) 简介\n\n[MaixHub](https://maixhub.com) 推出了新版！支持了更多设备，包括 K210, V831, NCNN, TFjs, TinyMaix(所有单片机上运行)！更新了一大堆功能：\n* 多种上传数据集方式：\n![上传数据集](./images/1.jpg)\n  * 本地文件上传\n  * 设备直接拍照后一键上传\n  ![采集](./images/采集.jpg)\n  * 导入本地标注\n* 支持在线标注，简单快速好用，以及支持视频自动标注哦\n  ![在线标注](./images/在线标注.jpg)\n* 多种训练参数自定义支持\n![参数](./images/参数.jpg)\n* 多种平台（硬件平台）\n  * 手机、电脑\n  ![浏览器](./images/浏览器.jpg)\n  * 带硬件NPU加速的高性价比板子，比如[Maix-II-Dock](https://wiki.sipeed.com/hardware/zh/maixII/M2/resources.html)\n  ![m1m2](./images/m1m2.jpg)\n  * 通用单片机，更是推出了超轻量的单片上运行的模型推理库[TinyMaix](https://github.com/sipeed/TinyMaix)，可以移植到任意单片机，甚至能在 2KiB RAM的arduino上跑\n  ![mcu](./images/mcu.jpg)\n* 训练过程、日志、图标展示，以及验证结果展示，方便分析和优化模型\n![maixhub](./images/maixhub.jpg)\n* 一键部署到设备\n![部署](./images/部署.jpg)\n* 一键分享到模型库（模型平台/model zoo），同时模型库也支持手动上传分享更厉害好玩的模型哦\n![模型库](./images/模型库.jpg)\n\n## 训练一个模型试试\n\n注意 MaixHub 在不停更新，可能有些地方在未来会有些许变化，原理变化不大，举一反三即可\n\n* 创建训练项目，选择检测任务，检测任务能框出物体的位置，后面需要我们标注数据，分类任务则不需要标注框，用来区分多种物体\n![创建训练项目](./images/创建训练项目.jpg)\n![创建项目](./images/创建项目.jpg)\n\n* 创建数据集\n![数据集](./images/数据集.jpg)\n\n* 采集数据，可以从本地上传，也可以用手机在线拍照采集，也可以用设备拍照采集（比如 Maix-II-DocK）\n![采集数据](./images/采集数据.jpg)\n![采集](./images/采集.jpg)\n\n* 标注数据，就是将要检测的物体框起来，并且给一个标签，比如这里只有一个duck；一张图里面可以有多个框，也可以只有一个框，最后训练出来的模型都能识别同一个画面的多个物体，数据量尽量多一点，覆盖的场景全面一点，这样实际到板子上跑起来准确率更高，最好使用什么板子跑就用什么板子采集数据，准确率会高些\n![collect](./images/collect.gif)\n这里有训练集，验证集，测试集的概念，训练集就是拿来参与训练的数据，验证集不参与训练，但是每隔一段训练时间后会将模型在验证集上面跑一次，得到 `val_acc`也就是验证集精确度，即这个模型在这个从来没参与过训练的数据上正确率如何，一般用这个指标来评判模型的好坏；测试集则是完全没在训练过程中参与的数据，比如部署到板子上后实际识别的物体就算是测试集。所以为了让实际在板子上跑的时候准确度高，尽量将验证集的数据和实际应用的场景（测试集）相近，这样训练时在验证集上的准确率（val_acc）才能更精确地反应在测试集上的效果。\n> 比如你实际识别的是哈士奇狗狗，训练集中放了各种狗狗图，验证集却放了中华田园犬，就算 val_acc 为 1，即全部识别正确，实际拿去识别哈士奇狗狗的时候可能完全无法识别\n\n* 创建训练任务，一个项目里面可以多次创建任务，调整不同的参数。\n注意要根据自己的设备选择对应的平台，比如 K210 选择 nncase， 普通没有 NPU 加速的单片机选择 TinyMaix，手机或者浏览器跑则选择 tfjs。分辨率使用默认的 224x224 效果最好，因为迁移训练就是基于一个已经训练过的模型微调训练模型，这个模型默认都是 224x224 下训练的，所以理论上效果最好。\n![参数](./images/参数.jpg)\n另外，选择模型主干网络，网络越大效果越好（精确度越高），但是占用的内存就越大，以及运行消耗的时间就越久，也就是对算力的要求越大，根据你的数据集复杂度以及设备性能选择合适的主干网络（backbone）。\n还有更多参数，可以看页面对应的文档说明即可，也可以到首页的交流群交流。\n\n* 启动训练，训练任务多的时候可能需要排队，耐心等待就好了，如果加急可以联系官方或者管理员钞能力解决哈哈哈。\n训练过程可以看到 `val_acc`， 即损失和精确度曲线，会慢慢上升，代表在验证集训练完成后可以看到\n![训练结果](./images/训练结果.jpg)\n如前面所说，这里最重要的是 `val_acc`，即在验证集上的准确度，越接近 `1`越好（实际上这里 val_acc 是 MAP，MAP相关含义可自行搜索）。以及展示了部分识别正确的和识别错误（或者未识别到）的样例。\n\n* 一键部署\n根据不同设备支持的方式可能不同，可能是直接下载文件（具体使用方法看对应说明），也有（比如 Maix-II-Dock）支持一键部署，在设备上选择模型部署，扫描部署页面的二维码即可自动部署，然后就可以进行识别了。\n需要注意的是一键部署需要设备已经联接互联网，设备选择联网功能，到 [maixhub.com/wifi](https://maixhub.com/wifi) 生成填写 WiFi 信息然后设备扫码连接 WiFi， 注意不是局域网，需要能访问互联网。\n![deploy](./images/deploy.gif)\n\n* 分享模型到模型库\n训练的模型可以一键分享到模型库，如果你训练的模型实际在板子上运行的识别效果不错，欢迎点击左边导航栏分享模型分享到模型库，可以多写点图文介绍，推荐在 B 站上传一个视频后嵌入到模型描述里面~\n\n更多功能持续在更新~ 持续关注哦~ 也欢迎来上传分享你自己的模型！\n\n\n## 训练优化建议\n\n* 尽量多采集实际使用场景的图片，覆盖更多使用场景有利于提高最终识别率。\n* 图片数量尽量不要太小，虽然平台限制最小数量为 20 张图才可以训练， 但要打到比较好的效果，显然一个分类 200 张都不算多，不要一直在 30 张训练图片上纠结为什么训练效果不好。。。\n* 默认分辨率但是 224x224， 是因为预训练模型是在 224x224 下训练的，当然也有其它分辨率的，比如 128x128，具体发现不支持的分辨率预训练模型，在训练日志中会打印警告信息。\n* 为了让验证集的精确度的可信度更高（也就是在实际开发板上跑的精确度更接近训练时在验证集上的精确度），验证集的数据和实际应用的场景数据一致。比如训练集是在网上找了很多图片，那这些图片可能和实际开发板的摄像头拍出来的图有差距，可以往验证集上传一些实际设备拍的图来验证训练的模型效果。\n这样我们就能在训练的时候根据验证集精确度（val_acc）来判断模型训练效果如何了，如果发现验证集精确度很低，那么就可以考虑增加训练集复杂度和数量，或者训练集用设备拍摄来训练。\n* 对于检测训练项目，如果检测训练的物体很准，但是容易误识别到其它物体，可以在数据里面拍点其它的物体当背景；或者拍摄一些没有目标的图片，不添加任何标注也可以，然后在训练的时候勾选“允许负样本”来使能没有标注的图片。\n* 检测任务可以同时检测到多个目标，如果你觉得识别类别不准，也有另外一种方式，先只检测模型检测到物体（一个类别），然后裁切出图片中的目标物体上传到分类任务，用分类任务来分辨类别。不过这样就要跑两个模型，需要写代码裁切图片（在板子跑就好了），以及需要考虑内存是否足够"}, "/news/others/20k_lite_start/20k_lite_start.html": {"title": "Primer 20K Lite 初见", "content": "---\ntitle: Primer 20K Lite 初见\nkeywords: Primer 20K, Lite, FPGA\ndesc: Primer 20K 上手\ndate: 2022-08-22\ntags: FPGA, Primer 20K\ncover: ./assets/cover.png\n---\n\n拿到 Primer 20K Lite 后上手点个灯\n\n<!-- more -->\n\n## 前言\n\n本篇文档引导新用户熟悉 IDE 流程并且完成点灯操作。\n\n出货固件已经默认为用户可用全 IO 闪灯，因此长期通电会发热，介意的话可以先擦除 Flash 或者根据本文走到最后烧录时将电脑与板子连接起来。\n\n## 安装 IDE\n\n参考 [安装IDE](https://wiki.sipeed.com/hardware/zh/tang/Tang-Nano-Doc/get_started/install-the-ide.html) 来完成我们需要准备的软件环境。\n\n对于 Windows 用户需要额外下载一下 [Programmer](https://dl.sipeed.com/shareURL/TANG/programmer) 烧录专用软件可以降低我们在烧录的时候出现问题的可能性。\n\n对于 Linux 用户的话建议使用 [openfpgaLoader](https://wiki.sipeed.com/hardware/zh/tang/Tang-Nano-Doc/get_started/flash_in_linux.html) 这软件来烧录 GW2A-18。\n\n## 新建工程\n\n文件 -> 新建 -> FPGA Design -> Project\n\n<div>\n    <img src=\"./assets/new_project.png\" width=58% alt=\"new_project\">\n    <img src=\"./assets/fpga_project.png\" width=35% alt=\"fpga_project\">\n</div>\n\n设置工程名称，要求只用英文的下划线命名，存放路径中不要有中文字符或者空格等。\n\n![project_path](./assets/project_path.png)\n\n然后在下面的芯片型号中选择 GW2A-LV18PG256C8/I7，使用上面的筛选能够更快地选择到正确的型号，注意 Device 那一栏为 GW2A-18C\n![device_choose](./assets/device_choose.png)\n\n然后点击确定后就可以进行最终项目预览了。确认无误后就完成工程创建了。\n\n## 编写代码\n\n### 新建文件\n\n高云半导体 IDE 提供了三种新建文件的方法。在此我们直接使用快捷键 `Ctrl + N` 来新建文件，其他两种不在此讲述。\n\n在弹出的窗口中选择 `Verilog File`，会 VHDL 的也可以选择下面的 `VHDL File`，这篇文章只用 Verilog 来做点灯示例。\n\n<img src=\"./assets/new_verilog_file.png\" width=50% alt=\"new_verilog_file\">\n    \n点击 OK 之后会提示让我们输入文件名称，此处以 `led` 为文件名做示范。\n\n<img src=\"./assets/file_name.png\" width=75% alt=\"file_name\">\n\n到这里我们就完成文件的创建了，可以直接编写代码了。\n\n![created_file](./assets/created_file.png)\n\n### Verilog 简单说明\n\nVerilog 是一种硬件描述语言，用来对数字电路进行抽象化描述。\n\nVerilog 的基本设计单元是“模块”(module)。\n\n一个模块是由两部分组成的：一部分描述接口，另一部分描述内部逻辑功能，即定义输入是如何影响输出的。\n\n一个模块长成这样：\n\n```v\nmodule module_name\n#(parameter)\n(port) ;\n    function   \nendmodule\n```\n\n模块整体结构由 module 和 endmodule 组成，module 后面跟着的是模块的名称(module_name)，可传递变量参数(parameter)，端口及其方向的申明(port)，紧接着就是内部逻辑功能描述(function) ,最后用 endmodule 来表示这一个模块，描述完毕。\n\n内部逻辑功能通常由 assign 和 always 块完成；其中 assign 语句描述逻辑功能，always 块常用于描述时序功能。\n\n### 代码思路\n\n写代码前我们需要先想清楚代码目的：每隔 0.5S 灯闪一次。\n\n对此所画的需求框图如下：\n\n![block_method](./assets/block_method.png)\n\n然后对于 0.5S 我们需要一个计数器来计时，LED 灯闪就是 IO 翻转\n\n![count_block](./assets/time_count.png)\n\n把上面的思维框图具体到实际使用的话，就变成下面的样式了:\n\n![clock_time_count](./assets/clock_time_count.png)\n\n其中 Clock 为时钟源，用来给计时器提供准确的时间。\n\n### 代码描述\n\n根据上文 Verilog 简单说明和所描述的框图，可以所要编写 Verilog 模块有 Clock 和 IO电平 两个端口；\n\n```v\nmodule led(\n    input  Clock,\n    output IO_voltage\n);\n\nendmodule\n```\n\n对于内部的计时模块，Primer 20K 核心板上的晶振为 27MHZ，因此我们每秒钟会有 27000000 个时钟上升沿，想要 0.5S 计数的话那么只需要计数 13500000 次上升沿就好。计数是从 0 开始的，数 13500000 的话就是从 0 数到 13499999。计数完后我们需要设置一个标志位，来通知 LED 的 IO 翻转一下电平。整体计数代码如下：\n\n```v\n//parameter Clock_frequency = 27_000_000; // 时钟频率为27Mhz\nparameter count_value       = 13_499_999; // 计时 0.5S 所需要的计数次数\n\nreg [23:0]  count_value_reg ; // 计数器\nreg         count_value_flag; // IO 电平翻转标志\n\nalways @(posedge Clock) begin\n    if ( count_value_reg <= count_value ) begin //没有计数到 0.5S\n        count_value_reg  <= count_value_reg + 1'b1; // 继续计数\n        count_value_flag <= 1'b0 ; // 不产生翻转标志\n    end\n    else begin //计数到 0.5S 了\n        count_value_reg  <= 23'b0; // 清零计数器，为重新计数最准备\n        count_value_flag <= 1'b1 ; // 产生翻转标志\n    end\nend\n```\n\n对于 LED IO 电平翻转代码如下：\n\n```v\nreg IO_voltage_reg = 1'b0; // 声明 IO 电平状态用于达到计时时间后的翻转，并赋予一个低电平初始态\n\nalways @(posedge Clock) begin\n    if ( count_value_flag )  //  电平翻转标志有效\n        IO_voltage_reg <= ~IO_voltage_reg; // IO 电平翻转\n    else //  电平翻转标志无效\n        IO_voltage_reg <= IO_voltage_reg; // IO 电平不变\nend\n```\n\n将上面的代码整合后就变成了下面的内容:\n\n```v\nmodule led(\n    input  Clock,\n    output IO_voltage\n);\n\n/**********计时部分**********/\n//parameter Clock_frequency = 27_000_000; // 时钟频率为27Mhz\nparameter count_value       = 13_499_999; // 计时 0.5S 所需要的计数次数\n\nreg [23:0]  count_value_reg ; // 计数器\nreg         count_value_flag; // IO 电平翻转标志\n\nalways @(posedge Clock) begin\n    if ( count_value_reg <= count_value ) begin //没有计数到 0.5S\n        count_value_reg  <= count_value_reg + 1'b1; // 继续计数\n        count_value_flag <= 1'b0 ; // 不产生翻转标志\n    end\n    else begin //计数到 0.5S 了\n        count_value_reg  <= 23'b0; // 清零计数器，为重新计数最准备\n        count_value_flag <= 1'b1 ; // 产生翻转标志\n    end\nend\nreg IO_voltage_reg = 1'b0; // 声明 IO 电平状态用于达到计时时间后的翻转，并赋予一个低电平初始态\n\n/**********电平翻转部分**********/\nalways @(posedge Clock) begin\n    if ( count_value_flag )  //  电平翻转标志有效\n        IO_voltage_reg <= ~IO_voltage_reg; // IO 电平翻转\n    else //  电平翻转标志无效\n        IO_voltage_reg <= IO_voltage_reg; // IO 电平不变\nend\n\n\n/**********补充一行代码**********/\nassign IO_voltage = IO_voltage_reg;\n\nendmodule\n```\n\n上面代码最后面补充了一行代码，是因为 IO_voltage 声明在了 port 位置，默认为 wire 型，想要将它与 reg 变量 IO_voltage_reg 连接起来，需要用到 assign 语句。\n\n## 综合，约束，布局布线\n\n### 综合\n\n代码保存后，可以双击 IDE 内部的 Process -> Synthesize 来进行代码综合，将 verilog 代码内容转换为综合网表。\n\n![synthesize](./assets/synthesize.png)\n\n关于网表有兴趣的可以自己去查阅相关资料，此处不再额外说明。\n\n### 约束\n\n综合完之后我们需要进行管脚约束，才能将所编写的模块端口与 FPGA 引脚相对应，并且实现模块的功能。\n\n点击上图 Synthesize 上面的 FloorPlanner 来进行管脚约束。\n\n![floorplanner](./assets/floorplanner.png)\n\n由于是首次创建，所以会弹出下面的对话框，点击 OK 后就弹出了图形化约束交互界面。\n\n![create_constrain_file](./assets/create_constrain_file.png)\n\n![floorplanner_intreface](./assets/floorplanner_interface.png)\n\n关于约束的方法可以查看 [SUG935-1.3_Gowin设计物理约束用户指南.pdf](http://cdn.gowinsemi.com.cn/SUG935-1.3_Gowin%E8%AE%BE%E8%AE%A1%E7%89%A9%E7%90%86%E7%BA%A6%E6%9D%9F%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97.pdf)\n\n此处因个人喜所以仅使用下图中 IO Constranins 方法来约束引脚：\n\n![floor_planner_ioconstrain](./assets/floor_planner_ioconstrain.png)\n\n根据[核心板原理图](https://dl.sipeed.com/fileList/TANG/Primer_20K/02_Schematic/Tang_Primer_20K_Core_board_3690.pdf)，我们可以知道晶振所输入的引脚为 H11。\n\n<img src=\"./assets/crystal_port.png\" alt=\"crystal_port\" width=45%>\n\n然后结合底板上的 IO 丝印，决定用地板上的 L14 引脚进行点灯。\n\n![l14_port](./assets/l14_port.png)\n\n因此对于在 FloorPlanner 交互窗口下面的 IO Constranins 中填入下面的值：\n\n![io_constrain_value](./assets/io_constrain_value.png)\n\n输入完毕后快捷键 Ctrl + S 来保存一下约束，然后接可以关闭 FloorPlanner 的交互图形界面了。\n\n接着发现在工程项目里面多出来刚刚创建的 cst 文件了，里面的内容也比较好理解。\n\n![cst_content](./assets/cst_content.png)\n\n### 布局布线\n\n完成约束后就要开始运行布局布线了，目的是为了把综合所生成的网表和我们自己定义的约束来通过 IDE 算出最优解然后将资源合理的分配在 FPGA 芯片上。\n\n双击下体红框处的 Place&Route 就开始运行了。\n![place_route](./assets/place_route.png)。\n\n紧接着没有报错，全部通过。就饿可以开始进行烧录了。\n\n## 烧录固件\n\n如果使用的是 bl702 下载器，那么要求使用  [此处](https://dl.sipeed.com/shareURL/TANG/programmer) 链接内的 Programmer 软件来进行烧录，来避免 Programmer 软件识别不到芯片等一系列问题。\n\n下载后解压替换掉 Gowin 对应安装目录的 Programmer 文件夹即可。\n不会替换的话可以在下载解压后的 Programmer 程序中手动添加需要下载的 .fs 文件来进行烧录。\n\n### 接线说明\n\n由于需要将核心板与下载器进行连线，这里说明一下所连接的对应端口。\n\n<table>\n    <tr>\n        <td>核心板</td>\n        <td>5V0</td>\n        <td>TMS</td>\n        <td>TDO</td>\n        <td>TCK</td>\n        <td>TDI</td>\n        <td>RX</td>\n        <td>TX</td>\n        <td>GND</td>\n    </tr>\n    <tr>\n        <td>调试器</td>\n        <td>5V0</td>\n        <td>TMS</td>\n        <td>TDO</td>\n        <td>TCK</td>\n        <td>TDI</td>\n        <td>TX</td>\n        <td>RX</td>\n        <td>GND</td>\n    </tr>\n</table>\n\n![cable_connect](./assets/cable_connect.png)\n\n### 扫描设备\n\n双击下图中的 下载程序(Program Device) 来运行 Programmer 软件\n\n![open_programmer](./assets/open_programmer.png)\n\n正确运行我们所提供下载的 Programmer 软件的话软件上面所显示的软件名称为 `Programmer 2` ，点击下图红框处来扫描设备，且在设备选择中选择 GW2A-18C 。\n\n![scan_device](./assets/scan_device.png)\n\n点击 OK 后就可以进行烧录操作了。\n\n烧录相关的文档可以参考 [SUG502-1.3_Gowin_Programmer用户指南.pdf](http://cdn.gowinsemi.com.cn/SUG502-1.3_Gowin_Programmer%E7%94%A8%E6%88%B7%E6%8C%87%E5%8D%97.pdf)\n\n### 下载到 SRAM\n\n一般来说这个模式是以用来快速要证所生成的固件是否满足自己目的的。\n\n因为其烧录快的特性所以使用的较多，然是当然断电会丢失数据，所以如果想上电运行程序的话是不能选这个的。\n\n点击 Operation 下面的功能框来打开设备设置界面，接着在 Operation 框中选择 SRAM Program 选项来设置为下载到 SRAM ，最后点击下面的那三个点点框来选择我们所生成的 .fs 下载固件。通常来说下载固件生成与工程文件目录下的 impl -> pnr 目录下。\n\n![sram_mode](./assets/sram_mode.png)\n\n接着来点击红框处开始进行烧录 \n\n![sram_download](./assets/sram_download.png)\n\n有问题的话可以前往 [常见问题](https://wiki.sipeed.com/hardware/zh/tang/Tang-Nano-Doc/questions.html) 自行排查。\n\n到这里就下载完成了。\n\n### 下载到 Flash\n\n上面说过下载到 SRAM 是为了快速验证，但是不能上电运行程序。\n所以想要上电运行的话我们需要设置下载到 Flash。\n\n和上面下载到 SRAM 的步骤几乎类似，先点开 Operation 下面的功能框来打开设备设置界面，接着在 Operation 框中选择 External Flash Mode 选项来设置为下载到外部 Flash ，最后点击下面的那三个点点框来选择我们所生成的 .fs 下载固件，通常来说下载固件生成与工程文件目录下的 impl -> pnr 目录下。最后在下面的外部 Flash 选项中选择设备为 Generic Flash 。\n\n![flash_mode](./assets/flash_mode.png)\n\n接着来点击红框处开始进行烧录 \n\n![flash_download](./assets/flash_download.png)\n\n然后我们的程序重新上电也能照样运行了。\n\n### 代码效果\n\n使用 Sipeed 的 SPMOD 后，如下图所示有一个灯在闪。\n\n![result](./assets/result.gif)"}, "/news/others/linux_adb/linux_adb.html": {"title": "Linux 连接不上 adb 设备", "content": "---\ntitle: Linux 连接不上 adb 设备\nkeywords: Linux, adb\ndate: 2022-07-21\ntags: Linux, adb\n---\n\n这里写一下 Linux 系统下连接 M2Dock 后可能出现的 adb 问题\n\n```bash\nuser@ubuntu:~$ adb shell\nerror: insufficient permissions for device \n```\n\n<!-- more -->\n\n在 linux 系统下连接 M2Dock 之后，使用 `adb shell` 后出现 `error: insufficient permissions for device` 。\n\n```bash\nuser@ubuntu:~$ adb shell\nerror: insufficient permissions for device \n```\n\n这时我们应该使用命令 `lsusb` 来查看一下是否成功连接到系统了\n\n```bash\nlee@ubuntu:~$ lsusb\nBus 001 Device 002: ID 18d1:0002 Google Inc. \nBus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub\n```\n\n比如上面的 `Google Inc` 就代表 adb 设备。\n执行 `lsusb` 没有 `Google Inc` 显示的话，就先检查一下 M2Dock 是否正常启动了（手摸屏幕试试温度是最快的方法），确定正常启动后可以更换数据线或者使用别的 USB 口等方式来尝试成功连接到电脑。\n\n根据上面显示出来的 `Bus 001 Device 002: ID 18d1:0002 Google Inc.` 信息，我们需要在 `/etc/udev/rules.d/` 新建一个名为 `51-android.rules` 的文件，其内容应该为 `SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"18d1\", ATTRS{idProduct}==\"0002\",MODE=\"0666\"` ，其中 `ATTRS{idVendor}` 和 `ATTRS{idProduct}` 的值应该根据前面 `lsusb` 命令中的而修改，这里自己注意一下；最后更改文件权限：\n\n```bash\nsudo echo \"SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"18d1\", ATTRS{idProduct}==\"0002\",MODE=\"0666\"\" | sudo tee /etc/udev/rules.d/51-android.rules\nsudo chmod a+x /etc/udev/rules.d/51-android.rules\n```\n\n上面的两行代码中，第一行写入了 `SUBSYSTEM==\"usb\", ATTRS{idVendor}==\"18d1\", ATTRS{idProduct}==\"0002\",MODE=\"0666\"` 内容到 `/etc/udev/rules.d/51-android.rules` 文件。\n第二行更改了对应的文件权限。\n\n接着重新插拔 USB，在使用 `adb shell` 就发现可以正常操作 M2Dock 了。\n\n有什么问题的话可以在下方留言来一起探讨"}, "/news/others/threshold/threshold.html": {"title": "在线图传调试 LAB 阈值", "content": "---\ntitle: 在线图传调试 LAB 阈值\nkeywords: LAB, 阈值, threshold,\ndesc: Threshold 简介\ndate: 2022-07-12\ntags: 图传 ,在线调试 , 阈值\ncover: ./assets/threshold.png\n---\n\n网上发现一篇不错的文章。这里转载一下，有较大改动 [原文链接](https://bbs.elecfans.com/jishu_2290503_1_1.html)\n\n<!-- more -->\n\n## 前言\n\n根据 官方MaixPy3 和 M2 Dock 的相关说明和官方文档与样例，在大家的帮助下，学习了基础的魔方色块的寻找功能。再此分享给大家。\n\n## 基础科普\n\n### 图传\n\n图传的概念，在无人机中非常常见。\n\n简单来讲，就是把摄像头拍摄的实时视频，又快又好的传递到终端设备上呈现——既要速度，不能卡，卡了没意思；也要质量，清晰度不能低，低了没得玩。而传输速度快，质量高，会占用较多的设备资源，以及需要较大的带宽。所以设计一个上好的图传方案和系统，是很多该行业厂家的重大追求目标之一。\n\n### LAB\n\nLab是一种用数字化的方法来描述人的视觉感应的颜色系统。它是一种设备无关的、基于生理特征的颜色系统。在机器视觉中，Lab的概念会经常提及。\n可以用一张图，来详细描述Lab颜色空间：\n\n![lab](./assets/lab.png)\n\n上示图片，是从人的视觉感应角度来看的。\n- 首先是L：表示亮度，从纯黑到纯白，取值为 [0 -> 100]\n- 然后是a：表示从蓝色到红色的范围，取值为 [-128 -> 127]\n- 最后是b：表示从蓝色到黄色的范围，取值为 [-128 -> 127]\n\n通常，Lab会以范围的形式来表示，也就是Lab阈值，因为因为现场环境的不同，我们看到的颜色，不可能是完完全全的理论纯色，所以给出一定的容错范围；\n例如：[(0, 100, 21, 127, -128, -9)]，分别表示：L-min、L-max、a-min、a-max、b-min、b-max，机器视觉就根据这个范围，来进行颜色判断。\n\n## M2Dock 启用图传\n\n### 相关代码\n\n了解相关概念后，可以开始使用 M2 Dock 图传功能了。\n\n下面就是相关启用图传代码了。其实在[官方文档](https://wiki.sipeed.com/soft/maixpy3/zh/usage/net.html#MJPG-%E5%9B%BE%E4%BC%A0-%E6%80%8E%E4%B9%88%E7%94%A8%EF%BC%9F)都写过\n\n```python\nfrom maix import camera, mjpg, utils, display\n\nqueue = mjpg.Queue(maxsize=8)\nmjpg.MjpgServerThread(\"0.0.0.0\", 18811, mjpg.BytesImageHandlerFactory(q=queue)).start()\n\nwhile True:\n    img = camera.capture()\n    jpg = utils.rgb2jpg(img.convert(\"RGB\").tobytes(), img.width, img.height)\n    queue.put(mjpg.BytesImage(jpg))\n    display.show(img)\n```\n\n上面的代码有三种运行方式：\n1. 在 jupyter 的网页编辑界面，运行上述代码\n2. 可以用 adb shell 或者 ssh 连接到 M2 Dock 后，运行 python ，再输入代码运行\n3. 用 adb shell 或者 ssh 连接到 M2 Dock 后，用vim编辑 tuchuan.py 并保存后，再执行 python ./tuchuan.py 来运行代码\n\n当然对于小白来说，方式1最方便，方式2最麻烦。但是方式3运行效率最好\n\n正确运行上述代码后，接可以开启图传功能了。\n\n### 实际使用\n\n要访问M2 Dock提供的图传功能，可以有几种方式，根据不同情况可以选择不同方式：\n\n#### 使用 OTG 连接运行代码\n\n使用 jyputer 或 adb shell 都可以算作是使用 otg 连接板子\n\n这种情况在成功弹出U盘（即正常连接）后可以在浏览器访问 [http://127.0.0.1:18811](http://127.0.0.1:188811) 来打开图传页面。\n\n![local](./assets/local.png)\n\n原理可查看[这里](https://wiki.sipeed.com/soft/maixpy3/zh/tools/MaixPy3_IDE.html#IDE-%E8%BF%9E%E6%8E%A5%E5%8E%9F%E7%90%86)\n\n#### 非 OTG 运行代码\n\n使用串口或者ssh或者设置成开机运行的代码的话，均可认为是非 OTG 运行代码，这个时候我们要通过IP来访问板子图传网址了，即访问 http://设备ip地址:18811 来查看图传实例\n\n![wireless](./assets/threshold.png)\n\n## Lab 阈值实例\n\n前面说过，机器视觉中会利用到 Lab，同样的，MaixPy3 也提供了很简单的方法来应用 Lab 阈值。\n\n这里有一个能直接使用的在线工具: [https://wiki.sipeed.com/threshold](https://wiki.sipeed.com/threshold)（加载有点慢） \n\n提供了一个简单的示例，可以自己通过调整下面滑块位置或者输入阈值数值来查看不同的效果\n\n![threshold](./assets/threshold.png)\n\n也可以自己手动上传图片然后查看不同阈值所产生的效果，当然这里主要介绍无线调整阈值功能。\n\n> 因为浏览器安全方面的权限，我们需要把 [https://wiki.sipeed.com/threshold](https://wiki.sipeed.com/threshold) 先保存到本地才能进行局域网图传。\n使用键盘上面的 `Ctrl + S` 两个按键来将该网页保存到本地，然后在浏览器打开保存到本地的网页就等于在本地运行 [https://wiki.sipeed.com/threshold](https://wiki.sipeed.com/threshold) 了，接着就可以开始后续的操作来实现局域网图传了。\n\n成功运行上面[图传代码](#m2dock-启用图传)且能够在浏览器地址栏中输入板子相应的 IP 地址和图传端口号来访问图传画面后，就可以在本地阈值网页中输入正确的图传 IP 后来直接调整阈值了。下图中右上角有一个图传地址的输入栏，在那里输入正确查看 m2dock 图传的 IP 后就能够实时查看摄像头录制画面且能够实时调整阈值了。\n\n![threshold_1](./assets/wireless_1.png)\n\n熟练的话可以知道能够调整a值来获取橙色色块阈值，做到下面的效果：\n\n![threshold_2](./assets/wireless_2.png)\n\n上图中可以看到除了红色和橙色之外的颜色都被过滤掉了，其他颜色都成了黑色区域。\n\n可以再调整一下L，来使得比较黑的红色也被排除掉\n\n![threshold_3](./assets/wireless_3.png)\n\n换到其他魔方面调整阈值来说橙色模块更准确点：\n\n![threshold_4](./assets/wireless_4.png)\n\n尽量在多个颜色里面来选出自己需要的颜色，因此像下面这种同一面颜色的不好选出差别\n\n![threshold_5](./assets/wireless_5.png)\n\n所以把魔方打乱后识别效果合适很多。\n\n通过上述说明，可以逐步得到模仿六种颜色的阈值了。\n\n## 实战演示\n\n这里使用一个金字塔魔方来进行展示：\n\n<img src=\"./assets/cube_1.png\" width=50% alt=cube_1><img src=\"./assets/cube_2.png\" width=50% alt=cube_2>\n\n使用支架来将 m2dock 摄像头对准魔方\n\n![snap](./assets/snap.png)\n\n在电脑上进行阈值的调整\n\n![change](./assets/change.png)\n\n最终得到下面的四组值：\n\n```python\n[(0, 100, -128, -23, -128, 127)], #绿色\n[(10, 100, 30, 127, -37, 127)],   #红色\n[(40, 100, -25, 42, 7, 127)],     #黄色\n[(0, 100, -128, 127, -128, -46)], #蓝色\n```\n\n根据官方例程修改后得到下面代码：\n\n```python\nfrom maix import image, display, camera\ncolor = [\n        [(0, 100, -128, -23, -128, 127)], #绿色\n        [(10, 100, 30, 127, -37, 127)], #红色\n        [(40, 100, -25, 42, 7, 127)], #黄色\n        [(0, 100, -128, 127, -128, -46)], #蓝色\n        ]  # 0.5.0 以后蓝色的 lab 阈值，0.4.9 之前为 [(13, 11, -91, 54, 48, -28)]\nfont_color = [ # 边框和文字颜色，暂时都用白色\n    (255,255,255), # 绿色\n    (255,255,255), # 红色\n    (255,255,255), # 黄色\n    (255,255,255)  # 白色\n]\nname_color = ('green', 'red', 'yellow', 'blue')\nwhile True:\n    img = camera.capture()\n    for n in range(0,4):\n        blobs = img.find_blobs(color[n])    #在图片中查找lab阈值内的颜色色块\n        if blobs:\n            for i in blobs:\n                if i[\"w\"]>15 and i[\"h\"]>15:\n                    img.draw_rectangle(i[\"x\"], i[\"y\"], i[\"x\"] + i[\"w\"], i[\"y\"] + i[\"h\"], \n                                       color=font_color[n], thickness=1) #将找到的颜色区域画出来\n                    img.draw_string(i[\"x\"], i[\"y\"], name_color[n], scale = 0.8, \n                              color = font_color[0], thickness = 1) #在红色背景图上写下hello worl\n    display.show(img)\n```\n\n运行上述代码后，识别的效果如下：\n\n<img src=\"./assets/result_1.png\" width=50% alt=result_1><img src=\"./assets/result_2.png\" width=50% alt=result_2>\n\n可以看到已经成功识别出魔方颜色块，且效果还不错。\n\n具体效果视频可以前往 [原链接](https://bbs.elecfans.com/jishu_2290503_1_1.html) 查看"}, "/news/others/maixII_connect_udisk.html": {"title": "MaixII 通过 USB OTG 口连接U盘", "content": "---\ntitle: MaixII 通过 USB OTG 口连接U盘\nkeywords: MaixII, U盘\ndate: 2022-06-14\ntags: MaixII, U盘\n---\n\nMaixII的USB口是用来做device连接电脑跑adb的。\n但是有没有方法可以在不跑adb的时候（总不能天天跑adb吧，再说adb也可以网络跑啊）连接一些USB设备玩玩呢。\n\n<!-- more -->\n\n原文链接：https://bbs.sipeed.com/thread/844 有改动\n\n## 摸索过程\n\nMaixII dock 有两个接口，我们要更改 otg 口因此我们使用 UART口 连接电脑来更改板子设置\n\n### 看看在那里定义了 \n\n在 /etc/init.d/ 文件夹里面可以看到有如下的文件\n\n```bash\nroot@sipeed:# ls /etc/init.d\nS00mpp       S10udev      S40network   S52ntpd      log          rc.preboot\nS01audio     S11dev       S41netparam  adbd         network      rcK\nS01logging   S12usb       S50telnet    cron         rc.final     rcS\nS02app       S20urandom   S51dropbear  fontconfig   rc.modules   sysntpd\n```\n\n注意到里面有一个 `S12usb`\n\n使用 `cat /etc/init.d/S12usb` 查看里面内容后发现有一句 \n\n```bash\notg_role=`cat /sys/devices/platform/soc/usbc0/otg_role`\n```\n\n抱着好奇的心态在设备上跑了这句脚本，结果如下所示：\n\n```bash\nroot@sipeed:~# cat /sys/devices/platform/soc/usbc0/otg_role\nusb_device\n```\n\n### 切换为 USB host\n\n再好奇下看这个 /sys/devices/platform/soc/usbc0 目录中都有啥，结果如下：\n\n```bash\nroot@sipeed:~# ls /sys/devices/platform/soc/usbc0\ndriver           hw_scan_debug    of_node          subsystem        usb_device       usb_null\ndriver_override  modalias         otg_role         uevent           usb_host\n```\n\n重点是里面的：`usb_device` `usb_host` `usb_null`\n\n那直接把 `usb_host` echo 到 `/sys/devices/platform/soc/usbc0/otg_role` 中看看啥效果：\n\n```bash\necho \"usb_host\" > /sys/devices/platform/soc/usbc0/otg_role\n```\n\n然后我们使用 `lsusb` 看看都有啥\n\n```bash\nroot@sipeed:~# lsusb\nBus 001 Device 001: ID 1d6b:0002\nBus 002 Device 001: ID 1d6b:0001\n```\n\n哈，USB控制器出来了。\n\n### 连接USB设备\n\n想着设备内识别SD卡，那U盘应该也差不多。插个U盘试下。\n\n```bash\nroot@sipeed:~# lsusb\nBus 001 Device 001: ID 1d6b:0002\nBus 001 Device 002: ID aaaa:8816\nBus 002 Device 001: ID 1d6b:0001\n```\n\n多出来一个设备，在 /dev 目录下看了下果然多出来一个sda：\n\n```bash\nroot@sipeed:/# ls /dev/sda\nsda   sda1  sda2\n```\n\n挂载 U盘 试试：\n\n出现 `Read-only file system` 的话，重烧是最快的解决方法。\n\n```bash\nroot@sipeed:~# mkdir -p /home/usbdisk\nroot@sipeed:~# mount /dev/sda2 /home/usbdisk/\nroot@sipeed:~# df\nFilesystem           1K-blocks      Used Available Use% Mounted on\n/dev/root               256512     88352    162920  35% /\ntmpfs                    29864        12     29852   0% /tmp\nnone                     29796         0     29796   0% /dev\n/dev/mmcblk0p3            2013         1      2013   0% /mnt/cfg\n/dev/mmcblk0p6         2939292     59664   2863244   2% /mnt/UDISK\n/dev/sda4              7926272    405644   7520628   5% /home/usbdisk\n```\n\n挂载成功。\n\n然后，试了下无线网卡、USB串口啥的，基本都没识别出来，估计是驱动没有编译进去吧。"}, "/news/others/v831_opencv/v831_opencv.html": {"title": "给 M2Dock 安装 Opencv", "content": "---\ntitle: 给 M2Dock 安装 Opencv\nkeywords: M2Dock, Opencv, Python, V831\ndate: 2022-07-23\ntags: Opencv, M2Dock, V831\n---\n\nM2Dock 既然能够运行 Python, 那么我们也可以给它安装 Opencv。\n\n<!-- more -->\n\n## 前言\n\n由于 V831 性能有限，且交叉编译什么的对于大多数人来说太烦琐了，因此这里提供一个 github 上的一位的大神 [irfan798](https://github.com/irfan798) 所 release 的一个适用于 M2Dock 的 Opencv 库。\n\n仓库地址: [v83x_opencv-4.5.5.62_numpy-1.19.2](https://github.com/irfan798/maix3_opencv_python/releases/tag/cv-4.5.5.62_numpy-1.19.2)\n\n相关的使用方法在 release 页面已经写了，但是不会操作话可以继续往下面看。\n\n## 下载、安装、运行\n\n![download_open](./assets/download_opencv.png)\n\n点击上图中红框处的文件即可下载由 [irfan798](https://github.com/irfan798) 所发布的适用于 M2Dock 的 Opencv Python 安装包。\n\n把下载下来的名为 `opencv_python_headless-4.5.5.62-cp38-cp38-linux_armv7l.whl` 文件复制到由 M2Dock 在电脑上所显示的 U 盘中。\n\n在 adb 命令行终端中依次执行下面的命令来在 M2Dock 上安装刚刚所下载的 Opencv Python 安装包：\n\n```shell\nsync  #刷新所有文件\npip install /root/opencv_python_headless-4.5.5.62-cp38-cp38-linux_armv7l.whl --upgrade #安装刚刚下载的 Opencv Python 安装包\n```\n\n要注意的是上面的操作是需要在 M2Dock 的命令终端执行。\n\n成功安装完之后我们可以在 M2Dock 上运行 Opencv 了。如下所示:\n\n```shell\nroot@sipeed:~# python\n>>> cv2.version\n'4.5.5'\n```\n\n## 其他\n\n如果你想使用其他的 numpy 版本，可以根据 [irfan798](https://github.com/irfan798) 所创建 [https://github.com/irfan798/maix3_opencv_python](https://github.com/irfan798/maix3_opencv_python) 仓库里面的 README.md 内的相关操作来自行编译自己想要的 numpy 版本。"}, "/news/others/v831_resnet18/v831_resnet18.html": {"title": "在V831上（awnn）跑 pytorch resnet18 模型", "content": "---\ntitle: 在V831上（awnn）跑 pytorch resnet18 模型\nkeywords: V831,awnn,resnet18\ndate: 2022-06-13\ntags: V831,awnn,resnet18\n---\n\n在V831上（awnn）跑 pytorch resnet18 模型，和模型转换方法\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/358\n\n原文时间：2021.04.10， 搬运有改动\n\n- 可以参考一下\n\n## 直接使用 Pytorch hub 与模型训练\n\n此处省略模型定义和训练过程，仅使用 pytorch hub 的 resnet18 预训练模型进行简单介绍\n\nhttps://pytorch.org/hub/pytorch_vision_resnet/\n\n## 在 PC 端测试模型推理\n\n根据上面链接的使用说明，使用下面代码可以运行模型\n\n其中，label 下载：https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n\n```python\nimport os\nimport torch\nfrom torchsummary import summary\n\n## model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\nmodel.eval()\ninput_shape = (3, 224, 224)\nsummary(model, input_shape, device=\"cpu\")\n\n## test image\nfilename = \"out/dog.jpg\"\nif not os.path.exists(filename):\n    if not os.path.exists(\"out\"):\n        os.makedirs(\"out\")\n    import urllib\n    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n    try: urllib.URLopener().retrieve(url, filename)\n    except: urllib.request.urlretrieve(url, filename)\nprint(\"test image:\", filename)\n\n## preparing input data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\ninput_image = Image.open(filename)\n\n# input_image.show()\npreprocess = transforms.Compose([\n    transforms.Resize(max(input_shape[1:3])),\n    transforms.CenterCrop(input_shape[1:3]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\nprint(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor)))\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n\n## forward model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\nwith torch.no_grad():\n    output = model(input_batch)\n\n## result    \n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n# print(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nmax_1000 = torch.nn.functional.softmax(output[0], dim=0)\nmax_idx = int(torch.argmax(max_1000))\nwith open(\"imagenet_classes.txt\") as f:\n    labels = f.read().split(\"\\n\")\nprint(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx]))\n```\n\n运行结果如下：\n\n```python\nUsing cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 44.59\nEstimated Total Size (MB): 107.96\n----------------------------------------------------------------\nout/dog.jpg\ntensor(2.6400) tensor(-2.1008)\nidx:258, name:Samoyed, Samoyede\n```\n\n可以看到模型有 11,689,512 个参数，差不多 11MiB 左右，这个大小也几乎是实际在 831 上运行的模型大小了\n\n## 将模型转换为 V831 能使用的模型文件\n\n转换过程如下：\n\n使用 Pytorch 将模型导出为 onnx 模型， 得到 onnx 文件\n\n```python\ndef torch_to_onnx(net, input_shape, out_name=\"out/model.onnx\", input_names=[\"input0\"], output_names=[\"output0\"], device=\"cpu\"):\n    batch_size = 1\n    if len(input_shape) == 3:\n        x = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype=torch.float32, requires_grad=True).to(device)\n    elif len(input_shape) == 1:\n        x = torch.randn(batch_size, input_shape[0], dtype=torch.float32, requires_grad=False).to(device)\n    else:\n        raise Exception(\"not support input shape\")\n    print(\"input shape:\", x.shape)\n    # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params=True)\n    torch.onnx.export(net, x, out_name, export_params=True, input_names = input_names, output_names=output_names)\nonnx_out=\"out/resnet_1000.onnx\"\nncnn_out_param = \"out/resnet_1000.param\"\nncnn_out_bin = \"out/resnet_1000.bin\"\ninput_img = filename\ntorch_to_onnx(model, input_shape, onnx_out, device=\"cuda:0\")\n```\n\n如果你不是使用 pytorch 转换的, 而是使用了现成的 ncnn 模型, 不知道输出层的名字, 可以在 https://netron.app/ 打开模型查看输出层的名字\n\n使用 onnx2ncnn 工具将 onnx 转成 ncnn 模型，得到一个 .param 文件和一个 .bin 文件\n按照 ncnn 项目的编译说明编译，在 build/tools/onnx 目录下得到 onnx2ncnn 可执行文件\n\n```python\ndef onnx_to_ncnn(input_shape, onnx=\"out/model.onnx\", ncnn_param=\"out/conv0.param\", ncnn_bin = \"out/conv0.bin\"):\n    import os\n    # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir\n    cmd = f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\"\n    os.system(cmd)\n    with open(ncnn_param) as f:\n        content = f.read().split(\"\\n\")\n        if len(input_shape) == 1:\n            content[2] += \" 0={}\".format(input_shape[0])\n        else:\n            content[2] += \" 0={} 1={} 2={}\".format(input_shape[2], input_shape[1], input_shape[0])\n        content = \"\\n\".join(content)\n    with open(ncnn_param, \"w\") as f:\n        f.write(content)\nonnx_to_ncnn(input_shape, onnx=onnx_out, ncnn_param=ncnn_out_param, ncnn_bin=ncnn_out_bin)\n\n```\n## 使用全志提供的awnn工具将ncnn模型进行量化到int8模型\n\n在 maix.sipeed.com 模型转换 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做）\n\n阅读转换说明，可以获得更多详细的转换说明\n\n![](./assets/convert.png)\n\n这里有几组参数：\n\n- 均值 和 归一化因子： 在 pytorch 中一般是 `(输入值 - mean ) / std`, awnn 对输入的处理是 `(输入值 - mean ) * norm`, 总之，让你训练的时候的输入到第一层网络的值范围和给 awnn 量化工具经过 `(输入值 - mean ) * norm` 计算后的值范围一致既可。 比如这里打印了实际数据的输入范围是 [-2.1008, 2.6400]， 是代码中 preprocess 对象处理后得到的，即 `x = (x - mean) / std ==> (0-0.485)/0.229 = -2.1179`, 到 awnn 就是 `x = (x - mean_2*255) * (1 / std * 255)` 即 `mean2 = mean * 255`, `norm = 1/(std * 255)`, 更多可以看[这里](https://github.com/Tencent/ncnn/wiki/FAQ-ncnn-produce-wrong-result#pre-process)。\n所以我们这里可以设置 均值为 `0.485 * 255 = 123.675`， 设置 归一化因子为 `1/ (0.229 * 255) = 0.017125`， 另外两个通道同理。但是目前 awnn 只能支持三个通道值一样。。。所以填 `123.675, 123.675, 123.675，0.017125, 0.017125, 0.017125` 即可，因为这里用了 pytorch hub 的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下\n\n- 图片分辨率（问不是图片怎么办？貌似 awnn 暂时之考虑到了图片。。）\n\n- RGB 格式： 如果训练输入的图片是 RGB 就选 RGB\n- 量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手\n\n自己写的其它模型转换如果失败，多半是啥算子不支持，上图框出的地方查看所支持的算子，比如现在的版本view、 flatten、reshape 都不支持所以写模型要相当小心，后面的版本会支持 flatten reshape 等 CPU 算子\n\n如果不出意外， 终于得到了量化好的 awnn 能使用的模型， *.param 和 *.bin\n\n## 使用模型，在v831上推理\n\n可以使用 python 或者 C 写代码，以下两种方式\n\n### MaixPy3\n\npython 请看 [MaixPy3](https://wiki.sipeed.com/soft/maixpy3/zh/)\n\n不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了：\n\n```bash\npip install --upgrade maixpy3\n```\n\n然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）：\n\nhttps://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/load_forward_camera.py\n\nlabel 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py\n\n```python\nfrom maix import nn\nfrom PIL import Image, ImageDraw\nfrom maix import camera, display\n\ntest_jpg = \"/root/test_input/input.jpg\"\nmodel = {\n    \"param\": \"/root/models/resnet_awnn.param\",\n    \"bin\": \"/root/models/resnet_awnn.bin\"\n}\n\ncamera.config(size=(224, 224))\n\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (1, 1, 1000)\n    },\n    \"first_layer_conv_no_pad\": False,\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n}\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nprint(\"-- read image\")\nimg = Image.open(test_jpg)\nprint(\"-- read image ok\")\nprint(\"-- forward model with image as input\")\nout = m.forward(img, quantize=True)\nprint(\"-- read image ok\")\nprint(\"-- out:\", out.shape)\nout = nn.F.softmax(out)\nprint(out.max(), out.argmax())\n\nfrom classes_label import labels\nwhile 1:\n    img = camera.capture()\n    if not img:\n        time.sleep(0.02)\n        continue\n    out = m.forward(img, quantize=True)\n    out = nn.F.softmax(out)\n    msg = \"{:.2f}: {}\".format(out.max(), labels[out.argmax()])\n    print(msg)\n    draw = ImageDraw.Draw(img)\n    draw.text((0, 0), msg, fill=(255, 0, 0))\n    display.show(img)\n```\n\n### C 语言 SDK,libmiax\n\n按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet\n\n上传编译成功后dist目录下的所有内容到 v831, 然后执行./start_app.sh即可"}, "/news/others/Python_call_so.html": {"title": "Python3调用c/cpp的方法", "content": "---\ntitle: Python3调用c/cpp的方法\nkeywords: python, c, cpp,\ndesc: python调用so\ndate: 2022-03-31\ntags: python, c, cpp\n---\n\n<!-- more -->\n\n原文链接：https://blog.csdn.net/springlustre/article/details/101177282\n作者：[springlustre](https://blog.csdn.net/springlustre?type=blog)\n有改动，仅供参考\n\npython中使用 ctypes 模块可以在python中直接调用C/C++。\n首先要将C/C++编译成动态库（.so)，然后python中调用即可。\n\n特别注意在调用C++函数需要在函数声明时，加入前缀 extern \"C\" ，这是因为C++支持函数重载功能，在编译时会改变函数名。在函数声明时，前缀extern \"C\"可以确保按C的方式进行编译。\n\n值得注意的是，一定要有函数输入输出类型的声明，int型不用转换，float和double类型需要进行转换；\nctypes中的变量类型与C中对应如下：\n\n| ctypes数据类型 | C数据类型     |\n| -------------- | ------------- |\n| c_char         | char          |\n| c_short        | short         |\n| c_int          | int           |\n| c_long         | long          |\n| c_float        | float         |\n| c_double       | double        |\n| c_void_p       | void          |\n| c_uint8        | unsigned char |\n\n使用方法：\n- 编写c++代码\n\n```cpp\n#include <iostream>\n#include <string>\n#include <cstdlib>\n#include <vector>\n#include <stdio.h>\n\n\nclass Test{\n    private:\n        double _calculate(int a, double b);\n    public:\n        double calculate(int a, double b, char c[], int * d, double * e, char ** f);\n};\n\ndouble Test::_calculate(int a, double b){\n    double res = a+b;\n    std::cout<<\"res: \"<<res<<std::endl;\n    return res;\n}\n\ndouble Test::calculate(int a, double b, char c[], int * d, double * e, char ** f){\n    std::cout<<\"a: \"<<a<<std::endl;\n    std::cout<<\"b: \"<<b<<std::endl;\n    std::cout<<\"c: \"<<c<<std::endl;\n    std::cout<<\"d: \"<<d[0]<<d[1]<<std::endl;\n    std::cout<<\"e: \"<<e[0]<<e[1]<<std::endl;\n    std::cout<<\"f: \"<<f[0]<<f[1]<<std::endl;\n    return this->_calculate(a, b);\n}\n\n\n// 封装C接口\nextern \"C\"{\n// 创建对象\n    Test* test_new(){\n        return new Test;\n    }\n    double my_calculate(Test* t, int a, double b, char c[], int * d, double * e, char ** f){\n        return t->calculate(a, b,c,d,e,f);\n    }\n}\n\n```\n- 将上面的代码编译成so文件\n\n> g++ -shared -Wl,-soname,test -o test.so -fPIC test.cpp\n\n- 使用python调用so文件\n\n```python\n# -*- coding: utf-8 -*-\nimport ctypes\n# 指定动态链接库\nlib = ctypes.cdll.LoadLibrary('./test.so')\n#需要指定返回值的类型，默认是int\nlib.my_calculate.restype = ctypes.c_double\n\nclass Test(object):\n    def __init__(self):\n        # 动态链接对象\n        self.obj = lib.test_new()\n\n    def calculate(self, a, b,c,d,e,f):\n        res = lib.my_calculate(self.obj, a, b,c,d,e,f)\n        return res\n\n#将python类型转换成c类型，支持int, float,string的变量和数组的转换\ndef convert_type(input):\n    ctypes_map = {int:ctypes.c_int,\n              float:ctypes.c_double,\n              str:ctypes.c_char_p\n              }\n    input_type = type(input)\n    if input_type is list:\n        length = len(input)\n        if length==0:\n            print(\"convert type failed...input is \"+input)\n            return null\n        else:\n            arr = (ctypes_map[type(input[0])] * length)()\n            for i in range(length):\n                arr[i] = bytes(input[i],encoding=\"utf-8\") if (type(input[0]) is str) else input[i]\n            return arr\n    else:\n        if input_type in ctypes_map:\n            return ctypes_map[input_type](bytes(input,encoding=\"utf-8\") if type(input) is str else input)\n        else:\n            print(\"convert type failed...input is \"+input)\n            return null\n\nif __name__ == '__main__':\n    t = Test()\n    A1\t= 123;\n    A2\t= 0.789;\n    A3\t= \"C789\";\n    A4\t= [456,789];\n    A5\t= [0.123,0.456];\n    A6\t= [\"A123\", \"B456\"];\n    print(t.calculate(convert_type(A1), convert_type(A2), convert_type(A3),convert_type(A4),convert_type(A5),convert_type(A6)))\n```"}, "/news/others/color_introduction/color_introduction.html": {"title": "常见的图像颜色空间解释", "content": "---\ntitle: 常见的图像颜色空间解释\nkeywords: Color, 色彩空间,RGB,HSV,YUV,LAB,CMYK\ndesc: 色彩空间科普\ndate: 2022-06-11\ntags: Color, 色彩空间\ncover: ./assets/cover.png\n---\n\n常用颜色表示方法： RGB HSV YUV LAB CMYK\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/294\n\n## RGB\n\n红绿蓝 三色，也是大家熟悉光学三原色\n\nRGB 使用加色模式，也就是默认是黑色，三原色相加获得白色， 比如下图`蓝色+绿色=青色(cyan)`，得到`蓝色=青色-绿色`也就是`蓝色=青色+绿色`的互补色， 绿色的互补色就是图中的`M(品红色）magenta`（同理蓝色的互补色是Y黄色）（互补色就是两者相加为白色），所以`蓝色=青色+品红色`\n\n![](./assets/color_add.png)\n\n![](./assets/coordinate_box.png)\n\n上图可以看到灰度图在正方体对角线上，即三个通道（轴）的值相等时，值越大越白\n\n![](./assets/color.png)\n\n一般两种表示方法：\n\n### RGB888(24bit)\n\nRGB三个通道，每个通道分别用8bit长度表示，比如(255, 255, 255), 每个通道取值范围为[0, 255]，或者0xFFFFFF三个字节表示\n\n### RGB565(16bit)\n\nRGB三个通道分别用5bit 6bit 5bit表示，比如(31, 63, 31)，但一般不这样表示，使用两个字节表示，比如0xFFFF, 一般在编写程序时在内存中多使用以下两种布局方式：\n\n- 第一种：\n\n![](./assets/bgr_color.png)\n\n- 第二种：\n\n![](./assets/grgb_color.png)\n\n这两种方式的不同主要是因为 RGB565 共占用 2 个字节， 两个字节的顺序不同造成的，把第二张图从中间分隔成两份，右边移到左边，就变成了第一种的排列方式了\n\nC语言结构体如下：\n```c\n#define COLOR_16_SWAP 1\ntypedef union\n{\n    struct\n    {\n#if COLOR_16_SWAP == 0\n        uint16_t blue : 5;\n        uint16_t green : 6;\n        uint16_t red : 5;\n#else\n        uint16_t green_h : 3;\n        uint16_t red : 5;\n        uint16_t blue : 5;\n        uint16_t green_l : 3;\n#endif\n    } ch;\n    uint16_t full;\n} color16_t;\n```\n\n比如`(1,2,3)` `(R, G, B)`：\n\n使用第一种方式二进制值为 `B00001 000010 00011 ` 即 `B0000 1000 0100 0011`, 十六进制表示为 `0x0843`（注意这里表示方法从左到右是从高位到低位，上面的图从左到右是低位到高位）；\n\n使用第二种方式二进制值为 `B010 00011 00001 000 `即 `B0100 0011 0000 1000`, 十六进制表示为 `0x4308`；\n\n## HSV\n\n相关解释：\n- Hue（色调、色相）\n- Saturation（饱和度、色彩纯净度）\n- Value（明度）\n\n![](./assets/hsv.png)\n\n## HLS\n\nHLS 中的 L 分量为亮度，亮度为100，表示白色，亮度为0，表示黑色；HSV 中的 V 分量为明度，明度为100，表示光谱色，明度为0，表示黑色。\n\n提取白色物体时，使用 HLS 更方便，因为 HSV 中的Hue里没有白色，白色需要由S和V共同决定（S=0, V=100）。而在 HLS 中，白色仅由亮度L一个分量决定。所以检测白色时使用 HSL 颜色空间更准确。\n\n![](./assets/hls.png)\n\n## YUV\n\nY'UV、YUV、YCbCr、YPbPr 几个概念其实是一回事儿。Y’UV、YUV 主要是用在彩色电视中，用于模拟信号表示。YCbCr 是用在数字视频、图像的压缩和传输，如 MPEG、JPEG。今天大家所讲的 YUV 其实就是指 YCbCr。Y 表示亮度（luma），CbCr 表示色度（chroma）。\n\n另外Y’UV在取值上可以使正或者负数，但是Y’CbCr一般是 `16–235` 或者 `0–255`\n\nY’UV 设计的初衷是为了使彩色电视能够兼容黑白电视。对于黑白电视信号，只需要 Y 通道， 在彩色电视则显示 YUV 信息\n\n![](./assets/yuv.png)\n\n![](./assets/y'uv.png)\n\n### 打包格式和采样\n\nYUV 格式通常有两大类:打包(packed)格式和平面(planar)格式， 前者是每个像素为基本单位，一个一个像素的数据连续排列在内存中， 后者则是YUV 分成3个数组（内存块）存放， 另外还有Y和UV分开存放的（比如 YUV420SP（ Semi-Planar， U和V交叉放，即YYYYYYYY…UVUV…） 和 YUV420P（先放U再放V，即YYYYYYYY…UUVV））\n\n人眼的视觉特点是对亮度更铭感，对位置、色彩相对来说不铭感。在视频编码系统中为了降低带宽，可以保存更多的亮度信息(luma)，保存较少的色差信息(chroma)。这叫做 chrominance subsamping, 色度二次采样。原则：在数字图像中，(1) 每一个图形像素都要包含 luma（亮度）值；（2）几个图形像素共用一个 Cb + Cr 值，一般是 2、4、8 个像素。 一般分为以下几种\n\n- YUV444：就是每个 Y 值对应一个 U 和 一个 V 值\n- YUV422: 就是图像的横轴（width方向） 4 个 Y 值公用 2 个 U 和V，纵轴（height方向）4 个 Y 对应了 4 个 U 和 4 个 V\n- YUV420: 就是在 YUV422 的基础上， 纵轴的 U 和 V 数量也减半，每 2 个提供给 4 个 Y 配对使用\n- YUV411: 同理，就是 4 个 Y， 对应横轴和纵轴的 1 个 U 和 V\n\n![](./assets/yuv_pack.png)\n\n`SP`(Semi-Planar) 和 `P` 的说法， 区别就是在内存中的存放顺序不同, 比如\n\nYUV420SP:\n\n![](assets/yuv420sp.jpg)\n\nYUV420P:\n\n![](assets/yuv420p.jpg)\n\n另外，还有 NV12 和 NV21 的区别， 就是在 UV 的存放上顺序不同\n\n- NV12: IOS只有这一种模式。存储顺序是先存Y，再UV交替存储。YYYYUVUVUV\n- NV21: 安卓的模式。存储顺序是先存Y，再存U，再VU交替存储。YYYYVUVUVU\n\nYUV与RGB之间的转换\n\n略\n\n参考代码： https://github.com/latelee/yuv2rgb\n\n## LAB\n\nLab颜色空间中的L分量用于表示像素的亮度，取值范围是[0,100],表示从纯黑到纯白；a表示从红色到绿色的范围，取值范围是[127,-128]；b表示从黄色到蓝色的范围，取值范围是[127,-128]\n\n![](./assets/LAB.png)\n\n## CMY & CMYK\n\n![](./assets/cmyk_1.jpg)\n\n一般用在印刷， 因为人眼看到的物体的颜色是反射光，而不是自发光，一束白光照到物体上，默认反射所有，即白色，人眼实际看到的颜色是用光的颜色减去材料吸收后的颜色,这时涂上黑色的颜料，即吸收了所有白光，所以人眼看到的是黑色。\n利用色料的三原色混色原理，加上黑色油墨，共计四种颜色混合叠加，形成所谓“全彩印刷”。四种标准颜色是：`C：Cyan = 青色`，又称为‘天蓝色’或是‘湛蓝’`M：Magenta = 品红色`，又称为‘洋红色’；`Y：Yellow = 黄色`；`K：blacK=黑色`，虽然有文献解释说这里的K应该是Key Color（定位套版色），但其实是和制版时所用的定位套版观念混淆而有此一说。此处缩写使用最后一个字母K而非开头的B，是为了避免与Blue混淆。CMYK模式是减色模式，相对应的RGB模式是加色模式。\n\n印刷三原色如何得到黑色， 理论配色如下：\n\n> C(100)  +M（100） +Y（100） = 黑色（100，100，100）\n\n可见黑色就是青色、品与黄色之和，但是这三种颜色混成的黑色不够纯，所以印刷学就引进了K(Black)黑色，因为B已经被Blue占用，所以黑色就只好用引文字母黑色的最后一个字母K, 哪么真正印刷的黑色配色如下:\n\n> C(100)  +M（100） +Y（100） + K(100) = 黑色 （100，100，100，100）\n\n或者\n\n> C(0)  +M(0) + Y(0) + K(100) = 黑色(0，0，0，100）\n\n减色模式：前面说的由于物体是吸收了部分光，才呈现出特有的颜色，比如品红的物体，是吸收了它的互补色绿色（看前面的RGB对互补色的描述），也就是白色光减去了绿色才得到的颜色，所以称之为减色模式"}, "/news/others/python_use.html": {"title": "Python 变量作用域", "content": "---\ntitle: Python 变量作用域\nkeywords: Python, 作用域,\ndesc: Python作用域说明\ndate: 2022-07-13\ntags: Python\n---\n\n这里说明一下 Python 变量作用域，帮助大家更好地使用 Python； [原文链接](https://www.cnblogs.com/Jolly-hu/p/12228320.html)\n\n<!-- more -->\n\n## 作用域简介\n\n**作用域指的是变量的有效范围**。变量并不是在任何位置都可以访问的，访问权限取决于这个变量是在哪里赋值的，也就是在哪个作用域内的。\n\n通常而言，在编程语言中，变量的作用域从代码结构形式来看，有块级、函数、类、模块、包等由小到大的级别。但是在 Python 中，没有块级作用域，也就是类似 if语句块、for语句块、with上下文管理器 等等是不存在作用域概念的，他们等同于普通的语句\n\n```python\nif True:            # if语句块没有作用域\n    x = 1\nprint(x)\n# 1\ndef func():         # 函数有作用域\n    a = 8\nprint(a)\n# Traceback (most recent call last):\n#   File \"<pyshell#3>\", line 1, in <module>\n#     a\n# NameError: name 'a' is not defined\n```\n\n上面的代码可以试着运行一下，然后发现在 if 语句内定义的变量 x，可以被外部访问，而在函数 func() 中定义的变量 a，则无法在外部访问。\n\n通常，函数内部的变量无法被函数外部访问，但内部可以访问；类内部的变量无法被外部访问，但类的内部可以。通俗来讲，就是内部代码可以访问外部变量，而外部代码通常无法访问内部变量。\n\n变量的作用域决定了程序的哪一部分可以访问哪个特定的变量名称。\nPython 的作用域一共有4层，分别是：\n- L （Local） 局部作用域\n- E （Enclosing） 闭包函数外的函数中\n- G （Global） 全局作用域\n- B （Built-in） 内建作用域\n\n```python\nglobal_var = 0  # 全局作用域\ndef outer():\n    out_var = 1  # 闭包函数外的函数中\n    def inner():\n        inner_var = 2  # 局部作用域\n```\n\n前面说的都是变量可以找得到的情况，那如果出现本身作用域没有定义的变量，那该如何寻找呢？\n\nPython 以 L –> E –> G –>B 的规则查找变量，即：在局部找不到，便会去局部外的局部找（例如闭包），再找不到就会去全局找，最后去内建中找。\n\n如果这样还找不到，那就提示变量不存在的错误。例如下面的代码，函数 func 内部并没有定义变量 a，可是 print 函数需要打印 a，那怎么办？\n\n向外部寻找！按照 L –> E –> G –>B 的规则，层层查询，这个例子很快就从外层查找到了 a，并且知道它被赋值为 1，于是就打印了 1。\n\n```python\na = 1\n\ndef func():\n    print(a)\n```\n\n## 全局变量和局部变量\n\n定义在函数内部的变量拥有一个局部作用域，被叫做局部变量，定义在函数外的拥有全局作用域的变量，被称为全局变量。（类、模块等同理）\n\n所谓的局部变量是相对的。局部变量也有可能是更小范围内的变量的外部变量。\n\n局部变量只能在其被声明的函数内部访问，而全局变量可以在整个程序范围内访问。调用函数时，所有在函数内声明的变量名称都将被加入到作用域中。\n\n```python\na = 1               # 全局变量\n\ndef func():\n    b = 2           # 局部变量\n    print(a)        # 可访问全局变量a,无法访问它内部的c\n\n    def inner():\n        c = 3       # 更局部的变量\n        print(a)    # 可以访问全局变量a\n        print(b)    # b对于inner函数来说，就是外部变量\n        print(c)\n```\n\n## global 和 nonlocal 关键字\n\n我们先看下面的例子：\n\n```python\ntotal = 0                        # total是一个全局变量\n\ndef plus( arg1, arg2 ):\n    total = arg1 + arg2          # total在这里是局部变量.\n    print(\"函数内局部变量total=  \", total)\n    print(\"函数内的total的内存地址是: \", id(total))\n    return total\n\nplus(10, 20)\nprint(\"函数外部全局变量total= \", total)\nprint(\"函数外的total的内存地址是: \", id(total))\n```\n很明显，函数 plus 内部通过 total = arg1 + arg2 语句，新建了一个局部变量 total，它和外面的全局变量 total 是两码事。而如果我们想要在函数内部修改外面的全局变量 total 呢？使用  global 关键字！ \n\n> global：指定当前变量使用外部的全局变量\n\n```python\nglobal：指定当前变量使用外部的全局变量\n\ntotal = 0                        # total是一个全局变量\n\ndef plus( arg1, arg2 ):\n    global total    # 使用global关键字申明此处的total引用外部的total\n    total = arg1 + arg2\n    print(\"函数内局部变量total=  \", total)\n    print(\"函数内的total的内存地址是: \", id(total))\n    return total\n\nplus(10, 20)\nprint(\"函数外部全局变量total= \", total)\nprint(\"函数外的total的内存地址是:\n```\n所运行结果：\n```python\n函数内局部变量total=   30\n函数内的total的内存地址是:  503494624\n函数外部全局变量total=  30\n函数外的total的内存地址是:  503494624\n```\n我们再来看下面的例子：\n```python\na = 1\nprint(\"函数outer调用之前全局变量a的内存地址： \", id(a))\n\ndef outer():\n    a = 2\n    print(\"函数outer调用之时闭包外部的变量a的内存地址： \", id(a))\n    def inner():\n        a = 3\n        print(\"函数inner调用之后闭包内部变量a的内存地址： \", id(a))\n    inner()\n    print(\"函数inner调用之后，闭包外部的变量a的内存地址： \", id(a))\nouter()\nprint(\"函数outer执行完毕，全局变量a的内存地址： \", id(a))\n```\n如果你将前面的知识点都理解通透了，那么这里应该没什么问题，三个 a 各是各的 a，各自有不同的内存地址，是三个不同的变量。\n\n打印结果也很好的证明了这点：\n```python\n函数outer调用之前全局变量a的内存地址：  493204544\n函数outer调用之时闭包外部的变量a的内存地址：  493204576\n函数inner调用之后闭包内部变量a的内存地址：  493204608\n函数inner调用之后，闭包外部的变量a的内存地址：  493204576\n函数outer执行完毕，全局变量a的内存地址：  493204544\n```\n那么，如果，inner 内部想使用 outer 里面的那个 a，而不是全局变量的那个 a，怎么办？用 global 关键字？先试试看吧：\n```python\na = 1\nprint(\"函数outer调用之前全局变量a的内存地址： \", id(a))\ndef outer():\n    a = 2\n    print(\"函数outer调用之时闭包外部的变量a的内存地址： \", id(a))\n    def inner():\n        global a   # 注意这行\n        a = 3\n        print(\"函数inner调用之后闭包内部变量a的内存地址： \", id(a))\n    inner()\n    print(\"函数inner调用之后，闭包外部的变量a的内存地址： \", id(a))\nouter()\nprint(\"函数outer执行完毕，全局变量a的内存地址： \", id(a))\n```\n运行结果如下，很明显，global使用的是全局变量a。\n```python\n函数outer调用之前全局变量a的内存地址：  494384192\n函数outer调用之时闭包外部的变量a的内存地址：  494384224\n函数inner调用之后闭包内部变量a的内存地址：  494384256\n函数inner调用之后，闭包外部的变量a的内存地址：  494384224\n函数outer执行完毕，全局变量a的内存地址：  494384256\n```\n那怎么办呢？使用 nonlocal 关键字！它可以修改嵌套作用域（enclosing 作用域，外层非全局作用域）中的变量。将 global a 改成 nonlocal a 后运行，代码这里我就不重复贴了，\n\n运行后查看结果，可以看到我们真的引用了 outer 函数的 a 变量。\n```python\n函数outer调用之前全局变量a的内存地址：  497726528\n函数outer调用之时闭包外部的变量a的内存地址：  497726560\n函数inner调用之后闭包内部变量a的内存地址：  497726592\n函数inner调用之后，闭包外部的变量a的内存地址：  497726592\n函数outer执行完毕，全局变量a的内存地址：  497726528\n```\n\n## 面试真题\n不要上机测试，请说出下面代码的运行结果：\n\n```python\na = 10\ndef test():\n    a += 1\n    print(a)\ntest()\n```\n\n很多同学会说，这太简单了！函数内部没有定义 a，那么就去外部找，找到 a=10，于是加 1，打印 11！\n\n我会告诉你，这段代码有语法错误吗？a += 1 相当于 a = a + 1，按照赋值运算符的规则是先计算右边的 a+1。但是，Python 的规则是，如果在函数内部要修改一个变量，那么这个变量需要是内部变量，除非你用 global 声明了它是外部变量。很明显，我们没有在函数内部定义变量 a，所以会弹出局部变量在未定义之前就引用的错误。\n\n## 更多的例子\n\n再来看一些例子（要注意其中的闭包，也就是函数内部封装了函数）：\n\n```python\nname = 'jack'\n\ndef outer():\n    name='tom'\n\n    def inner():\n        name ='mary'\n        print(name)\n\n    inner()\n\nouter()\n```\n上面的题目很简单，因为inner函数本身有name变量，所以打印结果是mary。那么下面这个呢？\n```python\nname ='jack'\n\ndef f1():\n    print(name)\n\ndef f2():\n    name = 'eric'\n    f1()\n\nf2()\n```\n\n这题有点迷惑性，想了半天，应该是‘eric’吧，因为 f2 函数调用的时候，在内部又调用了 f1 函数，f1 自己没有 name 变量，那么就往外找，发现 f2 定义了个 name，于是就打印这个 name。错了！！！结果是‘jack’！\n\n**Python函数的作用域取决于其函数代码块在整体代码中的位置，而不是调用时机的位置**。调用 f1 的时候，会去 f1 函数的定义体查找，对于 f1 函数，它的外部是 name ='jack'，而不是 name = 'eric'。\n\n再看下面的例子，f2 函数返回了 f1 函数：\n\n```python\nname = 'jack'\n\ndef f2():\n    name = 'eric'\n    return f1\n\ndef f1():\n    print(name)\n\nret = f2()\nret()\n```\n\n仔细回想前面的例子，其实这里有异曲同工之妙，所以结果还是‘jack’。"}, "/news/others/busybox_related/busybox_related.html": {"title": "Ubuntu 下 busybox 的妙用", "content": "---\ntitle: Ubuntu 下 busybox 的妙用\nkeywords: Ubuntu, busybox, 小技巧, mount, nfs\ndate: 2022-07-08\ntags: busybox, Linux\n---\n\nLinux 下有不少好用的工具，BusyBox 就是其中的一个\n\n<!-- more -->\n\n## Busybox 简介\n\n作为一个开源 (GPL) 项目，Busybox 一个没令人失望。在 Linux 这么多年且这么多次的版本更新中，一些类似于 `devmem` 之类的命令被更改掉了，然后挂载 nfs 系统的命令 `mount nfs` 也不一定直接适用与新版本系统了，然而这些都可以通过 Busybox 来解决。\n\n## 安装 Busybox\n\n对于普通的 Linux 发行版可以直接使用命令行来安装。比如 Ubuntu 系统直接使用 `sudo apt install Busybox` 命令就能完成安装了，其他的版本可以自行寻找或编译对应的 Busybox。\n\n## 相关使用\n\n安装完 Busybox 后，可以直接执行 `busybox` 来查看是否有信息打印出来，比如执行完`busybox`指令后的部分信息打印如下：\n\n```bash\nBusyBox v1.30.1 (Ubuntu 1:1.30.1-4ubuntu6.4) multi-call binary.\nBusyBox is copyrighted by many authors between 1998-2015.\nLicensed under GPLv2. See source distribution for detailed\ncopyright notices.\n\nUsage: busybox [function [arguments]...]\n   or: busybox --list[-full]\n   or: busybox --show SCRIPT\n   or: busybox --install [-s] [DIR]\n   or: function [arguments]...\n```\n\n对于一些程序，出于安全或者性能因素，在新版本中修改了部分内容。想执行旧版软件里的相关功能的话，可以通过 busybox 来解决。\n\n下面举两个例子\n\n### devmem 与 devmem2\n\n对于 `devmem` 命令，在新版的都被 `devmem2` 所替代了，但是可以通过 busybox 来调用运行 `devmem` 命令。\n\n```bash\nhome:~$ devmem2 \n\nUsage: devmem2 { address } [ type [ data ] ]\n address : memory address to act upon\n type    : access operation type : [b]yte, [h]alfword, [w]ord\n data    : data to be written\n\n```\n\n```bash\nhome:~$ busybox devmem\nBusyBox v1.30.1 (Ubuntu 1:1.30.1-4ubuntu6.4) multi-call binary.\n\nUsage: devmem ADDRESS [WIDTH [VALUE]]\n\nRead/write from physical address\n\n ADDRESS Address to act upon\n WIDTH Width (8/16/...)\n VALUE Data to be written\n```\n\n### mount nfs\n\n对于较新版本的 `mount` 命令，在挂载 nfs 文件系统的时候会出错。\n\n```bash\nmount: /mnt/nfs: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.\n```\n这个时候使用 `busybox mount` 来替代 `mount` 来挂载 nfs 文件系统就不会再报错了。\n\n大概原因是因为系统默认为安装 nfs 的相关工具，这个时候使用 busybox 里面的 mount 可以节约大量的时间。"}, "/news/others/831_ssh/831_ssh.html": {"title": "使用 SSH 来连接 M2Dock", "content": "---\ntitle: 使用 SSH 来连接 M2Dock\nkeywords: MaixII, SSH\ndate: 2022-07-19\ntags: MaixII, ssh, adb\n---\n\nLinux 系统可以使用 ssh 远程被操作，但是对于 M2Dock 还有另一种用法\n\n<!-- more -->\n\n内容较多，对新手很有帮助\n\n## SSH 简介\n\nSSH 为 Secure Shell 的缩写，是建立在应用层基础上的安全协议，可以有效防止远程管理过程中的信息泄露问题。\n因为其易用且安全，所以被广泛使用。\n\n## 软件准备\n\n### MobaXterm\n\n这里使用 [MobaXterm](https://mobaxterm.mobatek.net/) 来作为主要软件进行示例。\n因为它免费且易用，所以就用它了。\n当然这里主要是使用它的 SSH 功能，其他诸多强大功能各位可以自行尝试\n\n#### 下载 MobaXterm\n点击这个链接 [https://mobaxterm.mobatek.net/download-home-edition.html](https://mobaxterm.mobatek.net/download-home-edition.html) 可以跳转到下载界面。\n这里我们选择 [Portable edition](https://download.mobatek.net/2212022060563542/MobaXterm_Portable_v22.1.zip) （携带版（下载后解压就可以用））\n<details>\n  <summary><font color=#4F84FF>点开查看下载页面截图</font></summary>\n  <img src=\"./assets/mobaxterm_protable_edition.png\" alt=\"Portable edition\">\n</details>\n下载解压后文件夹里是这样的:\n<details>\n  <summary><font color=#4F84FF>点开查看解压后文件夹</font></summary>\n  <img src=\"./assets/mobaxterm_snapshot.png\" alt=\"mobaxterm_snapshot\">\n</details>\n\n里面只有一个可执行的 mobaxterm 本体文件与一个插件文件\n\n### ADB\n\nM2Dock 带有 adb 功能。\n\n可以进行端口转发然后通过 USB 数据线与电脑进行 SSH 连接，比使用无线 SSH 连接更加稳定。\n\n如果曾经自行配置过 adb 那可以直接看[操作步骤](#操作步骤)\n\n### 下载 adb\n\n#### Linux 安装 adb\n\nUbuntu 用户可以直接使用包管理器来安装。\n\n在命令行终端执行 `sudo apt-get install android-tools-adb` 来安装 adb。\n\n且安装后能直接通过命令行调用 adb 程序。\n\n#### Windows 配置 adb\n\nWindows 用户可以在 [这里](https://developer.android.google.cn/studio/releases/platform-tools?hl=zh-cn) 下载 adb。\n\n![下载adb](./assets/download_adb.png)\n\n然后自行选择合适的地方解压文件，并把其文件路径复制一下：\n比如我这里是直接把可执行文件(adb.exe)所在的路径全都复制了，因此**我**此次所用路径为 `Y:\\platform-tools_r33.0.2-windows\\platform-tools` (仅举例用)\n![copy_path](./assets/copy_path.png)\n\n但是为了能够随时在命令行终端里使用 adb 命令我们需要把它添加到系统路径：\n\n- Windows10 和 Windows11 用户可以直接在任务栏的搜索里面输入 `path` 来打开系统属性\n- 其他版本可以通过点开下面的箭头来查看系统属性的方法：<details>\n  <summary><font color=#4F84FF>点开查看打开系统属性方法</font></summary>\n  右键此电脑然后选择最下面的属性选项 （鼠标右键 ① 处，然后鼠标左键点击弹出菜单的最下面的 ② 处的属性）\n  <img src=\"./assets/right_click.png\" alt=\"open system properties\">\n</details>\n\n然后在弹出的系统属性窗口中依次点击 ① 和 ② 处来打开环境变量\n![system properties](./assets/system_properties.png)\n\n在弹出的窗口中上面的一栏中双击名为 Path 的变量来对其进行编辑\n![path](./assets/path.png)\n\n双击空白处相当于添加一个条目，这里我们把之前复制的 adb 路径存入其中，\n比如对于我之前复制的路径为 `Y:\\platform-tools_r33.0.2-windows\\platform-tools`，因此在下面所所添加的内容就是 `Y:\\platform-tools_r33.0.2-windows\\platform-tools`，具体内容应当根据每个人的情况而定(不会有人和我一样的)\n![adb_path](./assets/add_path.png)\n\n接着单击所有已打开窗口的 OK 来保存配置。\n\n> 如果进行了错误的操作那就点击取消然后从头再来，系统环境是很危险的配置，不要随意删除\n\n然后就可以在命令行终端中执行一次 adb 来检查一下是否正确配置了\n  - 需要保存环境变量配置和系统属性后在进行这一步\n  - 环境变量配置和系统属性保存需要新打开一个命令行终端来执行 adb 指令\n\n![adb_command](./assets/adb.png)\n下面那一大堆就是相关的说明。但是在这里我们不太需要用，只要有对应的输出就行。\n\n## 操作步骤\n\n1. 将 M2Dock 通过 otg接口与电脑相连且已弹出 U盘\n2. 在命令行中执行 `adb forward tcp:22 tcp:22` 来进行端口映射\n   ![adb_forward](./assets/adb_forward.png)\n3. 运行 MobaXterm ，可按照下面的 gif 或者图文进行创建 ssh 会话操作\n<details>\n  <summary><font color=#4F84FF>点开查看创建 ssh 会话的 gif 操作</font></summary>\n  <img src=\"./assets/create_ssh.gif\" alt=\"create_ssh\">\n</details>\n<details>\n  <summary><font color=#4F84FF>点开查看创建 ssh 会话的图文版操作</font></summary>\n  单机左上角的 session 来创建会话\n  <img src=\"./assets/click_session.png\" alt=\"click_session\">\n  在弹出的窗口中选择 SSH\n  <img src=\"./assets/click_ssh.png\" alt=\"click_ssh\">\n  因为前面执行过 adb forward 指令，因此在 Remote host 输入 127.0.0.1 \n  <img src=\"./assets/remote_host.png\" alt=\"remote_host\">\n  点击下方 OK 就完成了 SSH 会话的创建，接着会要求我们输入用户名和密码，均为 root，自行输入即可\n  输入密码的时候是没有输入显示的，但是错误就会提示。因此个人多试几次就行\n  <img src=\"./assets/login.png\" alt=\"login\">\n  紧接着就成功登陆进来了\n  <img src=\"./assets/succeed_login.png\" alt=\"succeed_login\">  \n  点一下左下角的 Remote monitoring ，就能监视设备运行状态了\n  <img src=\"./assets/monitor.png\" alt=\"monitor\">\n</details>\n\n在成功使用 SSH 在 MobaXterm 软件中连上 M2Dock 后，可以看到在会话栏中看到设备里的文件。默认所查看的文件路径为当前用户的根目录。\n<img src=\"./assets/monitor.png\" alt=\"monitor\">\n\n我们可以手动拖入或者拖出文件或者文件夹来进行电脑与板子互传文件的操作\n当然也可以双击文件然后对其进行文本编辑:\n![Edit_file](./assets/edit_file.gif)\n\n## 其他\n\n### 开启自动保存\n\n需要注意的是点击保存后还要选择一下确认保存到文件才行\n比如出现下图的情况的话点击一次 Autosave 就可以让它以后不再弹出\n![autosave](./assets/autosave.png)\n\n### 关闭 MaixPy3 IDE\n\n使用 USB 数据线使电脑与板子进行 SSH 通信时，最好关闭电脑端的 MaixPy3 IDE，这样可以避免一些麻烦\n\n### 使用无线连接\n\n将板子联网之后可以通过命令行来得到其 IP。\n\n然后可以在新建 SSH 会话的时候把板子的 IP 填入到 Remote host 中。\n就可以通过无线网来使电脑与板子使用 SSH 进行通信\n\n![ifconfig](./assets/ifconfig.png)\n\n板子没能成功连接到无线网就不会被分配 IP\n\n### adb forward\n\nadb forward 的功能是建立一个端口转发，比如 `adb forward tcp:11111 tcp:22222` 的是将 PC 端的11111 端口收到的数据，转发给到 adb 设备的 22222 端口。\n\nSSH 默认的端口为 22 ，因此使用 `adb forward tcp:22 tcp:22` 是将电脑端的 22 端口转发到 adb 设备的 22 端口。\n例如使用  `adb forward tcp:1145 tcp:22` 命令的话，需要把配置 IP 处的端口修改一下:\n![port](./assets/port.png)"}, "/news/others/tinymaix_cnx/tinymaix_cnx.html": {"title": "TinyMaix ：超轻量级推理框架", "content": "---\ntitle: TinyMaix ：超轻量级推理框架\nkeywords: TinyMaix, Sipeed, 框架, 机器学习\ndate: 2022-08-24\ntags: TinyMaix, 推理框架\n---\n\n<!-- more -->\n\n## 介绍\n\nTinyMaix 是面向单片机的超轻量级的神经网络推理库，即 TinyML 推理库，可以让你在任意单片机上运行轻量级深度学习模型。\n\n**关键特性**\n- 核心代码少于 **400行**(`tm_layers.c`+`tm_model.c`+`arch_cpu.h`), 代码段(.text)少于**3KB**   \n- 低内存消耗，甚至 **Arduino ATmega328** (32KB Flash, 2KB Ram) 都能基于 TinyMaix 跑 mnist(手写数字识别)\n- 支持 **INT8/FP32/FP16** 模型，实验性地支持 **FP8** 模型，支持 keras h5 或 tflite 模型转换 \n- 支持多种芯片架构的专用指令优化: **ARM SIMD/NEON/MVEI，RV32P, RV64V** \n- 友好的用户接口，只需要 load/run 模型~\n- 支持全静态的内存配置(无需 malloc )\n- 即将支持 [MaixHub](https://maixhub.com) **在线模型训练** \n\n**在Arduino ATmega328上运行 mnist demo 实例**\n```\nmnist demo\n0000000000000000000000000000\n0000000000000000000000000000\n0000000000000000000000000000\n000000000077AFF9500000000000\n000000000AFFFFFFD10000000000\n00000000AFFFD8BFF70000000000\n00000003FFD2000CF80000000000\n00000004FD10007FF40000000000\n00000000110000DFF40000000000\n00000000000007FFC00000000000\n0000000000004FFE300000000000\n0000000000008FF9000000000000\n00000000000BFF90000000000000\n00000000001EFE20000000000000\n0000000000CFF800000000000000\n0000000004FFB000000000000000\n000000001CFF8000000000000000\n000000008FFA0000000000000000\n00000000FFF10000000000000000\n00000000FFF21111000112999900\n00000000FFFFFFFFA8AFFFFFFF70\n00000000AFFFFFFFFFFFFFFA7730\n0000000007777AFFF97720000000\n0000000000000000000000000000\n0000000000000000000000000000\n0000000000000000000000000000\n0000000000000000000000000000\n0000000000000000000000000000\n===use 49912us\n0: 0\n1: 0\n2: 89\n3: 0\n4: 1\n5: 6\n6: 1\n7: 0\n8: 0\n9: 0\n### Predict output is: Number 2, prob=89\n```\n\n## TODO \n1. 将 `tm_layers.c` 优化到 `tm_layers_O1.c`, 目标提升速度到 `1.4~2.0X`\n2. 针对 64/128/256/512KB 内存限制，找到合适的骨干网络\n3. 增加例程：Detector,KWS,HAR,Gesture,OCR,...\n4. ...\n\n如果想参与进 TinyMaix 的开发，或者想与 TinyML 爱好者交流，   \n请加入 telegram 交流群：https://t.me/tinymaix  \n\n\n\n## TinyMaix 设计思路\n\nTinyMaix 是专为低资源的单片机所设计的 AI 神经网络推理框架，通常被称为 **TinyML**  \n\n现在已经有很多 TinyML 推理库，比如 TFLite micro, microTVM, NNoM, 那为什么又捏了 TinyMaix 这个轮子呢?   \n\nTinyMaix 是两个周末业余时间完成的项目，所以它足够简单，可以再30分钟内走读完代码，可以帮助TinyML新手理解它是怎么运行的。\n\nTinyMaix 希望成为一个足够简单的 TinyML 推理库，所以它放弃了很多特性，并且没有使用很多现成的 NN 加速库，比如 CMSIS-NN\n\n在这个设计思路下，TinyMaix 只需要5个文件即可编译~\n\n我们希望 TinyMaix 可以帮助任何单片机运行 AI 神经网络模型, 并且每个人都能移植 TinyMaix 到自己的硬件平台上~\n\n> 注意：虽然 TinyMaix 支持多架构加速，但是它仍然需要更多工作来平衡速度和尺寸\n\n### 设计特性\n- [x] 最高支持到 mobilenet v1, RepVGG 的骨干网络\n  - 因为它们对单片机来说是最常用的，最高效的结构\n  - [x] 基础的 Conv2d, dwConv2d, FC, Relu/Relu6/Softmax, GAP, Reshape\n  - [ ] MaxPool, AvgPool (现在使用 stride 代替)\n- [x] FP32 浮点模型, INT8 量化模型, **FP16**半精度模型\n- [x] 转换 keras h5 或 tflite 到 tmdl\n  - 简单模型使用keras/tf训练已经足够\n  - 复用了tflite现成的量化功能\n- [x] 模型统计功能\n  - 可选以减少代码尺寸\n\n### 可考虑添加的特性\n- [ ] INT16 量化模型\n  - 优点: \n    - 更精确\n    - 对于 SIMD/RV32P 指令加速更友好\n  - 缺点: \n    - 占用了 2 倍的 FLASH/RAM\n- [ ] Concat 算子\n  - 优点: \n    - 支持 mobilenet v2, 模型精度更高\n  - 缺点: \n    - 占用了 2 倍的 RAM\n    - concat 张量占用了更多时间，使得模型运算变慢\n    - 需要更多转换脚本工作转换分支模型到扁平结构\n- [ ] Winograd 卷积优化\n  - 优点:\n    - 可能加速卷积计算\n  - 缺点: \n    - 增加了 RAM 空间和带宽消耗\n    - 增大了代码段(.text)尺寸\n    - 需要很多变换，弱单片机可能会消耗更多时间\n    \n### 不考虑添加的特性\n- [ ] BF16 模型\n  - 多数单片机不支持 BF16 计算\n  - 精度不会比 INT16 高太多\n  - 占用了 2 倍的 FLASH/RAM\n- [ ] AVX/vulkan 加速\n  - TinyMaix 是为单片机设计的，所以不考虑电脑/手机的支持\n- [ ] 其他多样化的算子\n  - TinyMaix 仅为单片机提供基础模型算子支持，如果你需要更特殊的算子，可以选择 TFlite-micro/TVM/NCNN... \n\n## 例程体验\n\n### mnist\nMNIST 是手写数字识别任务，简单到以至于可以在 ATmega328 这样的 8 位单片机上运行。  \n在电脑上测试： \n```\ncd examples/mnist\nmkdir build\ncd build \ncmake ..\nmake\n./mnist\n```\n\n### mbnet\nmbnet (mobilenet v1) 是适用于移动手机设备的简单图像分类模型，不过对单片机来说也稍微困难了些。\n例程里的模型是 mobilenet v1 0.25，输入 128x128x3 的RGB图像，输出 1000 分类的预测\n它需要至少 128KB SRAM 和 512KB Flash, STM32F411 是典型可以运行该模型的最低配置。\n\n在 PC 上测试运行 mobilenet 1000分类图片例程\n```\ncd examples/mbnet\nmkdir build\ncd build \ncmake ..\nmake\n./mbnet\n```\n\n## 如何使用 (API)\n### 加载模型\n```\ntm_err_t tm_load  (tm_mdl_t* mdl, const uint8_t* bin, uint8_t*buf, tm_cb_t cb, tm_mat_t* in);   \n```\nmdl: 模型句柄;   \nbin: 模型bin内容;   \nbuf: 中间结果的主缓存；如果NULL，则内部自动malloc申请；否则使用提供的缓存地址\ncb: 网络层回调函数;   \nin: 返回输入张量，包含输入缓存地址 //可以忽略之，如果你使用自己的静态输入缓存\n\n### 移除模型\n```\nvoid     tm_unload(tm_mdl_t* mdl);                         \n```\n### 输入数据预处理\n```\ntm_err_t tm_preprocess(tm_mdl_t* mdl, tm_pp_t pp_type, tm_mat_t* in, tm_mat_t* out);              \n```\nTMPP_FP2INT    //用户自己的浮点缓存转换到int8缓存\nTMPP_UINT2INT  //典型uint8原地转换到int8数据；int16则需要额外缓存\nTMPP_UINT2FP01 //uint8转换到0~1的浮点数 u8/255.0  \nTMPP_UINT2FPN11//uint8转换到-1~1的浮点数\n\n### 运行模型\n```\ntm_err_t tm_run   (tm_mdl_t* mdl, tm_mat_t* in, tm_mat_t* out);\n```\n\n\n## 如何移植\n\nTinyMaix的核心文件只有这5个：`tm_model.c`, `tm_layers.c`, `tinymaix.h`, `tm_port.h`, `arch_xxx.h`  \n\n如果你使用没有任何指令加速的普通单片机，选择 `arch_cpu.h`, 否则选择对应架构的头文件 \n\n然后你需要编辑 `tm_port.h`，填写你需要的配置，所有配置宏后面都有注释说明 \n\n注意 `TM_MAX_CSIZE`,`TM_MAX_KSIZE`,`TM_MAX_KCSIZE` 会占用静态缓存。\n\n最后你只需要把他们放进你的工程里编译~\n\n## 怎样训练/转换模型\n\n在 examples/mnist 下有训练脚本可以学习如何训练基础的mnist模型\n\n> 注意：你需要先安装TensorFlow (>=2.7) 环境.\n\n完成训练并保存h5模型后，你可以使用以下脚本转换原始模型到 tmdl 或者 c 头文件。 \n\n1. h5_to_tflite.py   \n  转换 h5 模型到浮点或者 int8 量化的 tflite 模型\n  python3 h5_to_tflite.py h5/mnist.h5 tflite/mnist_f.tflite 0   \n  python3 h5_to_tflite.py h5/mnist.h5 tflite/mnist_q.tflite 1 quant_img_mnist/ 0to1   \n2. tflite2tmdl.py\n  转换 tflite 文件到 tmdl 或者 c 头文件   \n  python3 tflite2tmdl.py tflite/mnist_q.tflite tmdl/mnist_q.tmdl int8 1 28,28,1 10  \n```\n================ pack model head ================\nmdl_type   =0\nout_deq    =1\ninput_cnt  =1\noutput_cnt =1\nlayer_cnt  =6\nbuf_size   =1464\nsub_size   =0\nin_dims    = [3, 28, 28, 1]\nout_dims   = [1, 1, 1, 10]\n================   pack layers   ================\nCONV_2D\n    [3, 28, 28, 1] [3, 13, 13, 4]\n    in_oft:0, size:784;  out_oft:784, size:680\n    padding valid\n    layer_size=152\nCONV_2D\n    [3, 13, 13, 4] [3, 6, 6, 8]\n    in_oft:784, size:680;  out_oft:0, size:288\n    padding valid\n    layer_size=432\nCONV_2D\n    [3, 6, 6, 8] [3, 2, 2, 16]\n    in_oft:0, size:288;  out_oft:1400, size:64\n    padding valid\n    layer_size=1360\nMEAN\n    [3, 2, 2, 16] [1, 1, 1, 16]\n    in_oft:1400, size:64;  out_oft:0, size:16\n    layer_size=48\nFULLY_CONNECTED\n    [1, 1, 1, 16] [1, 1, 1, 10]\n    in_oft:0, size:16;  out_oft:1448, size:16\n    layer_size=304\nSOFTMAX\n    [1, 1, 1, 10] [1, 1, 1, 10]\n    OUTPUT!\n    in_oft:1448, size:16;  out_oft:0, size:56\n    layer_size=48\n================    pack done!   ================\n    model  size 2.4KB (2408 B) FLASH\n    buffer size 1.4KB (1464 B) RAM\n    single layer mode subbuff size 1.4KB (64+1360=1424 B) RAM\nSaved to tmdl/mnist_q.tmdl, tmdl/mnist_q.h\n```\n\n现在你有了 tmdl 或者 C 头文件，把它放到你的工程里编译吧~\n\n## 使用 Maixhub 在线训练模型\n\nTODO\n\n## 怎样添加新平台的加速代码\n\nTinyMaix 使用基础的点积函数加速卷积运算   \n你需要在 src 里添加 arch_xxx_yyy.h, 并添上你自己平台的点积加速函数：\n```\nTM_INLINE void tm_dot_prod(mtype_t* sptr, mtype_t* kptr,uint32_t size, sumtype_t* result);\n```\n\n\n## 贡献/联系\n\n如果你需要向TinyMaix贡献代码，请先阅读“TinyMaix设计思路”一节，我们只需要“设计内的特性”和“可考虑添加的特性”。\n\n如果你想要提交你的移植测试结果，请提交到 benchmark.md.\n\n我们非常欢迎你移植 TinyMaix 到自己的芯片/板子上，这会证明使用 TinyMaix 运行深度学习模型是非常容易的事情~\n\n如果你对 TinyMaix 的使用和移植有问题，可以在此仓库提交 Issues。\n\n如果你有商业或私有项目咨询，你可以发邮件到 support@sipeed.com 或 zepan@sipeed.com (泽畔)."}, "/news/index.html": {"title": "动态", "content": "---\n\ntitle: 动态\nkeywords: teedoc, 博客生成, 静态博客\ndesc: teedoc 静态博客页面生成\nshow_source: false\ndate: true\n\n---\n<div id=\"blog_list\"></div>"}, "/news/MaixPy/star_maixpy.html": {"title": "MaixPy 上手指南（避坑）之上手篇", "content": "---\ntitle: MaixPy 上手指南（避坑）之上手篇\nkeywords: MaixPy, K210, Python, MicroPython\ndesc: MaixPy 上手指南（避坑） 之上手篇\ndate: 2022-04-01\ntags: MaixPy, K210\n---\n\n> 作者：Ray（Rui）\n\n拿到热乎的 K210 开发板，如何上手使用。我接触了许许多多的小白开发者后，整理出来的资料和路线，希望可以减少你们遇到的问题，可以更加愉快的使用 K210 进行自己的项目开发。\n\n<!-- more -->\n\n## K210 开发板\n\n市面上有很多中关于 K210 的开发板，但是并不是所有的开发板都是可以使用 MaixPy 进行开发的。毕竟不同厂商使用的摄像头、屏幕、引脚上使用，都是由差异性的。目前支持的使用 MaixPy 开发的板子有 Sipeed 家的 [Maix 系列](/hardware/zh/maix/index.html)。\n\n如果是试用别家的开发板，并不能很好的兼容 MaixPy，存在差异性。\n\n## 开箱\n\n拿到开发板，首先需要根据屏幕和摄像头排线上的丝印提示来安装好，即排线上的数字 “1” 和板子卡座边上引脚丝印 “1” 方位对应接上。上电之后，屏幕上会显示出一个红色的界面这是开发板已经正常启动了。（也可能存在部分丝印印反）\n\n### 首先要安装开发环境：\n\n1. [【安装驱动】](/soft/maixpy/zh/get_started/env_install_driver.html) 根据自己使用的开发进行选择需要按安装驱动\n2. [【更新固件】](/soft/maixpy/zh/get_started/upgrade_maixpy_firmware.html) 确保使用的是最新版本的固件，并学习一下每个固件之间的[差异](/soft/maixpy/zh/get_started/upgrade_maixpy_firmware.html#固件命名说明)\n3. [【安装 MaixPy IDE】](/soft/maixpy/zh/get_started/env_maixpyide.html)\n\n如果安装驱动的时候出现安装失败，或者是安装驱动之后，电脑上没有显示 COM 口的，就需要更新一下系统或者是检查一下自己的系统是不是正版的了。因为有部分的盗版系统安装不上驱动，或者是安装驱动之后并显示。或者通过换 USB 口进行连接，也许就可以检测到开发板\n\n### 运行代码检测摄像头\n\n将开发板接到电脑上，打开 MaixPy IDE，运行打开的例程代码，检查自己的屏幕和摄像头是否正确连接上了。如果运行例程代码之后，并没有图像出现来屏幕和 IDE 上时，可能摄像头接反了。\n\n## 开始学习使用\n\n开始使用 K210 之前，一定要学习 Python，如果你连 Python 都不会的，就不要继续往下走，可以快速的过一遍 [Python](/soft/maixpy3/zh/origin/python.html) 的语法和使用，一定要会 Python !一定要会 Python !一定要会 Python !\n\n现在就当你懂 Python 了，这是就可以开始看 MaixPy 文档中的入门指南，进行对于 MaixPy 的使用和 K210 的基本了解。\n\n【更多功能应用】中将有 MaixPy 更多的使用案例和使用方式，一定要确保自己已经对应入门教程中内容已经了解和掌握了再去看，否则你在学习的时候还是会一脸懵逼，不知所云。\n\n## 获取 AI 模型文件\n\n在【更多功能应用】中是有讲述如何运行神经网络模型，也知道怎么去获取示例中的模型文件，但是少了如何获取机器码这个操作，这里详细的讲述一下何如获取机器码。\n\n1. 将 [key_gen.bin](https://dl.sipeed.com/fileList/MaixHub_Tools/key_gen_v1.2.bin) 这个固件通过 Kflash 烧录到开发板上。烧录这个机器码固件之后，开发板是处于一个不能使用的状态，上电屏幕只会变成一个白屏。\n2. 这时将开发板通过 USB 连接到电脑上，利用[【串口连接】](/soft/maixpy/zh/get_started/env_serial_tools.html)中的方式来连接开发板。注：IDE 中的串口终端和 IDE 的连接方式相对独立的，而且串口不能通过多种方式进行连接\n3. 利用串口软件连接上开发板，这时按下开发板上的 reset 的按键，就会出现一串字符在终端窗口上，这就机器码。如果机器码\n\n> 推荐使用 IDE 中的 串口终端进行查看，这个相对别的软件更加适合 K210\n\n机器码是一机一码的一种加密方式，用于模型文件的加密。如果使用别的机器码去加密或者下载以 smodel 为文件后缀的模型文件，开发板是无法使用该模型文件的。"}, "/news/MaixPy/kmodel_datastruct.html": {"title": "K210 kmodel 模型储存数据结构", "content": "---\ntitle: K210 kmodel 模型储存数据结构\nkeywords: K210, kmodel\ndate: 2022-06-09\ntags: K210, kmodel\n---\n\nK210 kmodel 模型储存结构\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/307 有改动\n\n## K210 kmodel 简介\n\nV3 由 nncase v0.1.0 RC5 转换而来\nV4 由 nncase v0.2.0 从 tflite 转换而来\n\nV4相比于V3, 支持了更多算子, 但是运算速度更慢, 部分算子使用了CPU运算, K210侧也使用了C++编写, 部分库会拉低速度, 如果你需要移植或者优化可以注意这一点\n\n## kmodel V3 数据结构\n| 头                  | 输出层信息                        | 各层的头                                 | 各个层数据内容 |\n| ------------------- | --------------------------------- | ---------------------------------------- | -------------- |\n| kpu_kmodel_header_t | kpu_model_output_t * output_count | kpu_model_layer_header_t * layers_length | layers_body    |\n\n这里有个注意点, kpu_kmodel_header_t 没有8字节对齐, 所以在第一层的数据实际是保存在8字节对齐处, 比如前面所有header 长度为 228 字节, 那么 第一层数据中, 头 kpu_model_conv_layer_argument_t 占用24个字节, 228+24 不是8的整数倍, 所以层数据保存在 228+24+4 处, 所以在 kpu_model_conv_layer_argument_t 中用了 layer_offset 这个来表示层数据相对于模型起始地址的偏移\n\n```c\ntypedef struct\n{\n    uint32_t version;     // 固定  0x00000003, 0x03在低地址\n    uint32_t flags;       // 最低位 为1, 表示8bit模式\n    uint32_t arch;\n    uint32_t layers_length;\n    uint32_t max_start_address;\n    uint32_t main_mem_usage;\n    uint32_t output_count;\n} kpu_kmodel_header_t;\ntypedef struct\n{\n    uint32_t address;\n    uint32_t size;\n} kpu_model_output_t;\ntypedef struct\n{\n    uint32_t type;\n    uint32_t body_size;\n} kpu_model_layer_header_t;\n```\n\n## kmodel V4 数据结构\n| 头                    | 输入                     | 输入形状                    | 输出                      | 常量          | 各层的头                        | 各个层数据内容 |\n| --------------------- | ------------------------ | --------------------------- | ------------------------- | ------------- | ------------------------------- | -------------- |\n| struct modelv4_header | memory_range\\*hdr.inputs | runtime_shape_t\\*hdr.inputs | memory_range\\*hdr.outputs | hdr.constants | hdr.nodes \\* struct node_header | nodes content  |\n\n```c\nstruct modelv4_header\n{\n    uint32_t identifier; // 固定为 KMDL, L在低地址\n    uint32_t version;    // 固定为 0x00000004, 0x04 在低位\n    uint32_t flags;\n    uint32_t target;     // CPU: 0, K210: 1\n    uint32_t constants;  // 多少个 uint_t 类型的常量\n    uint32_t main_mem;   // 主内存, 用于AI, 运行时会先把输入的数据拷贝到这里\n    uint32_t nodes;\n    uint32_t inputs;     // input size\n    uint32_t outputs;    // output size\n    uint32_t reserved0;\n};\nstruct node_header\n{\n    uint32_t opcode;\n    uint32_t size;\n};\nstruct memory_range\n{\n    memory_type_t memory_type;\n    datatype_t datatype;\n    uint32_t start;\n    uint32_t size;\n};  // 16 Bytes\ntypedef enum _datatype\n{\n    dt_float32,\n    dt_uint8\n} datatype_t;\ntypedef enum _memory_type\n{\n    mem_const,\n    mem_main,\n    mem_k210_kpu\n} memory_type_t;\nusing runtime_shape_t = std::array<int, 4>;\n```"}, "/news/MaixPy/mind_application/mind_application.html": {"title": "Maixduino 如何实现积木编程", "content": "---\ntitle: Maixduino 如何实现积木编程\nkeywords: K210, Maixduino, Mind++\ndate: 2022-08-18\ntags: K210, Maixduino, Mind++\n---\n\n快来体验 Maixduino 不一样的有趣玩法\n\n<!-- more -->\n\n[原文链接](https://mindplus.dfrobot.com.cn/maixduino)\n\n## 说明\n\nMind+ 从1.6.6版本开始支持基于 K210 主控的 Maixduino 开发板，可满足对于 K210 开发有兴趣的用户。\n![mind-1](assets/mind-1.jpg)\n\n## 使用流程\n\n### 器材准备\n\n- Mind+ 1.6.6及以上版本\n- Maixduino主控板\n- Type-C数据线\n\n### 环境准备\n\n1. 打开 Mind+ 切换至上传模式，扩展库中选择主控板下的 Maixduino 后返回主界面。\n![mind-2](assets/mind-2.jpg)\n![mind-3](assets/mind-3.jpg)\n\n1. 使用 USB 线连接主控板与电脑，设备菜单中会出现两个 COM 口，选择其中 Maixduino 的 COM 口，软件会自动烧录固件，右下角小黑窗会显示提示语。\n\n|              |                                                                  |       |\n| :----------- | :----------------------------------------------------------------- | :--- |\n| 打开设备管理器       |  打开系统的设备管理器，方便检查端口或驱动问题。                      |      |\n| 一键安装串口驱动       |  如果首次使用软件板子没有出现 COM 口，则可以安装驱动。                                                |      |\n| 恢复设备初始设置       |   首次使用 Mind+ 或出现异常情况时，可以使用恢复设备初始设置功能擦除板子内的固件,<br>使用时需要先选择 COM 口再选择此功能，恢复完成后手动断开连接再选择端口。                                 |      |   |\n\n注意\n\n- 如果是首次使用需要选择 ESP32 (网卡)的 COM 口然后选择恢复设备初始设置功能更新网卡，否则可能会出现模拟输入功能无法使用。\n- Mind+ 中的固件与 Maixduino 官方固件不同，可以线选择 Maixduino 的 COM 口然后选择恢复设备初始设置，然后断开再次连接，即可自动刷入 Mind+ 固件。\n![mind-4](assets/mind-4.jpg)\n\n### 编程使用\n\n编写一个程序，在屏幕中显示摄像头画面，完成后点击运行，程序即可运行，板子上就可以看到效果了。\n![mind-5](assets/mind-5.jpg)\n![mind-6](assets/mind-6.jpg)\n\n## 手动编辑\n\n手动编辑模式中可以手动编写代码，需要先在文件系统新建文件然后打开，再保存，运行程序需要右键选择运行。\n![mind-7](assets/mind-7.jpg)\n\n## 固件说明\n\nMaixduino 有各种版本的固件，Mind+ 图形化部分为保持积木生成代码的稳定，内置的固件包含了图形化需要的库，同时为保持 Maixduino 使用的灵活性，Mind+ 也支持使用其他固件。\n\n### 如何区分是否为 Mind+ 内置固件？\n\n1. 如下图，终端中的输出信息以 Maixduino 开头的即为 Mind+ 内置固件。\n![mind-8](assets/mind-8.jpg)\n2. 如下图，终端中输出信息为 MiaxPy 或者其他则不是 Mind+ 内置固件，则这些固件可能会缺失图形化中的某些功能导致无法使用，如果需要使用图形化的功能请刷入 Mind+ 内置固件。\n![mind-9](assets/mind-9.jpg)\n\n### Mind+ 中固件烧录逻辑说明\n\n选择 Maixduino 的 COM 口之后，软件会检测板子中是否有固件，如果有固件（不论是 Mind+ 内置的固件或第三方均为有固件）则连接终端开始使用，如果没有则自动烧录固件选择区所选择的固件。\n  \n### 因此如果需要烧录第三方固件，操作方法为：\n\n1. 选择 Maixduino 的 COM 口，选择恢复设备初始设置（这个功能会擦除板子 flash 固件会被擦除）.\n2. 在选择固件处点击本地加载，加载想烧录的第三方固件文件，然后点击加载的固件，然后再次选择 Maixduino 的 COM 口，此时因为固件被擦除了就会烧录这个新的固件，等待烧录完成即可。\n3. 注意：因 Mind+ 积木生成的代码是固定的，因此如果烧录了第三方固件可能会出现部分功能无法使用（例如引脚映射文件在新的固件下没有会出错）此时可以使用手动编辑功能直接使用代码，如果继续想使用图形化，则可以使用开放扩展库自己写图形化积木扩展实现，教程[点击](https://mindplus.dfrobot.com.cn/extensions-user).\n![mind-10](assets/mind-10.jpg)\n\n### 同理如果要从第三方固件切换为 Mind+ 内置固件，操作方法为：\n\n1. 选择 Maixduino 的 COM 口，选择恢复设备初始设置（这个功能会擦除板子 flash，固件会被擦除）。\n2. 在选择固件处点击官方固件，然后再次选择 Maixduino 的 COM 口，此时因为固件被擦除了就会烧录 Mind+ 内置的官方固件，等待烧录完成即可。\n![mind-11](assets/mind-11.jpg)\n\n## 教程\n\n**社区用户驴友花雕的系列教程**\n- [【花雕测评】【AI】Mind+文字图片显示、呼吸灯及网络应用的22项小实验](https://makelog.dfrobot.com.cn/article-311386.html)\n- [【花雕测评】【AI】Mind+机器视觉之数字图像处理和显示的22种小测试](https://makelog.dfrobot.com.cn/article-311405.html)\n- [【花雕测评】【AI】Mind+机器视觉之颜色、维码与形状识别的8个小实验](https://makelog.dfrobot.com.cn/article-311417.html)\n\n**社区用户 hockel 的系别教程**\n- [【mind+ 玩转MAIXDUINO 系列0】 工欲善其事，必先利其器](https://mc.dfrobot.com.cn/thread-307857-1-1.html)\n- [【Mind+ 玩转Maixduino系列1】你好，世界](https://mc.dfrobot.com.cn/thread-307857-1-1.html)\n- [【mind+ Maixduino用户库】NES 游戏扩展库 【mind+ 用户库】Maixduino 中文字模、图片英文显示](https://mc.dfrobot.com.cn/thread-308037-1-1.html)\n\n**社区用户 DFByaoZQN5E 的系列教程**\n- [[教程]mind+ k210主板第一课 hello word!](https://mc.dfrobot.com.cn/thread-307820-1-1.html)\n- [[教程]mind+ k210主板第二课 gpio(1)](https://mc.dfrobot.com.cn/thread-307850-1-1.html)\n- [[教程]mind+ k210 第三课 gpio (2)](https://mc.dfrobot.com.cn/thread-307877-1-1.html)\n- [[教程]mind+ k210 第四课 gpio (3)](https://mc.dfrobot.com.cn/thread-307969-1-1.html)\n\n**社区用户肥罗-阿勇的教程**\n- [Mind+Maixduino应用案例集合](https://mc.dfrobot.com.cn/thread-307946-1-1.html)\n\n**社区用户 hmilycheng 的系列教程**\n- [Maixduino轻松学系列 —— （1）初识Maixduino](https://makelog.dfrobot.com.cn/article-311375.html)\n- [Maixduino轻松学系列 —— （2）Mind+带你畅玩经典红白机游戏](https://makelog.dfrobot.com.cn/article-311392.html)\n- [Maixduino轻松学系列 —— （3）基于Mind+的简易NTP网络时钟](https://makelog.dfrobot.com.cn/article-311401.html)\n- [Maixduino轻松学系列 —— （4）基于Mind+的图像识别：人脸检测](https://makelog.dfrobot.com.cn/article-311411.html)\n- [Maixduino轻松学系列 —— （5）有屏幕的地方就有BAD APPLE](https://makelog.dfrobot.com.cn/article-311418.html)\n- [Maixduino轻松学系列 —— （6）基于ASR语音识别控制红绿灯](https://makelog.dfrobot.com.cn/article-311420.html)\n- [Maixduino轻松学系列 —— （7）超声波传感器的认识与使用](https://makelog.dfrobot.com.cn/article-311425.html)\n\n## 注意事项 & FAQ\n\n0. 断电重启之后，按一下板子上的 RESET 键即可启动程序。\n1. 如果是首次使用需要选择 ESP32 (网卡)的 COM 口然后选择恢复设备初始设置功能更新网卡，否则可能会出现模拟输入功能没有读值的情况。\n2. 涉及到 SD 卡读取或存储的操作，需要插入内存卡（断电后插拔内存卡）才可正常使用。\n![mind-13](assets/mind-13.jpg)\n![mind-12](assets/mind-12.jpg)\n3. 不支持的硬件可以使用用户库功能自行添加，详情查看官方文档中自定义用户库教程。\n4. 人工智能项目需要导入模型文件，请将模型文件拷贝到内存卡目录下。\n[下载连接](https://pan.baidu.com/share/init?surl=dTB0UHRKVrCtS4cMrxgDhQ) 提取码：mind\n5. 注意变量名字不要与内置的库名字重复，例如不要命名变量为 image\n![mind-14](assets/mind-14.jpg)\n\n6. FAQ\n\n|  问题        |   解决思路                                                       |       |\n| :----------- | :----------------------------------------------------------------- | :--- |\n|  在执行有模型的 AI 程序时出现 out of memory 或者 memory not enough 等内存不足的错误怎么办？                       |  程序使用的模型对应的固件可能不是 Mind+ 内置的，尝试找到对应的固件或[寻找maixpy的固件](https://mc.dfrobot.com.cn/thread-308995-1-1.html)。                                                                   |      | \n|  报错 kpu: img w=xxx,h=xxx, but model w=xxx,h=xxx kpu: check img format err! 怎么办？                      |  使用的 kmodel 模型文件训练的时候的分辨率与程序中使用的摄像头分辨率不同，尝试修改相机设置窗口。                                                                      |      |     \n| 提示 no module named 'pin' 错误怎么办？                     |   说明固件不是 Mind+ 中的，如果需要使用 Mind+ 内置固件，则选择 Maixduino 的 COM 口,恢复设备初始设置擦除固件，然后断开再次连接即会自动刷入 Mind+ 提供的固件。如果依然要使用非 Mind+ 内置固件，则需要导入对应依赖库，[点击查看教程](https://mc.dfrobot.com.cn/thread-309510-1-1.html)。                                                                   |      |      \n|  使用 WiFi 连接功能时提示 hard spi Get version fail hard spi 或卡住怎么办？<br>![mind-15](assets/mind-15.jpg)                     |  用到网络功能时需要插上内存卡再使用。                                                                     |      |"}, "/news/MaixPy/K210_kflash_ISP_download_progress.html": {"title": "K210 kflash ISP 下载程序流程", "content": "---\ntitle: K210 kflash ISP 下载程序流程\nkeywords: K210, kflash, ISP\ndate: 2022-06-09\ntags: K210, kflash\n---\n\nK210详细的的程序下载流程，包括芯片侧和kflash侧\n\n<!-- more -->\n\n版权声明：本文为 neucrack 的原创文章，遵循 CC 4.0 BY-SA 版权协议，转载请附上原文出处链接及本声明。\n原文链接：https://neucrack.com/p/312 有改动\n\n## 术语\n\n- ISP: In System Programming, 在系统编程\n\n## kflash 下载流程\n\n> 带 || 开头的是芯片侧的操作\n\n- || 芯片拷贝 `boot rom` (特定的硬件，一次性写入)中的 boot 程序到内存末尾1()，并调用运行这个 boot 程序\n\n- || boot 程序运行后， 从 otp 读取信息来判断是否需要从 top 中读取新的 boot，因为 boot rom 区域是一次性写入的，如果 boot 写出了 bug，可以用 otp 中写入新的 boot 来挽救，相当于芯片出厂有两次写入 boot 的机会。而事实是 k210 确实用上了这个功能\n\n- || 如果需要使用 otp 中的新 boot，则读取到内存末尾，但是需要在现在正在运行的 boot 前面，比如之前的末尾1是倒数16k，那么这个 otp 的 boot 就需要写到之前,比如倒数32k到倒数16k位置，然后跳转执行这个新的 boot\n\n- || boot 程序判断 boot 引脚是否被拉低，没被拉低则进入正常启动模式，读取整个固件到内存，然后启动，否则进入ISP模式\n\n- 上位机打开串口\n\n- 上位机通过串口的 dtr rts 来设置 boot 和 reset 引脚，保持拉低 boot引脚，然后拉低reset引脚再拉高reset引脚， 即让芯片重启的时候保持boot引脚为低电平\n\n- || 芯片boot程序检测，如果boot引脚被拉低了，则进入 ISP 模式(输入boot程序的一部分)\n\n- 上位机向芯片发送握手信号(b'\\xc0\\xc2\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xc0')，然后等待响应信号\n\n- 默认芯片是115200波特率，如果需要更高波特率，发送修改波特率命令给芯片，芯片程序(boot)修改通信波特率\n\n- 向芯片发送写 ramrum 程序的命令，并将需要 RAMRUM 的程序或者新的 ISP 程序发过去，写到芯片内存的起始地址\n\n注意到，这里没有直接发送要写入到flash的程序过去让boot里面的ISP程序去写入到Flash，而是重新发送了一份ISP程序过去，后面会运行这份新的ISP程序来从串口获取固件并烧录到flash，这可以说是为了更灵活，可以自定义ISP程序，而且ISP程序的大小只要小于前面的 boot 和 otp 的新boot的大小之和就行，boot里面做这些事可能面临boot程序过大或者有bug后期更新的问题。但是代价就是每次下载程序都要花费几秒钟下载新的ISP程序，这样用户每次下载的时间会变长，所以最好的肯定是一次性把boot程序写完美，没有bug。其实也可以在boot的ISP程序中加入写如程序到flash的命令,这样是否使用新的ISP就可选\n\n- 向芯片发送启动 ramrum 命令\n\n- 如果是想在ram中运行程序，到这一步为止即可，程序已经在ram中运行了，否则往下一步\n\n- 接下来就是运行发送过去的新ISP程序了\n\n- 上位机和ISP程序握手\n\n- isp程序默认波特率115200，如果需要更高，这里发送更改命令让isp程序修改串口波特率\n\n- 通过串口发送程序文件到isp程序，isp程序写入到 flash\n\n- 通过串口的 dtr 和 rts 控制芯片的reset和boot引脚来正常启动，不进入 ramrum 模式，而是正常从flash加载程序启动"}, "/news/MaixPy/reload_python_module.html": {"title": "关于 MicroPython import 指定 flash 或 sd 分区的代码与重载 Python 模块的方法", "content": "---\ntitle: 关于 MicroPython import 指定 flash 或 sd 分区的代码与重载 Python 模块的方法\nkeywords: MicroPython\ndate: 2022-06-09\ntags: MicroPython, reload\n---\n\n如果在 maixpy (micropython) 上同时存在 flash 和 sd 等多个分区挂载 / 目录下，且均存在 boot.py 文件，如何加载指定分区下的 boot.py 模块代码呢？\n\n<!-- more -->\n\n[原文链接](https://www.cnblogs.com/juwan/p/14517375.html) https://www.cnblogs.com/juwan/p/14517375.html\n\n`import boot` 时取决于 os 的 vfs (虚拟文件系统) 对象，它会根据 os.getcwd() 和 os.chdir('/sd') 决定代码寻找的位置（/sd 分区路径），如果是某目录下的代码，则可以使用类似 import test.boot 的结构来查找并 import 它。\n\n示例：\n\n```python\n>>> os.chdir('/flash')\n>>> import boot\nflash: 2942\n>>> os.getcwd()\n'/flash'\n>>> \n```\n\n拓展来讲，如何重载 import boot 后的 boot 模块，管理 sys.modules 模块就行，如下示意。\n\n```python\n>>> import sys\n>>> import boot\n2433\n>>> import boot\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>> sys.modules.pop('boot')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: boot\n>>> os.chdir('/flash')\n>>> import boot\nflash: 2479\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>> os.chdir('/sd')\n>>> import boot\n2488\n>>> sys.modules.pop('boot')\n<module 'boot' from 'boot.py'>\n>>>\n```\n\n即先使用 sys.modules.pop('boot') 后再重新 import 目标 boot 就行"}, "/news/MaixPy3/v831_Distance/v831_Distance.html": {"title": "V831完美的单目测距", "content": "---\ntitle: V831完美的单目测距\nkeywords: V831, 单目, 测距\ndate: 2022-03-28\ndesc: V831完美的单目测距\ntags: V83x,单目测距\n---\n\n<!-- more -->\n\n作者[我与nano](https://qichenxi.blog.csdn.net/?type=blog)，[原文链接](https://blog.csdn.net/qq_51963216/article/details/123745657)\n\n## 前言\n\n经过一下午的努力，最终终于实现了完美的单目测距，网上教的都是opencv怎么测算距离，人家有函数唉，入手了V831，做了人脸识别，同时进行了测距，K210通用。废话不多说上图。\n\n![单目测距](./assets/distance_measure.png)\n![摄像头距离](./assets/Camera_length.png)\n它那个镜头其实还要在靠近里面一点，距离应该是28.4到28.5之间。测得真的特别准。\n\n## 单目测距的原理\n![principle](./assets/principle.png)\n\n小孔成像。很简单，用的是小孔成像，原理大家都知道。该怎么做呢。\n我们需要以下几个参数：\n1、相机焦距\n2、物体宽度\n3、一个常数\n\n## 参数计算\n\n### 相机焦距\n假设我们有一个宽度为 W 的目标。然后我们将这个目标放在距离我们的相机为 D 的位置。我们用相机对物体进行拍照并且测量物体的像素宽度 P 。这样我们就得出了相机焦距的公式：\n\nF = (P x D) / W\n\n举个例子，假设我在离相机距离 D = 28cm的地方放一张 待识别图片（W = 13)并且拍下一张照片。我测量出照片的像素宽度为 P = 53 像素\n\n![](./assets/length_calculate.png)\n\n因此我的焦距 F 是：\n\nF = (53*28) / 13 = 116\n\n有人会问像素怎么获得呢，直接看代码吧\n```python\n img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n            img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n            img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\n            img.draw_string(0,30, \"x=\"+str(((box[0]+box[3])/2)-35), color= font_color)\n            img.draw_string(70,30, \"y=\"+str((box[1]+box[2])/2), color= font_color)\n\n            Lm = (box[1]+box[3])/2\n            length = K*13/Lm\n            img.draw_string(0,60 , \"Z=\"+str(length), color= font_color)\n\n```\n你识别到一个物体，然后给它画框，用一个列表表示出来四个点\nLm=（box[1]+box[3]）/2 这个就是像素值\n\n### 测距\n继续将相机移动，靠近或者离远物体或者目标时，可以用相似三角形计算出物体离相机的距离：\nL= (W x F) / P\n假设我将相机移到距离目标 28cm 的地方识别物体。通过自动的图形处理我可以获得图片的像素为 53像素。将这个代入公式，得：\nL= (13 x 116) / 53 = 28\n这样我们就精准的算出了距离。\n\n附上代码\n```python\nfrom maix import camera, image, display\nimport serial\nser = serial.Serial(\"/dev/ttyS1\",115200)    # 连接串口\nK=116\nclass Face_recognize :\n    score_threshold = 70                            #识别分数阈值\n    input_size = (224, 224, 3)                      #输入图片尺寸\n    input_size_fe = (128, 128, 3)                   #输入人脸数据\n    feature_len = 256                               #人脸数据宽度\n    steps = [8, 16, 32]                             #\n    channel_num = 0                                 #通道数量\n    users = []                                      #初始化用户列表\n    threshold = 0.5                                         #人脸阈值\n    nms = 0.3\n    max_face_num = 3                                        #输出的画面中的人脸\n    def __init__(self):\n        from maix import nn, camera, image, display\n        from maix.nn.app.face import FaceRecognize\n        for i in range(len(self.steps)):\n            self.channel_num += self.input_size[1] / self.steps[i] * (self.input_size[0] / self.steps[i]) * 2\n        self.channel_num = int(self.channel_num)     #统计通道数量\nglobal face_recognizer\nface_recognizer = Face_recognize()\nwhile True:\n    img = camera.capture()                       #获取224*224*3的图像数据\n    AI_img = img.copy().resize(224, 224)\n    faces = face_recognizer.face_recognizer.get_faces(AI_img.tobytes(),False)           #提取人脸特征信息\n\n    if faces:\n        for prob, box, landmarks, feature in faces:\n            disp_str = \"face\"\n            bg_color = (0, 255, 0)\n            font_color=(255, 0, 0)\n            box,points = face_recognizer.map_face(box,landmarks)\n            font_wh = image.get_string_size(disp_str)\n            for p in points:\n                img.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)\n            img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n            img.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n            img.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\n            img.draw_string(0,30, \"x=\"+str(((box[0]+box[3])/2-28)), color= font_color)\n            img.draw_string(70,30, \"y=\"+str((box[1]+box[2])/2-20), color= font_color)\n            x=(box[0]+box[3])/2-28\n            y=(box[1]+box[2])/2\n            Lm = (box[1]+box[3])/2\n            length = K*13/Lm\n            img.draw_string(0,60 , \"Z=\"+str(round(length)), color= font_color)\n           \n    display.show(img)\n\n```\n\n## 总结\n\n**主要原理就是小孔成像**"}, "/news/MaixPy3/maixpy3_easyuse/maixpy3_easyuse.html": {"title": "MaixPy3 源码怎么样", "content": "---\ntitle: MaixPy3 源码怎么样\nkeywords: V831, Maixpy3\ndate: 2022-04-29\ntags: MaixPy3,QQ\n---\n\n这里只使用一张图来说明一下相关的结论\n\n<!-- more -->\n\n![](./assets/pic.jpg)"}, "/news/MaixPy3/difference.html": {"title": "MaixPy 与 MaixPy3 的区别", "content": "---\ntitle: MaixPy 与 MaixPy3 的区别\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: MaixPy 与 MaixPy3 的区别\ndate: 2022-03-07\ntags: MaixPy,MaixPy3\n---\n\n<!-- more -->\n\n## 区别是？\n\n因为使用 MaixPy 的同学可能有两类人群，一类是从 MicroPython 一路使用过来的，另一类是从 Python3 过来的，所以针对两边的差异，分别做一下说明。\n\n可以这样理解，它们都是专门为 AIoT 提供的 Python 开发环境，提供了各种各样的模块。\n\n- MaixPy 指的是基于 MicroPython 的环境制作的。\n\n- MaixPy3 指的是基于 Linux Python3 的环境制作的。\n\n> 前者是基于 MCU 无系统的，后者是基于 Linux 系统。\n\n除了基本的 Python3 语法一致，在提供的模块方面的存在着不小的差异。\n\n### Python3 与 MicroPython 的区别\n\n大多数时候，Python 的发展以 Python3 为主，以下列出一些与 Python3 的差异化信息。\n\n- MicroPython 和 Python3 在 Python 语法上保持高度的一致性，常用的标准语法命令都已经支持。\n\n- MicroPython 虽然只实现了 Python3 的标准库和容器库的一些部分，常见容器库有同类功能，但不同名的模块，但大多算法类的 Python 逻辑代码是可以拿来即用的。\n\n- MicroPython 兼容实现的 Python3 的异常机制、没有实现元类（metaclass）机制，独立的 GC 机制。\n\n- 在许当不同的硬件微芯片（最低在 nRF51）的移植上， MicroPython 代码接口缺乏一致性，呈现碎片化。\n\n- MicroPython 编译（mpy-corss）后得到的是 mpy ，而不是 Python3 的 pyc 文件。\n\n- MicroPython 在移植 Python3 代码时，经常缺少各种方法，所以要习惯寻找同类接口，而它们的使用方法除了看文档外就只能看源码。\n\n### 总结\n\n- MaixPy 相比 MaixPy3 功能要更简单（简陋）。\n- MaixPy 和 MaixPy3 的开发工具不同。\n- MaixPy 标准库（MicroPython）相比 MaixPy3 有一定的不足。\n- MaixPy 的外设驱动模块具体函数存在差异。\n- 不同的芯片执行效率有差异，MaixPy 和 MaixPy3 的有着不同的内存与性能消耗。\n\n> 如有更多欢迎补充。"}, "/news/MaixPy3/key_face_recognize.html": {"title": "V831的人脸识别", "content": "---\ntitle: V831的人脸识别\nkeywords: MaixII-Dock, MaixPy3, 人脸识别, V831\ndesc: V831的人脸识别\ndate: 2022-03-15\ntags: MaixII-Dock, MaixPy3\n---\n\n在文档中看到 V831 可以用来实现人脸识别，于是就将按键也添加到人脸识别中。\n\n<!-- more -->\n\n实现一个可以通过按键进行控制的人脸识别，进行人脸信息的添加和删除控制\n## 源码\n\n```python\nfrom maix import nn, camera, image, display\nfrom maix.nn.app.face import FaceRecognize\nimport time\nfrom evdev import InputDevice\nfrom select import select\n\n\nscore_threshold = 70                            #识别分数阈值\ninput_size = (224, 224, 3)                      #输入图片尺寸\ninput_size_fe = (128, 128, 3)                   #输入人脸数据\nfeature_len = 256                               #人脸数据宽度\nsteps = [8, 16, 32]                             #\nchannel_num = 0                                 #通道数量\nusers = []                                      #初始化用户列表\nnames = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]  #人脸标签定义\nmodel = {                                                                                                                                   \n    \"param\": \"/home/model/face_recognize/model_int8.param\",\n    \"bin\": \"/home/model/face_recognize/model_int8.bin\"\n}\nmodel_fe = {\n    \"param\": \"/home/model/face_recognize/fe_res18_117.param\",\n    \"bin\": \"/home/model/face_recognize/fe_res18_117.bin\"\n}\n\n\nfor i in range(len(steps)):\n    channel_num += input_size[1] / steps[i] * (input_size[0] / steps[i]) * 2\nchannel_num = int(channel_num)     #统计通道数量\noptions = {                             #准备人脸输出参数\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": input_size\n    },\n    \"outputs\": {\n        \"output0\": (1, 4, channel_num) ,\n        \"431\": (1, 2, channel_num) ,\n        \"output2\": (1, 10, channel_num) \n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\noptions_fe = {                             #准备特征提取参数\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"inputs_blob\": input_size_fe\n    },\n    \"outputs\": {\n        \"FC_blob\": (1, 1, feature_len)\n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\nkeys = InputDevice('/dev/input/event0')\n\nthreshold = 0.5                                         #人脸阈值\nnms = 0.3                                               \nmax_face_num = 1                                        #输出的画面中的人脸的最大个数\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\nprint(\"-- load model:\", model_fe)\nm_fe = nn.load(model_fe, opt=options_fe)\nprint(\"-- load ok\")\nface_recognizer = FaceRecognize(m, m_fe, feature_len, input_size, threshold, nms, max_face_num)\n\ndef get_key():                                      #按键检测函数\n    r,w,x = select([keys], [], [],0)\n    if r:\n        for event in keys.read(): \n            if event.value == 1 and event.code == 0x02:     # 右键\n                return 1\n            elif event.value == 1 and event.code == 0x03:   # 左键\n                return 2\n            elif event.value == 2 and event.code == 0x03:   # 左键连按\n                return 3\n    return 0\n\ndef map_face(box,points):                           #将224*224空间的位置转换到240*240或320*240空间内\n    # print(box,points)\n    if display.width() == display.height():\n        def tran(x):\n            return int(x/224*display.width())\n        box = list(map(tran, box))\n        def tran_p(p):\n            return list(map(tran, p))\n        points = list(map(tran_p, points))\n    else:\n        # 168x224(320x240) > 224x224(240x240) > 320x240\n        s = (224*display.height()/display.width()) # 168x224\n        w, h, c = display.width()/224, display.height()/224, 224/s\n        t, d = c*h, (224 - s) // 2 # d = 224 - s // 2 == 28\n        box[0], box[1], box[2], box[3] = int(box[0]*w), int((box[1]-28)*t), int(box[2]*w), int((box[3])*t)\n        def tran_p(p):\n            return [int(p[0]*w), int((p[1]-d)*t)] # 224 - 168 / 2 = 28 so 168 / (old_h - 28) = 240 / new_h\n        points = list(map(tran_p, points))\n    # print(box,points)\n    return box,points\n\ndef darw_info(draw, box, points, disp_str, bg_color=(255, 0, 0), font_color=(255, 255, 255)):    #画框函数\n    box,points = map_face(box,points)\n    font_wh = image.get_string_size(disp_str)\n    for p in points:\n        draw.draw_rectangle(p[0] - 1, p[1] -1, p[0] + 1, p[1] + 1, color=bg_color)\n    draw.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color=bg_color, thickness=2)\n    draw.draw_rectangle(box[0], box[1] - font_wh[1], box[0] + font_wh[0], box[1], color=bg_color, thickness = -1)\n    draw.draw_string(box[0], box[1] - font_wh[1], disp_str, color=font_color)\ndef recognize(feature):                                                                   #进行人脸匹配\n    def _compare(user):                                                         #定义映射函数\n        return face_recognizer.compare(user, feature)                      #推测匹配分数 score相关分数\n    face_score_l = list(map(_compare,users))                               #映射特征数据在记录中的比对分数\n    return max(enumerate(face_score_l), key=lambda x: x[-1])                #提取出人脸分数最大值和最大值所在的位置\n\ndef run():\n    img = camera.capture()                       #获取224*224*3的图像数据\n    AI_img = img.copy().resize(224, 224)\n    if not img:\n        time.sleep(0.02)\n        return\n    faces = face_recognizer.get_faces(AI_img.tobytes(),False)           #提取人脸特征信息\n    if faces:\n        for prob, box, landmarks, feature in faces:\n            key_val = get_key()\n            if key_val == 1:                                # 右键添加人脸记录\n                if len(users) < len(names):\n                    print(\"add user:\", len(users))\n                    users.append(feature)\n                else:\n                    print(\"user full\")\n            elif key_val == 2:                              # 左键删除人脸记录\n                if len(users) > 0:\n                    print(\"remove user:\", names[len(users) - 1])\n                    users.pop()\n                else:\n                    print(\"user empty\")\n\n            if len(users):                             #判断是否记录人脸\n                maxIndex = recognize(feature)\n\n                if maxIndex[1] > score_threshold:                                      #判断人脸识别阈值,当分数大于阈值时认为是同一张脸,当分数小于阈值时认为是相似脸\n                    darw_info(img, box, landmarks, \"{}:{:.2f}\".format(names[maxIndex[0]], maxIndex[1]), font_color=(0, 0, 255, 255), bg_color=(0, 255, 0, 255))\n                    print(\"user: {}, score: {:.2f}\".format(names[maxIndex[0]], maxIndex[1]))\n                else:\n                    darw_info(img, box, landmarks, \"{}:{:.2f}\".format(names[maxIndex[0]], maxIndex[1]), font_color=(255, 255, 255, 255), bg_color=(255, 0, 0, 255))\n                    print(\"maybe user: {}, score: {:.2f}\".format(names[maxIndex[0]], maxIndex[1]))\n            else:                                           #没有记录脸\n                darw_info(img, box, landmarks, \"error face\", font_color=(255, 255, 255, 255), bg_color=(255, 0, 0, 255))\n\n\n    display.show(img)\n\n\n\nif __name__ == \"__main__\":\n    import signal\n    def handle_signal_z(signum,frame):\n        print(\"APP OVER\")\n        exit(0)\n    signal.signal(signal.SIGINT,handle_signal_z)\n    while True:\n        run()\n\n```"}, "/news/MaixPy3/camera_resize/camera_resize.html": {"title": "MaixPy3 Image.resize 的效果", "content": "---\ntitle: MaixPy3 Image.resize 的效果\nkeywords: MaixPy3,\ndesc: 这里展示一下 MaixPy3 Image.resize 的效果\ndate: 2022-07-01\ntags: MaixPy3, resize, teedoc\n---\n\n点击下载对应的 ipynb 文件后导入到 jupyter 照样能阅读 [源文件](https://dl.sipeed.com/fileList/others/wiki_news/maixpy3_resize/camera_resize.ipynb)\n\n<!-- more -->\n\n## 先写一些 jupyter 用法\n\n- 每一个框框都被称之为单元格\n\n- 单元格左方会有 蓝色 或者 绿色 两种颜色。绿色表示编辑模式；蓝色表示命令模式。\n    - 通用：\n        - Shift+ Enter ：运行单元格，且以命令模式切换到下一个单元格\n        - Ctrl + Enter ：运行单元格，且进入命令模式\n    - 编辑模式中：\n        - Esc       ：进入命令模式\n    - 命令模式中：\n        - **h    :打开帮助**\n        - Enter :进入编辑模式\n        - x    :剪切单元格\n        - c    :复制单元格\n        - v    :粘贴单元格\n        - dd   :删除整个单元格\n        - ii   :终止运行 \n\n## 开始演示效果\n\n### 试一下推流\n\n```python\nimport os\nos.system(\"killall python3\")             #先杀一次预防 camera is busy\nfrom maix import camera, display, image #引入python模块包\nwhile True:\n    img = camera.capture()    #从摄像头中获取一张图像\n    display.show(img)         #将图像显示出来\n    \n    #因为这是死循环，所以按一下 Esc 进入编辑模式然后 ii 终止一下代码\n```\n\n效果如下（视频被截成图片了）：\n\n![推流](./assets/forever_show.jpeg)\n\n### 设置一下图像分辨率\n\n下面就直接捕获原图和使用 image.resize() 放一起对比一下\n\n#### 试试 240*240 的显示效果\n\n``` python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(240, 240))   #设置获取图像分辨率\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"240x240.jpg\")\n```\n\n![240*240](./assets/240_240.jpeg)\n\n#### 240\\*240图片resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(240, 240))\nimg = camera.capture().resize(224, 224)\ndisplay.show(img)\n```\n\n![240*240->224*224](./assets/240_240_224_224.png)\n\n#### 试试 320*240 的显示效果\n\n```python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(320, 240))\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"320x240.jpg\")\n```\n\n![320*240](./assets/320_240.jpeg)\n\n#### 320\\*240图像resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(320, 240))\nimg = camera.capture().resize(224, 244)\ndisplay.show(img)\n```\n\n![320*240->224*224](./assets/320_240_224_224.png)\n\n#### 再试试 320*180 显示效果\n\n```python\nimport os\nos.system(\"killall python3\")\nfrom maix import camera, display\ncamera.config(size=(320, 180))    #设置摄像头分辨率\nimg = camera.capture()\nprint(img)\ndisplay.show(img)\nimg.save(\"320x180.jpg\")\n```\n\n![320*180](./assets/320_180.jpeg)\n\n#### 320\\*180图像resize到224\\*224\n\n```python\nfrom maix import camera, display\n\ncamera.config(size=(320, 180))\nimg = camera.capture().resize(224, 244)\ndisplay.show(img)\n```\n\n![320*180->224*224](./assets/320_180_224_224.png)"}, "/news/MaixPy3/run_lvgl/run_lvgl.html": {"title": "V831 如何使用 libmaix SDK C++ 开发", "content": "---\ntitle: V831 如何使用 libmaix SDK C++ 开发\nkeywords: V831, 交叉编译, lvgl\ndate: 2022-05-19\ndesc: V831 如何使用 libmaix SDK C++ 开发 交叉编译\ntags: V831，交叉编译\n---\n\n<!-- more -->\n\n作者[Song](QQ群友)，原文文件 [点我下载](https://dl.sipeed.com/fileList/others/wiki_news/v831_lvgl_news/220519UbuntuForV831%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.pptx)\n\n下面为重新整理的部分内容\n\n## 准备环境\n\n### 准备 linux\n\n一般在 linux 系统下开发比 windows 系统里问题少。因此首先自己整一个 linux 环境先。\n\n- 物理机和虚拟机都可以\n\n下面使用 Ubuntu18.04 作为实验系统。\n\n首先安装相关依赖 `sudo apt install build-essential cmake python3 sshpass git`\n\n然后确保一下cmake版本>=3.9\n![](./assets/cmake-version.png)\n\n### 配置交叉编译链\n\n先下载这个[点我跳转](https://dl.sipeed.com/shareURL/MaixII/MaixII-Dock/SDK/Toolchain)\n![](./assets/toolchain.png)\n\n接着再对应的下载目录执行下面的命令将工具链解压到 /opt 目录下\n`sudo tar -Jxvf toolchain-sunxi-musl-pack-2021-01-09.tar.xz -C /opt`\n\n### 获取libmaix源码\n\n新建一个文件夹后打开文件夹。\n在对应文件夹的终端使用下面命令来获取源码\n`git clone https://github.com/sipeed/libmaix --recursive`\n\n一定要确保全部下载完成，否则后面会因为找不到文件编译出错。\n\n### 开始编译\n\n#### 尝试编译helloworld\n\n先进入 libmaix 源码目录的 helloworld 文件夹里\n```bash\ncd libmaix/examples/hello-world\n```\n\n根据 CPU 架构选择工具链和前缀\n```bash\npython3 project.py --toolchain /opt/toolchain-sunxi-musl/toolchain/bin --toolchain-prefix arm-openwrt-linux-muslgnueabi- config\n```\n![](./assets/helloworld.png)\n\n接着就可以编译 helloworld 了。在上面的命令成功执行后接着执行下面的命令\n```bash\npython3 project.py menuconfig\n```\n\n若出现以下画面，则说明下载内容完整。若报错先尝试使用sudo执行，否则需重新下载解压\n![](./assets/helloworld-menuconfig.png)\n\n如第三项选择了Enable 3rd party component，则在编译时时间就会较长，因为会编译所有勾选的第三方组件。编译过程中可能会报一些warning，但最终出现下图画面则说明编译过程无误\n<img src=\"./assets/enable-3rd-party-component.png\">\n<img src=\"./assets/finish-helloworld.png\">\n\n前面的都顺利结束后在当前目录下会有一个 dist 文件夹。里面的 helloworld 文件就是 v831 的可执行程序。\n\n我们可以用 ssh 或者使用 v831在电脑上显示的U盘 把 helloworld 可执行文件传输到板子上\n\n接着在对应的目录下直接执行就可以看到结果了\n![](./assets/run-helloworld.png)\n\n#### lvgl编译与测试\n\n在 libmaix 源码的路径下的 /examples/mpp_v83x_vivo 执行下面命令后按照图示配置\n```bash\npython3 project.py menuconfig\n```\n<html>\n<div class=\"imbox\">\n    <img src=\"./assets/lvgl-1.png\" >\n    <img src=\"./assets/lvgl-2.png\" >\n</div>\n</html>\n\n检查选项是否如以上配置所示，确认无误后在命令行执行\n```bash\npython3 project.py build\n```\n- 注：若同时勾选所有组件，则可能会发生重复定义函数的报错导致编译失败\n\n若出现如图所示情况，则说明编译成功\n<img src=\"./assets/lvgl-3.png\">\n\n在板子上运行\n<html>\n<div class=\"imbox\">\n    <img src=\"./assets/lvgl-4.png\" height=300>\n    <img src=\"./assets/lvgl-5.png\" height=300>\n<style>\n.imbox{\n     display:flex;\n     flex-direction: row;\n     }\n</style>\n</div>\n</html>"}, "/news/Lichee/RV/run_nonos_program/nonos_run.html": {"title": "在D1上使用裸机程序", "content": "---\ntitle: 在D1上使用裸机程序\nkeywords: RV, D1, 裸机\ndate: 2022-04-29\ntags: MaixPy,MaixPy3\n---\n近日一国内小哥实现了一种在D1上运行裸机程序的方法。让我们一起来看一下吧\n\n<!-- more -->\n\nGithub仓库地址在这里：https://github.com/Ouyancheng/FlatHeadBro\n\n相关使用方法在仓库的 readme 写的很详细了。\n\n大概就是用先编译出一个 启动固件，烧录到SD卡启动板子后可以看到串口有相关的信息打印出来。\n\n接着只需要使用 python 将想要运行的程序通过串口传送到开发板上面即可。"}, "/news/Lichee/RV/D1_RTL8723DS_Drivers/D1_RTL8723DS_Drivers.html": {"title": "D1 LicheeRV Dock 移植RTL8723DS驱动", "content": "---\ntitle: D1 LicheeRV Dock 移植RTL8723DS驱动\nkeywords: D1, RTL8723DS, 驱动\ndesc: RTL8723DS驱动移植\ndate: 2022-04-02\ntags: linux, D1\n---\n\n这里讲解怎样自己添加驱动\n\n<!-- more -->\n\n[原文链接](https://bbs.aw-ol.com/topic/994/d1-licheerv-dock-%E7%A7%BB%E6%A4%8Drtl8723ds%E9%A9%B1%E5%8A%A8)\n\n手动焊接RTL8723DS之后，现在开始移植驱动程序。\n\n先获取源码：https://github.com/lwfinger/rtl8723ds\n\n下载完成后，把驱动文件复制到 tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds 里，没有rtl8723ds文件夹记得新建一个。\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\Makefile，加一行 \nobj-$(CONFIG_RTL8723DS) += rtl8723ds/\n\n![](./assets/rtl8723ds.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\Kconfig，加一行 \nsource \"drivers/net/wireless/rtl8723ds/Kconfig\"\n\n![](./assets/Kconfig.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds\\os_dep\\linux\\os_intfs.c；\n加一行\nMODULE_IMPORT_NS(VFS_internal_I_am_really_a_filesystem_and_am_NOT_a_driver);\n\n![](./assets/os_intfs.png)\n\n修改tina-d1-open\\lichee\\linux-5.4\\drivers\\net\\wireless\\rtl8723ds\\os_dep\\linux\\rtw_cfgvendor.c\n在每一行.policy = VENDOR_CMD_RAW_DATA, 下面加上 .maxattr = 1,\n\n![](./assets/rtw_cfgvendor.png)\n\n修改tina-d1-open\\target\\allwinner\\d1-lichee_rv_dock\\modules.mk，增加以下内容：\n\n![](./assets/modules.png)\n\n（其中的d1-lichee_rv_dock 是我的板级配置，请选择自己的板级配置比如d1-nezha，如下图）\n\n![](./assets/borad_config.png)\n\n进入内核配置，勾选Realtek 8723D SDIO or SPI WiFi为Module（ < M > 不是 < * > ）\n\n```menuconfig\nmake kernel_menuconfig\n\nDevice Drivers ->\n     Network device support -> \n           Wireless LAN -> \n                  <M>   Realtek 8723D SDIO or SPI WiFi\n```\n\n进入Tina配置，勾选相关驱动\n\n```bash\nmake menuconfig\n\nFirmware  ->\n     <*> r8723ds-firmware.............................. RealTek RTL8723DS firmware\n\nKernel modules -> \n     Wireless Drivers  ->\n        <*> kmod-net-rtl8723ds........................... RTL8723DS support (staging)\n```\n\n保存，编译，打包\n\n```bash\nmake -j8\npack\n```\n\n烧录后就能看到\n\n![](./assets/apperance.jpg)"}, "/news/Lichee/RV/D1-ncnn/D1_ncnn.html": {"title": "在全志d1开发板上玩ncnn", "content": "---\ntitle: 在全志d1开发板上玩ncnn\nkeywords: D1, RV, Lichee, ncnn, \ndesc: 在全志d1开发板上玩ncnn\ndate: 2022-03-28\ntags: RV, ncnn\n---\n\n<!-- more -->\n\n转载自知乎用户 [nihui](https://www.zhihu.com/people/nihui-2) [原文链接](https://zhuanlan.zhihu.com/p/386312071)，原文写于 2021-07-03\n\n在全志d1开发板上玩ncnn\n\n**可在不修改本文章内容和banner图前提下，转载本文**\n```\n这是我最后一次优化 risc-v\n这 1.4w 行代码是我最后的倔强\n你们不可能再看见我为这个 d1 写一行代码，不可能\n这 96 个 cpp 文件，我要用到 2030 年\n```\n**首先感谢全志科技公司送了我d1开发板，以及sipeed、rvboards在系统底层技术工作和支持，才有了ncnn AI推理库在risc-v架构上更好的优化 qwqwqwq**\n\n## 0x0 ncnn risc-v 优化情况\n[ncnn](https://github.com/Tencent/ncnn) 是腾讯开源的神经网络推理框架\n- 支持深度学习模型 caffe/mxnet/keras/pytorch(onnx)/darknet/tensorflow(mlir)\n- 跨平台：Windows/Linux/MacOS/Android/iOS/WebAssembly/...\n- 兼容多种 CPU 架构：x86/arm/mips/risc-v/...\n- 支持 GPU 加速：NVIDIA/AMD/Intel/Apple/ARM-Mali/Adreno/...\n- 支持各种常见的模型结构，比如 mobilenet/shufflenet/resnet/LSTM/SSD/yolo...\n- 很强，qq群请移驾 ncnn github 首页README\n_因为据全（某）志（人）说，全志的用户基础都挺一般，可能不知道 ncnn 是什么东西，所以便罗嗦一番..._\n\n从上次发了开箱自拍jpg，到现在一个月了，ncnn risc-v vector 优化情况还算不错，大部分重要的优化都做了，剩下一些会留给社区学生pr，和慢慢变聪明的编译器\n\nncnn risc-v 目前使用 rvv-1.0 intrinisc 编写优化代码，并支持任意 vlen 的配置，面向未来顺便兼容了 d1开发板\n\n- rvv-0.7.1 某些 intrinisc 转换可能有效率问题\n- 有遇到过 0.7.1 intrinisc 行为怪异只能写 C 代码绕过\n- gcc 还比较笨，每行 intrinisc 都会加一条无用的 setvli 指令\n- 因为没法同时兼容 rvv-1.0 和 rvv-0.7.1，便没有写汇编\n- 一些算子，如 hardswish/hardsigmoid/binaryop/eltwise/slice/... 待优化（欢迎pr！！！qaq）\n下面这张表只是最近一周多的情况。如果跟最开始比，柱状图就太高了...\n![](./assets/ncnn/001.jpg)\n\n![](./assets/ncnn/002.jpg)\n\n## 0x1 准备交叉编译工具链\n去平头哥芯片开放社区下载 工具链-900 系列 \nhttps://occ.t-head.cn/community/download?id=3913221581316624384\n比如 riscv64-linux-x86_64-20210512.tar.gz，下载后解压缩，设置环境变量\n```bash\ntar -xf riscv64-linux-x86_64-20210512.tar.gz\nexport RISCV_ROOT_PATH=/home/nihui/osd/riscv64-linux-x86_64-20210512\n```\n## 0x2 下载和编译ncnn\n\n为 d1 架构交叉编译 ncnn\n\n**因为编译器 bug，release 编译会导致运行时非法指令错误，必须使用 relwithdebinfo 编译哦**\n\n- ncnn 已支持直接用 simpleocv 替代 opencv 编译出 examples\n- **不需要配opencv啦！**\n- **不需要配opencv啦！**\n- **不需要配opencv啦！（重要，说了三遍）**\n\n```bash\ngit clone https://github.com/Tencent/ncnn.git\ncd ncnn\nmkdir build-c906\ncd build-c906\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/c906.toolchain.cmake -DCMAKE_BUILD_TYPE=relwithdebinfo -DNCNN_OPENMP=OFF -DNCNN_THREADS=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_RVV=ON -DNCNN_SIMPLEOCV=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j32\n```\n## 0x3 测试benchncnn\n**d1 默认的 TinaLinux 执行 ncnn 程序时会发生非法指令错误，必须使用 Debian 系统哦**\n- vgg16 这类大型模型在内存不足时会发生 segmentation fault，是 d1开发板硬件条件不够，不管即可\n  \n将 `ncnn/build-c906/benchmark/benchncnn` 和 `ncnn/benchmark/*.param` 拷贝到 d1开发板上\n```bash\n./benchncnn 4 1 0 -1 0\n```\n\n## 0x4 测试example\n将 `ncnn/build-c906/examples/nanodet` 和测试图片拷贝到 d1开发板上\n从这里下载 nanodet 模型文件并拷贝到 d1开发板上\nhttps://github.com/nihui/ncnn-assets/tree/master/models\n```bash\n./nanodet test.jpg\n```\n输出检测结果信息，并保存在 image.png\n```\n0 = 0.82324 at 200.04 44.89 198.96 x 253.33\n0 = 0.78271 at 32.98 63.45 178.15 x 232.92\n56 = 0.45923 at 1.46 71.92 90.14 x 117.85\nimshow save image to image.png\nwaitKey stub\n```\n把image.png下载到本地查看，结果已经画在图片上了！d1开发板AI目标检测成功 w\n![](./assets/ncnn/003.jpg)\n\n## 0x5 mips大概也会安排啦，欢迎关注 ncnn github，加qq群交流！\nhttps://github.com/Tencent/ncnn\n\nqq群在 ncnn github 首页 readme 中～"}, "/news/Lichee/RV/D1-ncnn/D1_ncnn_new.html": {"title": "又在全志d1开发板上玩ncnn", "content": "---\ntitle: 又在全志d1开发板上玩ncnn\nkeywords: D1, RV, Lichee, ncnn, \ndesc: 又在全志d1开发板上玩ncnn\ndate: 2022-03-28\ntags: RV, ncnn\n---\n\n\n<!-- more -->\n\n转载自知乎用户 [nihui](https://www.zhihu.com/people/nihui-2) [原文链接](https://zhuanlan.zhihu.com/p/441176926)，原文写于 2021-07-03\n\n又在全志d1开发板上玩ncnn\n\n**可在不修改本文章内容和banner图前提下，转载本文**\n\n## 0x0 工具链变得更好了\n距上次[在全志d1开发板上玩ncnn](./D1_ncnn.html)，已经过去了5个月\n\n在此期间，ncnn收到perfxlab和腾讯犀牛鸟开源人才的学生有关riscv vector的优化\n\n但更重要的是，平头哥收到了社区的反馈，提供了新版工具链\n\n- 支持了 risc-v vector intrinsic v1.0\n- 修复了 release 模式编译 ncnn 时的非法指令问题\nhttps://occ.t-head.cn/community/download?id=3987221940543754240\n\n旧版本工具链的 gcc 比较笨，经常做些负优化，于是试试全新的工具链\n\n## 0x1 配置新的 cmake toolchain\n```bash\n旧\n-march=rv64gcvxtheadc -mabi=lp64d -mtune=c906 -DRVV_SPEC_0_7 -D__riscv_zfh=1 -static\n\n新\n-march=rv64gcv0p7_zfh_xtheadc -mabi=lp64d -mtune=c906 -static\n```\n- arch 参数要用 v0p7，不能用默认的 v，否则会生成非法指令\n- 删除 -DRVV_SPEC_0_7，开启 ncnn 的 rvv-1.0 intrinsic 代码\n- 删除 -D__riscv_zfh=1，arch 参数的 zfh 中已经指代\n\n放在 ncnn/toolchains/c906-v222.toolchain.cmake\n\n## 0x2 工具链修复\n\n因为 rvv-0.7 缺少某些指令支持，遇到一些 rvv-1.0 的代码会生成 unknown op\n```bash\nfneg\nfrec7\nfrsqrt7\n```\n因此要修改下工具链头文件\n\n打开 Xuantie-900-gcc-linux-5.10.4-glibc-x86_64-V2.2.2/lib/gcc/riscv64-unknown-linux-gnu/10.2.0/include/riscv_vector.h\n\n- 找到以下三行\n```h\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rec7)\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rsqrt7)\n_RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, neg)\n```\n- 注释掉\n\n```h\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rec7)\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, rsqrt7)\n// _RVV_FLOAT_ITERATOR_ARG (_RVV_FLOAT_UNARY_OP, neg)\n```\n\n- 找到文件末尾的三个 #endif，添加以下兼容代码，保存\n\n```h\n#endif\n\n#define vfneg_v_f32m1(x, vl) vfsgnjn_vv_f32m1(x, x, vl)\n#define vfneg_v_f32m2(x, vl) vfsgnjn_vv_f32m2(x, x, vl)\n#define vfneg_v_f32m4(x, vl) vfsgnjn_vv_f32m4(x, x, vl)\n#define vfneg_v_f32m8(x, vl) vfsgnjn_vv_f32m8(x, x, vl)\n#define vfneg_v_f16m1(x, vl) vfsgnjn_vv_f16m1(x, x, vl)\n#define vfneg_v_f16m2(x, vl) vfsgnjn_vv_f16m2(x, x, vl)\n#define vfneg_v_f16m4(x, vl) vfsgnjn_vv_f16m4(x, x, vl)\n#define vfneg_v_f16m8(x, vl) vfsgnjn_vv_f16m8(x, x, vl)\n\n#define vfrec7_v_f32m1(x, vl) vfrdiv_vf_f32m1(x, 1.f, vl)\n#define vfrec7_v_f32m2(x, vl) vfrdiv_vf_f32m2(x, 1.f, vl)\n#define vfrec7_v_f32m4(x, vl) vfrdiv_vf_f32m4(x, 1.f, vl)\n#define vfrec7_v_f32m8(x, vl) vfrdiv_vf_f32m8(x, 1.f, vl)\n#define vfrec7_v_f16m1(x, vl) vfrdiv_vf_f16m1(x, 1.f, vl)\n#define vfrec7_v_f16m2(x, vl) vfrdiv_vf_f16m2(x, 1.f, vl)\n#define vfrec7_v_f16m4(x, vl) vfrdiv_vf_f16m4(x, 1.f, vl)\n#define vfrec7_v_f16m8(x, vl) vfrdiv_vf_f16m8(x, 1.f, vl)\n\n#define vfrsqrt7_v_f32m1(x, vl) vfrdiv_vf_f32m1(vfsqrt_v_f32m1(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m2(x, vl) vfrdiv_vf_f32m2(vfsqrt_v_f32m2(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m4(x, vl) vfrdiv_vf_f32m4(vfsqrt_v_f32m4(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f32m8(x, vl) vfrdiv_vf_f32m8(vfsqrt_v_f32m8(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m1(x, vl) vfrdiv_vf_f16m1(vfsqrt_v_f16m1(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m2(x, vl) vfrdiv_vf_f16m2(vfsqrt_v_f16m2(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m4(x, vl) vfrdiv_vf_f16m4(vfsqrt_v_f16m4(x, vl), 1.f, vl)\n#define vfrsqrt7_v_f16m8(x, vl) vfrdiv_vf_f16m8(vfsqrt_v_f16m8(x, vl), 1.f, vl)\n\n#endif\n#endif\n```\n\n## 下载和编译ncnn\n这次可以用 release 编译啦！\n```bash\ngit clone https://github.com/Tencent/ncnn.git\ncd ncnn\nmkdir build-c906\ncd build-c906\ncmake -DCMAKE_TOOLCHAIN_FILE=../toolchains/c906-v222.toolchain.cmake -DCMAKE_BUILD_TYPE=release -DNCNN_OPENMP=OFF -DNCNN_THREADS=OFF -DNCNN_RUNTIME_CPU=OFF -DNCNN_RVV=ON -DNCNN_SIMPLEOCV=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j32\n```\n\n## 新旧工具链的性能测试对比\n![](./assets/ncnn_new/ncnn_new_001.jpg)\n\n![](./assets/ncnn_new/ncnn_new_002.jpg)\n\n## 0x5 欢迎关注 ncnn github，加qq群交流！\nhttps://github.com/Tencent/ncnn\nqq群在 ncnn github 首页 readme 中～"}}