{"/soft/maixpy3/zh/question/how_to_ask.html": {"title": "", "content": ""}, "/soft/maixpy3/zh/question/driver_issue.html": {"title": "常见问题", "content": "# 常见问题\n## 问题\n> 21.06.24 目前暂时不支持使用etcher进行系统的烧录，会出现烧录之后无法进入系统，推荐使用dd命令进行烧录"}, "/soft/maixpy3/zh/question/maixpy3_issue.html": {"title": "", "content": ""}, "/soft/maixpy3/zh/others/develop.html": {"title": "MaixPy3 开发文档", "content": "---\ntitle: MaixPy3 开发文档\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 如何参与项目（开发文档）\n---\n\nMaixPy3 并不是为了某一款芯片平台制作的，它的初衷就是为了通过 Python 编程简化用户在嵌入式 Linux 上开发程序的过程，所以是建立在所有 Linux 设备都能使用的基础上去设计的，但由于 Sipeed 官方的能力有限，难以同时照顾所有开源硬件的同步开发，所以提供一些官方的基本芯片移植参考，方便第三方的开源爱好者提交其他芯片平台、镜像、工具推送到 MaixPy3 的环境中。\n\n## 一般开发流程\n\n从 MaixPy3 仓库的 [setup.py](https://github.com/sipeed/MaixPy3/blob/main/setup.py) 进行项目的编译。\n\n对于一台 Linux X86 的个人计算机而言，我们使用如下命令进行构建。\n\n- 编译 `python3 setup.py build`\n- 清理 `python3 setup.py clean`\n- 安装 `pip3 install .`\n\n```bash\njuwan@juwan-N85-N870HL:~/Desktop/v831_toolchain_linux_x86/MaixPy3$ python3 setup.py build\nrunning build\nrunning build_py\nrunning egg_info\nwriting MaixPy3.egg-info/PKG-INFO\nwriting dependency_links to MaixPy3.egg-info/dependency_links.txt\nwriting entry points to MaixPy3.egg-info/entry_points.txt\nwriting requirements to MaixPy3.egg-info/requires.txt\nwriting top-level names to MaixPy3.egg-info/top_level.txt\nwriting manifest file 'MaixPy3.egg-info/SOURCES.txt'\nrunning build_ext\njuwan@juwan-N85-N870HL:~/Desktop/v831_toolchain_linux_x86/MaixPy3$ python3 setup.py clean\nrunning clean\njuwan@juwan-N85-N870HL:~/Desktop/v831_toolchain_linux_x86/MaixPy3$ pip3 install .Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\nProcessing /home/juwan/Desktop/v831_toolchain_linux_x86/MaixPy3\nRequirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from MaixPy3==0.2.9) (7.0.0)\nRequirement already satisfied: evdev in /home/juwan/.local/lib/python3.8/site-packages (from MaixPy3==0.2.9) (1.4.0)\nRequirement already satisfied: gpiod in /home/juwan/.local/lib/python3.8/site-packages (from MaixPy3==0.2.9) (1.4.0)\nRequirement already satisfied: numpy in /home/juwan/.local/lib/python3.8/site-packages (from MaixPy3==0.2.9) (1.19.4)\nRequirement already satisfied: opencv-python in /home/juwan/.local/lib/python3.8/site-packages (from MaixPy3==0.2.9) (4.5.1.48)\nRequirement already satisfied: pyserial in /usr/local/lib/python3.8/dist-packages (from MaixPy3==0.2.9) (3.4)\nRequirement already satisfied: rpyc in /home/juwan/.local/lib/python3.8/site-packages (from MaixPy3==0.2.9) (5.0.1)\nRequirement already satisfied: spidev in /home/juwan/.local/lib/python3.8/site-packages (from MaixPy3==0.2.9) (3.5)\nRequirement already satisfied: plumbum in /home/juwan/.local/lib/python3.8/site-packages (from rpyc->MaixPy3==0.2.9) (1.6.9)\nBuilding wheels for collected packages: MaixPy3\n  Building wheel for MaixPy3 (setup.py) ... done\n  Created wheel for MaixPy3: filename=MaixPy3-0.2.9-cp38-cp38-linux_x86_64.whl size=115611 sha256=54f70f181ccc629f1eaf470bf30eccd20389c6333814d7145e16a31db7f6cdcd\n  Stored in directory: /tmp/pip-ephem-wheel-cache-9bf1q3wt/wheels/53/7d/47/6cd374fab930089f96a0a3185f5677e52a9b71dbbee769935d\nSuccessfully built MaixPy3\nInstalling collected packages: MaixPy3\n  Attempting uninstall: MaixPy3\n    Found existing installation: MaixPy3 0.2.8\n    Uninstalling MaixPy3-0.2.8:\n      Successfully uninstalled MaixPy3-0.2.8\nSuccessfully installed MaixPy3-0.2.9\njuwan@juwan-N85-N870HL:~/Desktop/v831_toolchain_linux_x86/MaixPy3$\n```\n\n而对于不能在目标平台上编译安装的环境，就需要使用预编译的 whl 包来辅助安装，以 Maix V831 为例。\n\n- 编译 `python3.8 setup.py maix_v831 bdist_wheel`\n\n- 安装 `export TMPDIR=/root && pip install ./dist/*.whl`\n\n```bash\nroot@sipeed:/# export TMPDIR=/root && pip install maixpy3 --upgrade\nCollecting maixpy3\n  Downloading MaixPy3-0.1.9-cp38-cp38-linux_armv7l.whl (1.0 MB)\n     |████████████████████████████████| 1.0 MB 43 kB/s\nCollecting pexpect\n  Downloading pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n     |████████████████████████████████| 59 kB 71 kB/s\nCollecting rpyc\n  Downloading rpyc-5.0.1-py3-none-any.whl (68 kB)\n     |████████████████████████████████| 68 kB 42 kB/s\nRequirement already satisfied, skipping upgrade: Pillow in /usr/lib/python3.8/site-packages (from maixpy3) (7.2.0)\nCollecting ptyprocess>=0.5\n  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\nCollecting plumbum\n  Downloading plumbum-1.6.9-py2.py3-none-any.whl (115 kB)\n     |████████████████████████████████| 115 kB 84 kB/s\nInstalling collected packages: ptyprocess, pexpect, plumbum, rpyc, maixpy3\nSuccessfully installed maixpy3-0.1.9 pexpect-4.8.0 plumbum-1.6.9 ptyprocess-0.7.0 rpyc-5.0.1\nWARNING: You are using pip version 20.1.1; however, version 21.0 is available.\nYou should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\n\nroot@sipeed:/#\n```\n\n对于一些安装失败，缺少了依赖库的场合，需要从外部去引入该包的安装，例如这个问题 [error happened when install maixpy3](https://github.com/sipeed/MaixPy3/issues/4) ，这通常需要升级镜像来解决，或手动安装相关的依赖包。\n\n至此以后，在发布软件包的时候可以通过 `export TMPDIR=/root && pip install maixpy3` 让目标机器直接安装 maixpy3 的包即可使用。\n\n## 一般测试流程\n\n项目引入 tox 进行软件接口交互的自动化测试，通常用它进行虚拟 Python 环境测试，确保软件代码的依赖关系和接口逻辑测试，如测试 `from xxx import *` 是否可行。\n\n```bash\njuwan@juwan-N85-N870HL:~/Desktop/v831_toolchain_linux_x86/MaixPy3$ tox\nGLOB sdist-make: /home/juwan/Desktop/v831_toolchain_linux_x86/MaixPy3/setup.py\npy38 inst-nodeps: /home/juwan/Desktop/v831_toolchain_linux_x86/MaixPy3/.tox/.tmp/package/1/MaixPy3-0.1.2.zip\npy38 installed: attrs==20.3.0,iniconfig==1.1.1,packaging==20.8,Pillow==8.1.0,pluggy==0.13.1,py==1.10.0,pyparsing==2.4.7,pytest==6.2.1,MaixPy3 @ file:///home/juwan/Desktop/v831_toolchain_linux_x86/MaixPy3/.tox/.tmp/package/1/MaixPy3-0.1.2.zip,scripttest==1.3,toml==0.10.2\npy38 run-test-pre: PYTHONHASHSEED='820562099'\npy38 run-test: commands[0] | py.test\n======================================= test session starts ========================================\nplatform linux -- Python 3.8.5, pytest-6.2.1, py-1.10.0, pluggy-0.13.1\ncachedir: .tox/py38/.pytest_cache\nrootdir: /home/juwan/Desktop/v831_toolchain_linux_x86/MaixPy3\ncollected 5 items\n\next_modules/_maix/example/test__maix.py .                                                    [ 20%]\ntests/test_maix.py ....                                                                      [100%]\n\n======================================== 5 passed in 0.05s =========================================\n_____________________________________________ summary ______________________________________________\n  py38: commands succeeded\n  congratulations :)\n```\n\n对于硬件模块，通常不好自动化测试，所以会做成 example 提供。\n\n关于代码覆盖性测试，暂时不做。\n\n## 一般发布流程\n\n2021年02月21日 关于自动化构建，还在考虑到导入多个平台的编译链编译的问题，暂时还没有准备好。\n\n2022年7月24日 已经[配置了 github CI 自动构建后发布到 pypi.org ](https://github.com/sipeed/MaixPy3/actions)。\n\n## Python 模块编译说明\n\nMaixPy3 使用面向模块接口开发，链接跨平台的 Python 或 C 包，统一加载到 Python3 环境当中。\n\n目前支持的 Python3 环境如下：\n\n- [PC x86_64 的 Pyhon3 环境](https://www.python.org/downloads/release/python-380/)\n\n- [Sipeed v831 的 Python3 交叉编译环境](https://github.com/sipeed/MaixPy3/releases/tag/20210613) (需要使用 source toolchain_v83x_linux_x86/envsetup.sh 获得链接 V831 编译链的 python3.8 环境，注意这不是本机的 Python3 环境！！！)\n\n通常拿到一个 Python 模块，对它的 `setup.py` 执行 `python setup.py build` 即可进行构建，它的内容通常有如下示例（只是举例）。\n\n```python\n\nfrom setuptools import setup, Extension, find_packages\n\n_maix_module = Extension('_maix', include_dirs=['ext_modules/_maix/include'], sources=get_srcs('ext_modules/_maix'), libraries=['jpeg'])\n\nlibi2c_module = Extension('pylibi2c',  include_dirs=['ext_modules/libi2c/src'], sources=get_srcs('ext_modules/libi2c/src'))\n\nsetup(\n    name='MaixPy3',\n    version='0.1.2',\n    license='MIT',\n    author='Sipeed',\n    author_email=\"support@sipeed.com\",\n    url='https://github.com/sipeed/MaixPy3',\n    description=\"MaixPy Python3 library\",\n    long_description=open('README.md').read(),\n    install_requires=[\"Pillow\"],\n    ext_modules=[\n        _maix_module,\n        libi2c_module,\n    ],\n    packages = find_packages(), # find __init__.py packages\n    classifiers=[\n        'Programming Language :: Python :: 3',\n    ],\n)\n\n```\n\n只需要关心 setup 函数的参数中 packages 、 ext_modules 定义下的模块。\n\n- find_packages() 会自动寻找根目录下所有带有 `__init__.py` 的包导入到 Python3 的 site-packages 中，import 的时候就会找到它。\n- ext_modules 是需要经过编译的 C 模块。\n\n## 通用 Python 模块开发\n\n以 maix 模块为例，完全用 Python 实现的模块需要按以下结构进行构建。\n\n- maix/`__init__.py`\n- maix/video.py\n- maix/xxxxx.py\n\n首先 setuptools 打包系统会找到该模块的 maix 文件夹并将其安装到 `site-packages/maix` 下，这样用户就可以在 Python3 中 `import maix` 了，注意它与 setup.py 的相对目录（`/maix`）与安装目录（`site-packages/maix`）位置保持一致。\n\n如何控制 from maix import * 的内容可以看 `__init__.py` 了解。\n\n```python\nfrom .video import camera\nfrom .import display\n\n__all__ = ['display', 'video', 'camera']\n```\n\n其中 `__all__` 可以控制 import 加载的模块、对象或变量，这样一个最基本的 Python 模块就制作完成了。\n\n关于编写后的测试看 [test_maix.py](https://github.com/sipeed/MaixPy3/tree/main/tests/test_maix.py) 代码可知，关于 tox 测试框架会在最后简单说明。\n\n## 关于 C++ 拓展模块开发\n\n> 2022年07月22日 已经将 camera、display、image 等模块移植采用该方式开发。\n\n[使用pybind11 将C++代码编译为python模块](https://zhuanlan.zhihu.com/p/52619334)\n\n## 关于 C 拓展模块开发\n\n> 如今已经不再推荐使用，但适合学习和了解以往的最初的开发方法。\n\n以 [libi2c](https://github.com/amaork/libi2c) 举例说明原生 C 开发的模块。\n\n如果是用 C 开发就需要配合 Makefile 的规则来操作，可以直接在 MaixPy3/ext_modules/libi2c 目录下直接运行 `make all` 进行构建，此时就会得到 `libi2c.so \\ libi2c.a \\ pylibi2c.so` 等模块。\n\n这样目标系统就可以通过 C 代码链接(-l)该 libi2c 模块执行，而 `pylibi2c.so` 模块是可以直接在 Python 里面直接 import 就可以使用的。\n\n```shell\njuwan@juwan-N85-N870HL:~/Desktop/v831_toolchain_linux_x86/MaixPy3/ext_modules/libi2c$ python3\nPython 3.8.5 (default, Jul 28 2020, 12:59:40)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import pylibi2c\n>>> pylibi2c\n<module 'pylibi2c' from '/home/juwan/Desktop/v831_toolchain_linux_x86/MaixPy3/ext_modules/libi2c/pylibi2c.cpython-38-x86_64-linux-gnu.so'>\n>>>\n```\n\n注意 `pylibi2c.so` 是经过 `python3 setup.py build_ext --inplace` 命令编译 [ext_modules/libi2c/src/pyi2c.c](https://github.com/sipeed/MaixPy3/tree/main/ext_modules/libi2c/src/pyi2c.c) 得到的模块。\n\n其中 `#include <Python.h>` 的是来自于系统的 `usr/include` 目录，这取决于你的编译环境。\n\n> 注意，编译通过不代表可以运行，如果发现运行时丢失函数（undefined symbol），可以通过 ldd 查询 .so 依赖函数, 通过 nm -D 查询 .a 函数，通过 readelf -e 查询程序编译版本，有些平台可能没有 ldd 的话，就用 `readelf -d /bin/ls | grep \"Shared library\"` 来查看了，缺啥就往环境里补就对了。\n\n### 导入 pyXXX.c 的 C 拓展模块\n\n对于 make / gcc 的模块包以 ext_modules/xxxx 方式加入 MaixPy3 的编译环境（setup.py）， 请确保该包可以跨平台编译通过后，同步修改 [MaixPy3/envs/general.py](https://github.com/sipeed/MaixPy3/blob/main/envs/general.py) 的 ext_modules 模块。\n\n```python\n\nfrom setuptools import Extension\nfrom .utils import get_srcs\n\nlibi2c_module = Extension('pylibi2c',  include_dirs=[\n                          'ext_modules/libi2c/src'], sources=get_srcs('ext_modules/libi2c/src'))\n\n_maix_module = Extension('_maix', include_dirs=['ext_modules/_maix/include'],\n                         sources=get_srcs('ext_modules/_maix'),\n                         libraries=[\n    \"jpeg\"\n],\n)\n\n_maix_camera_module = Extension('_maix_camera', include_dirs=['ext_modules/_maix_camera/include'],\n                                sources=get_srcs('ext_modules/_maix_camera'),\n                                )\n\n_maix_display_module = Extension('_maix_display', include_dirs=['ext_modules/_maix_display/include'],\n                                 sources=get_srcs('ext_modules/_maix_display'),\n                                 )\n\n_maix_modules = [\n    libi2c_module,\n    _maix_module,\n    _maix_camera_module,\n    _maix_display_module\n]\n\n_maix_data_files = [\n\n]\n\n_maix_py_modules = [\n    \"numpy\",\n    \"opencv-python3\",\n    \"opencv-python\",\n    \"Pillow\",\n    \"rpyc\",\n    \"gpiod\",\n    \"evdev\",\n    \"spidev\",\n    \"pyserial\"\n]\n```\n\n以 _maix_module 为例，在加入编译之前，该包结构如下（目录结构可能会过时）。\n\n- ext_modules/_maix\n- ext_modules/_maix/include/_maix.h\n- ext_modules/_maix/_maix.c\n- ext_modules/_maix/setup.py\n- /example/test__maix.py\n\n此时我们可以在 MaixPy3 根目录下使用 `python3 setup.py build` 调用 [setup.py](https://github.com/sipeed/MaixPy3/blob/main/setup.py) 進行构建，默认构建 linux_x86_64 的包。\n\n```python\n#!/usr/bin/env python\n\n\"\"\"\nsetup.py file for MaixPy3\n\"\"\"\n\nimport sys\nfrom setuptools import setup, Extension, find_packages\n\next_modules = []\ndata_files = []\npy_modules = []\n\nif 'maix_v831' in sys.argv:\n  sys.argv.remove('maix_v831')\n  from envs.maix_v831 import _maix_modules, _maix_data_files, _maix_py_modules\nelse:\n  from envs.general import _maix_modules, _maix_data_files, _maix_py_modules\n\next_modules.extend(_maix_modules)\ndata_files.extend(_maix_data_files)\npy_modules.extend(_maix_py_modules)\n\n```\n\n如果在本机 Python 编译时出现如下错误：\n\n```shell\next_modules/_maix/pyCamera.c:4:10: fatal error: jpeglib.h: 没有那个文件或目录\n    4 | #include \"jpeglib.h\"\n      |          ^~~~~~~~~~~\ncompilation terminated.\n```\n\n运行 `sudo apt-get install libjpeg-dev` 后会在本机 usr/include 和 usr/bin 中加入 libjpeg 的模块，其他编译链同理。\n\n注意 Extension 的代码的链接时的相对地址（include_dirs & sources），以及本地打包时链接时缺少的（.h）文件，注意 [MANIFEST.in](https://github.com/sipeed/MaixPy3/tree/main/MANIFEST.in) 会链接本地的文件加入 Python 模块的打包。\n\n> 默认配置下打包中不会带入模块的（.h）文件，这会导致运行 tox 自动化打包构建模块时出错。\n\n```in\ninclude ext_modules/libi2c/src/*.h\ninclude ext_modules/_maix/include/*.h\n```\n\n> 关于 setup.py 的用法可以参考 [2021年，你应该知道的Python打包指南](https://frostming.com/2020/12-25/python-packaging)\n\n### 编写 C 拓展模块的参考\n\n接下来说明 CPython 的代码编写规范说明：\n\n- 如何编写一个 CPython 模块（PyModule）。\n- 如何 CPython 模块添加类对象（全局对象）、全局函数、全局变量。\n- 一个 PyObject 类对象的结构代码。\n- 标准 CPython 模块的命令规则。\n\n以 MaixPy3/ext_modules/_maix 模块为例，首先提供一个 C 实现的 Python 模块入口 [_maix.c](https://github.com/sipeed/MaixPy3/tree/main/ext_modules/_maix/_maix.c) 。\n\n```c\n\n#include \"_maix.h\"\n\n#define _VERSION_ \"0.1\"\n#define _NAME_ \"_maix\"\n\nPyDoc_STRVAR(_maix_doc, \"MaixPy Python3 library.\\n\");\n\nstatic PyObject *_maix_help() {\n    return PyUnicode_FromString(_maix_doc);\n}\n\nstatic PyMethodDef _maix_methods[] = {\n    {\"help\", (PyCFunction)_maix_help, METH_NOARGS, _maix_doc},\n    {NULL}\n};\n\nvoid define_constants(PyObject *module) {\n    PyModule_AddObject(module, \"_VERSION_\", Py_BuildValue(\"H\", _VERSION_));\n}\n\nstatic struct PyModuleDef _maixmodule = {\n    PyModuleDef_HEAD_INIT,\n    _NAME_,         /* Module name */\n    _maix_doc,\t/* Module _maixMethods */\n    -1,\t\t\t    /* size of per-interpreter state of the module, size of per-interpreter state of the module,*/\n    _maix_methods,\n};\n\nPyMODINIT_FUNC PyInit__maix(void)\n{\n\n    PyObject *module;\n\n    if (PyType_Ready(&CameraObjectType) < 0) {\n        return NULL;\n    }\n\n    module = PyModule_Create(&_maixmodule);\n    PyObject *version = PyUnicode_FromString(_VERSION_);\n\n    /* Constants */\n    define_constants(module);\n\n    /* Set module version */\n    PyObject *dict = PyModule_GetDict(module);\n    PyDict_SetItemString(dict, \"__version__\", version);\n    Py_DECREF(version);\n\n    /* Register CameraObjectType */\n    Py_INCREF(&CameraObjectType);\n    PyModule_AddObject(module, Camera_name, (PyObject *)&CameraObjectType);\n\n    return module;\n}\n\n\n```\n\n此时 Python 在 import 该模块的时候就会调用 PyInit_xxxx 函数进行初始化，在 Python 里 import 该模块只会执行一次，想要再次执行需要 reload 函数（`from imp import reload`）。\n\n通过 `PyModule_AddObject` 注册 PyObject 对象到该模块中，而该对象被公开到一个头文件当中进行交换，从而给 PyModule 提供多个 PyObject 的实现，添加模块的全局变量与此同理。\n\n```c\nstatic PyMethodDef _maix_methods[] = {\n    {\"help\", ()_maix_help, METH_NOARGS, _maix_doc},\n    {NULL}\n};\n```\n\n通过 `_maix_methods` 结构体为模块添加全局函数，如果你认为某个函数是公共函数，则将其放置模块顶层，表示全局公共函数。\n\n### PyObject 的结构参考\n\n一个基础的格式参考如下：\n\n定义一个对象必要的对外引用，将模块和对象实现分离，模块再通过（.h）文件链接对象实现，可见 [MaixPy3/ext_modules/_maix_camera/include/_maix_camera.h](https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_camera/include/_maix_camera.h) 。\n\n```c\n\n#ifndef _MAIX_CAMERA_H\n#define _MAIX_CAMERA_H\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\n#include <Python.h>\n\n/* Macros needed for Python 3 */\n#ifndef PyInt_Check\n#define PyInt_Check PyLong_Check\n#define PyInt_FromLong PyLong_FromLong\n#define PyInt_AsLong PyLong_AsLong\n#define PyInt_Type PyLong_Type\n#endif\n\nPyDoc_STRVAR(VirtualCamera_name, \"VirtualCamera\");\nextern PyTypeObject VirtualCameraObjectType;\n\n// #define V831Camera\n#ifdef V831Camera\nPyDoc_STRVAR(V831Camera_name, \"V831Camera\");\nextern PyTypeObject V831CameraObjectType;\n#endif\n\n#ifdef  __cplusplus\n}\n#endif\n\n#endif\n\n```\n\n此时（PyInit__maix）就可以加载该对象（CameraObjectType）到 _maix 模块当中。\n\n```c\nif (PyType_Ready(&VirtualCameraObjectType) < 0) {\n    return NULL;\n}\n\n/* Register VirtualCameraObjectType */\nPy_INCREF(&VirtualCameraObjectType);\nPyModule_AddObject(module, VirtualCamera_name, (PyObject *)&VirtualCameraObjectType);\n\n```\n\n现在看到 PyObject 的实现参考，以 [MaixPy3/ext_modules/_maix_camera/_camera_virtual.c](https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_camera/_camera_virtual.c) 为范本。\n\n```c\n\nPyDoc_STRVAR(VirtualCameraObject_type_doc, \"VirtualCamera(width, height) -> VirtualCamera object.\\n\");\ntypedef struct\n{\n  PyObject_HEAD;\n  unsigned int width, height;\n} VirtualCameraObject;\n\nstatic PyGetSetDef VirtualCamera_getseters[] = {\n    {\"width\", (getter)VirtualCamera_get_width, (setter)VirtualCamera_set_width, VirtualCamera_width_doc},\n    {\"height\", (getter)VirtualCamera_get_height, (setter)VirtualCamera_set_height, VirtualCamera_height_doc},\n    {NULL},\n};\n\nPyTypeObject VirtualCameraObjectType = {\n    PyVarObject_HEAD_INIT(NULL, 0)\n    VirtualCamera_name,                           /* tp_name */\n    sizeof(VirtualCameraObject),                      /* tp_basicsize */\n    0,                                        /* tp_itemsize */\n    (destructor)VirtualCamera_free,                   /* tp_dealloc */\n    0,                                        /* tp_print */\n    0,                                        /* tp_getattr */\n    0,                                        /* tp_setattr */\n    0,                                        /* tp_compare */\n    0,                                        /* tp_repr */\n    0,                                        /* tp_as_number */\n    0,                                        /* tp_as_sequence */\n    0,                                        /* tp_as_mapping */\n    0,                                        /* tp_hash */\n    0,                                        /* tp_call */\n    VirtualCamera_str,                                /* tp_str */\n    0,                                        /* tp_getattro */\n    0,                                        /* tp_setattro */\n    0,                                        /* tp_as_buffer */\n    Py_TPFLAGS_DEFAULT | Py_TPFLAGS_BASETYPE, /* tp_flags */\n    VirtualCameraObject_type_doc,                     /* tp_doc */\n    0,                                        /* tp_traverse */\n    0,                                        /* tp_clear */\n    0,                                        /* tp_richcompare */\n    0,                                        /* tp_weaklistoffset */\n    0,                                        /* tp_iter */\n    0,                                        /* tp_iternext */\n    VirtualCamera_methods,                            /* tp_methods */\n    0,                                        /* tp_members */\n    VirtualCamera_getseters,                          /* tp_getset */\n    0,                                        /* tp_base */\n    0,                                        /* tp_dict */\n    0,                                        /* tp_descr_get */\n    0,                                        /* tp_descr_set */\n    0,                                        /* tp_dictoffset */\n    (initproc)VirtualCamera_init,                     /* tp_init */\n    0,                                        /* tp_alloc */\n    VirtualCamera_new,                                /* tp_new */\n};\n```\n\n实现任何模块时需重点关注如下基本函数接口实现，忽略（Camera）前缀，且下文函数只做举例。\n\n- xxxxx_new （对象构造函数）\n- xxxxx_free （对象析构函数）\n- xxxxx_init （对象初始化函数）\n- xxxxx_getseters （对象属性定义结构）\n- xxxxx_methods （对象方法定义结构）\n\n开发上遵循基本结构即可，展示 PyArg_ParseTupleAndKeywords 传递参数用法，以 Camera_init 为例，如果不想写 keyword （kwlist） 就用 PyArg_ParseTuple 函数。\n\n```c\nstatic int Camera_init(CameraObject *self, PyObject *args, PyObject *kwds)\n{\n  // default init value\n  self->width = 640, self->height = 480;\n\n  static char *kwlist[] = {\"width\", \"height\", NULL};\n\n  if (!PyArg_ParseTupleAndKeywords(args, kwds, \"|ii:__init__\", kwlist,\n                                   &self->width, &self->height))\n  {\n    return -1;\n  }\n\n  return 0;\n}\n```\n\n为 PyObject 对象链接函数符号的时候可以看 xxxxx_getseters 和 xxxxx_methods 的结构定义。\n\n```c\nstatic PyMethodDef Camera_methods[] = {\n\n    {\"close\", (PyCFunction)Camera_close, METH_NOARGS, Camera_close_doc},\n    {\"__enter__\", (PyCFunction)Camera_enter, METH_NOARGS, NULL},\n    {\"__exit__\", (PyCFunction)Camera_exit, METH_NOARGS, NULL},\n    {NULL},\n};\n\nstatic PyGetSetDef Camera_getseters[] = {\n    {\"width\", (getter)Camera_get_width, (setter)Camera_set_width, Camera_width_doc},\n    {\"height\", (getter)Camera_get_height, (setter)Camera_set_height, Camera_height_doc},\n    {NULL},\n};\n```\n\n以 Python3 的 _maix.Camera 为例：\n\n```python\n\nimport _maix\n\ntmp = _maix.Camera()\n\nprint(\"this is method\", Camera.close)\n\nprint(\"this is var\", Camera.width)\n\n```\n\n一个简单的 PyCFunction 函数实现方法如下：\n\n```c\n/* str */\nstatic PyObject *Camera_str(PyObject *object)\n{\n  PyObject *dev_desc = PyUnicode_FromString(\"Camera_str\");\n\n  return dev_desc;\n}\n```\n\n如果是定义模块的全局函数则可以配置 METH_NOARGS 并移除函数参数，参考如下代码。\n\n```c\n\nstatic PyObject *_maix_help() {\n    return PyUnicode_FromString(_maix_doc);\n}\n\nstatic PyMethodDef _maix_methods[] = {\n    {\"help\", (PyCFunction)_maix_help, METH_NOARGS, _maix_doc},\n    {NULL}\n};\n\n```\n\n关于编写 CPython 模块的参考资料很多，这里只说明 MaixPy3 模块常用的程序设计，具体到函数的如何实现的细节就不在此赘述。\n\n### CPython 的内存标记用法\n\n可知 Python 拥有自动回收内存的 gc 机制，但在使用 Python C/C++ API 扩展 Python 模块时，对象指针标记不当可能会导致扩展的模块存在内存泄漏，可以使用 Py_INCREF（增加） & Py_DECREF（减少） 指针引用计数。\n\n```c\nPy_INCREF(ref);\n......\nPy_DECREF(ref); // Py_XDECREF(ref);\n```\n\n对应 Python 代码就是：\n\n```python\nref = 1\n....\ndel ref\n```\n\n可以理解为想要 gc 主动释放一个对象，就需要将其引用标志减少到无（0）。\n\n关于标记指针的说明上有用的文章。\n\n- 在开发时的注意事项请查阅 [使用 C 写 Python 模块时内存回收管理，Py_INCREF() 和 Py_DECREF() 的使用方式和注意点](https://neucrack.com/p/340)\n- 关于原理性的源码解析 [解密Python中的垃圾回收机制](https://www.cnblogs.com/traditional/p/13698244.html)\n\n如果你不能确定当前指针是否已经被回收，则你可以在使用前对 PyObject 结构指针进行引用计数的判断，也可以对该结构的类型做判断，从而确保可以操作该对象。\n\n```c\n\nassert(self->ob_refcnt > 0);\n\nPyAPI_DATA(PyTypeObject) PyBool_Type;\n#define PyBool_Check(x) Py_IS_TYPE(x, &PyBool_Type)\n\n```\n\n这样你就可以放心的操作内部创建的对象实例了。\n\n### CPython 模块的编写约束\n\n因为强调面向接口编程，所以 Python 模块下的 libXXXX 模块都是在各自的仓库编译通过后，再通过 setup.py 模块定义接口之间进行链接的，有些子仓库就是这么来的。\n\n也就是说不对编写代码风格做约束，但会对模块的接口做约束。\n\n要求每个模块的层次关系分离，以模块（PyModule）、对象（PyObject）、方法（PyCFunction）为接口参考，有如下结构。\n\n```shell\n+----------+         +-------------+\n|          +---------+ PyCFunction | 全局函數\n| PyModule |         +-------------+\n|          +<---+\n+----------+    |\n                |模块对象\n             +--+-------+\n             | PyObject +<---+\n             +----------+    |\n                             |\n                     +-------+-----+\n                     | PyCFunction | 成员函數\n                     +-------------+\n```\n\n因此请遵循该接口设计进行 Python 模块的开发。\n\n## 一些额外的内容\n\n### 使用 bdist_wheel 打包对应平台 wheel 包\n\n打包成对应平台的 wheel 的 bdist_wheel 的命令需要 setuptools 中支持。\n\n> 而 distutils 只可以构建 bdist 包。\n\nbdist_wheel 是将当前代码构建的最终文件都打包好，然后在安装的时候只需要释放到具体的安装目录下就结束了，这对于一些不能进行编译工作的硬件来说是极好的。\n\n确认 wheel 包是否可以被安装，只需要看名称就知道了，例如 `python3_maix-0.1.2-cp38-cp38-linux_x86_64.whl` 包，我们可以看到 `cp38-cp38-linux_x86_64` 标识。\n\npip 在安装的时候就会通过 `from pip._internal.utils.compatibility_tags import get_supported` 函数判断当前系统是否可以支持这个包，如果你改名了，它也是可以安装进去的,但能不能运行就取决于系统环境了，注意 armv7.whl 和 armv7l.whl 并不相同。\n\n> 细节阅读 [2021 年 当安装 wheel 出现 whl is not a supported wheel on this platform. 的时候](https://www.cnblogs.com/juwan/p/14250104.html)\n\n###  自动化测试框架 tox 的使用说明\n\n在本机上使用 `pip3 install tox` 完成安装，接着在 MaixPy3 根目录下运行 tox 即可。\n\n它会自动构建指定的 Python 虚拟测试环境，进行打包构建，安装解包的测试，最后会收集整个目录下的 `test_*.py` 的代码加入到自动测试当中，如果你不想让个别代码参与测试，你可以改名成 `no_test_*.py` 方便排除和保留文件。\n\n更多请自行查阅 [Python 任务自动化工具 tox 教程](https://www.cnblogs.com/daniumiqi/p/12179453.html) 和官方文档 [tox.readthedocs.io](tox.readthedocs.io) 。\n\n### *关于 V831 或其他平台芯片如何使用\n\n以上文档为通用说明，使用方法差异的地方在于调用 Python 指令有所不同。\n\n例如加载 V831 等其他平台的 SDK 环境后，要将上述命令中的 python3 改成对应 SDK 环境的 python3.8 用以调用交叉编译的 Python 解释器，从而完成目标 arm 平台的交叉编译，这是由 SDK 提供时决定的，其他平台统一按这个结构载入即可。\n\n### 调用 get-pip.py 手动为 Python pip 安装指定包。\n\n有时候一些交叉编译里面的 Python 环境可能会缺少 pip ，如果想要安装包，就可以用这样的方式从外部装进去。\n\n- `./python3.7 get-pip.py Cython --target=../usr/lib/python3.7/site-packages/`"}, "/soft/maixpy3/zh/others/product.html": {"title": "如何提交你的产品", "content": "---\ntitle: 如何提交你的产品\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 如何提交你的产品\n---\n\n当完成了一款芯片平台的适配后，想要合并进 MaixPy3 仓库，本文会对此做出说明。\n\n## 提供你的编译配置\n\n如 [envs/maix_v831.py](https://github.com/sipeed/MaixPy3/blob/main/envs/maix_v831.py) Python 包编译的配置，主要是用于区分适配在 maix 系列的 v831 产品，建议以芯片型号为区分，也许产品定义会不同，这时候就需要示例代码或文档来完成产品功能的区分了。\n\n## 提供你的示例代码\n\n如 [examples](https://github.com/sipeed/MaixPy3/tree/main/examples) 目录下的 maix_v831 文件夹，你可以在这里放置与你平台有关的程序、配置脚本、代码等资源。\n\n## 提供你的相关文档\n\n> 一般情况下可以不提供编译文档说明，这层的差异可能会在交叉编译链时解决，编译命令类似于 `python3.x setup.py xxxxx build` 的结构。\n\n你可以在 [docs](https://github.com/sipeed/MaixPy3/tree/main/docs) 目录下存放公共文档，也可以在 [examples](https://github.com/sipeed/MaixPy3/tree/main/examples) 下的产品文件夹里存放专用的文档。\n\n提供的文档类型可以是 markdown 或 jupyter notebook 文档。\n\n可以提供开发方法、如何编译的文档，也可以提供各类设备特有的示例文档，建议通过 jupyter notebook 文档可以达到所见即所得的效果。\n\n## 关于其他内容\n\n2021年02月24日 现在仓库里还不会收录有关于交叉编译链、量产工具、烧录工具、训练工具等等与代码或文档无关的内容。\n\n若是上述内容有不能够适应其他平台的地方，可以在 issue 里发起讨论，一起探讨和分享如何改进项目结构。\n\n> 快快把你的代码提交进来吧！"}, "/soft/maixpy3/zh/others/framework.html": {"title": "MaixPy3 架构介绍", "content": "---\ntitle: MaixPy3 架构介绍\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 认识项目整体框架（开发文档）\n---\n\n## 前言：开发基础\n\n> 本文为大佬鼠为开发者写下的指导文，希望你可以基于此进入 linux 嵌入式开发的领域。\n\n想要让 MaixPy3 软件体系更合乎你的心意，你需要花一点时间了解一下它的整体架构。\n\n掌握了它后，你就可以得到一个非常巨大的究极缝合怪，里面有开源的大量的代码供你测试和参考。\n\n这一切需要你需要具备以下基础技能：\n\n- 拥有一台 Linux 系统的电脑，至少安装了 Ubuntu20 也可以是你喜欢的桌面操作系统。\n- 知道什么是 gcc 、python3 、 opencv 、openmv 等相关软件的基础用法。\n- 最好有过 openwrt 、 buildroot 、debian 等 linux 的应用基础。\n- 了解 python3 的编译与安装，了解什么是交叉编译安装软件。\n\n如果你完全不知道上面的内容，你可以到网上获取一些资料学习一些基本内容后再继续会好一些。\n\n## 项目架构一览\n\n鼠鼠我设计的这个项目是基于 linux cpython 自顶向下设计的，它受到以下需求约束。\n\n- 用户需要 Python 语言调参验证原型功能，也需要 C / C++ 优化性能和减少内存占用用于开发商业项目。\n\n- 尽可能最大程度的跨平台支持 Python / C++ / C 开发框架。\n\n所以形成了两大 [libmaix](github.com/sipeed/libmaix) [maixpy3](github.com/sipeed/maixpy3) 开发仓库，前者为 C / C++ ，后者为 Python / C / C++ 实现。\n\n对应的系统层级关系如下：\n\n```bash\n+----------------------------------------+\n|                                        |\n|    User develop sipeed all product.    |\n|                                        |\n+----------------------------------------+\n\n+-----------------+     +----------------+\n|                 |     |                |\n| Run Python Code |     | Run C/C++ Code |\n|                 |     |                |\n+--------^--------+     +-------^--------+\n         |                      |\n   +-----+-----+                |\n   |           |                |\n   |  maixpy3  |                |\n   |           |                |\n   +-----^-----+                |\n         |                      |\n    +----+----+                 |\n    |         |                 |\n    | libmaix +-----------------+\n    |         |\n    +----^----+\n         |\n+----------------------------------------+\n|  +-------------+                       |\n|  |             |   openwrt   debian    |\n|  |    Linux    |                       |\n|  |             |   armbian   ubuntu    |\n|  +-------------+                       |\n+-------------------------+------------+-+\n      ^        ^          ^            ^\n      |        |          |            |\n+-----++ +-----++ +-------++ +---------+-+\n|      | |      | | x86/64 | |  AX620A   |\n| V83X | | R329 | | debian | |   V85X    |\n|      | |      | | ubuntu | |  more...  |\n+------+ +------+ +--------+ +-----------+\n\n```\n\n所以项目的目录结构如下：\n\n```bash\n\njuwan@juwan-n85-dls:~/v83x/MaixPy3$ tree -I build -I \"source|test|reference|common|CMake|words|waves|utils|asr_lib|tea|assets|tools|inc|src|lib|include|compile|dist|main|example|doc|lvgl\" -P . > result.log\njuwan@juwan-n85-dls:~/v83x/MaixPy3$ cat result.log\n.\n├── docs\n├── envs\n├── ext_modules\n│   ├── libi2c\n│   │   └── tests\n│   ├── libmaix\n│   │   ├── components\n│   │   │   ├── libmaix\n|       |   |   |-- lib\n|       |   |   |   -- arch\n|       |   |   |       |-- desktop\n|       |   |   |       |   |-- libmaix_cam\n|       |   |   |       |   |-- libmaix_disp\n|       |   |   |       |   |-- libmaix_image\n|       |   |   |       |   -- libmaix_utils\n|       |   |   |       |-- r329\n|       |   |   |       |   |-- opencv4\n|       |   |   |       |   -- src\n|       |   |   |       |       |-- libmaix_cam\n|       |   |   |       |       |   -- src\n|       |   |   |       |       |-- libmaix_disp\n|       |   |   |       |       |   -- src\n|       |   |   |       |       |-- libmaix_image\n|       |   |   |       |       |   -- src\n|       |   |   |       |       |-- libmaix_nn\n|       |   |   |       |       |   -- src\n|       |   |   |       |       |-- libmaix_nn_app\n|       |   |   |       |       |   |-- libmaix_nn_app_Classifier\n|       |   |   |       |       |   -- libmaix_nn_app_FaceRecognize\n|       |   |   |       |       |-- libmaix_nn_decoder\n|       |   |   |       |       |   |-- libmaix_nn_decoder_retinaface\n|       |   |   |       |       |   |   -- src\n|       |   |   |       |       |   |-- libmaix_nn_decoder_yolo2\n|       |   |   |       |       |   |   -- src\n|       |   |   |       |       |   -- src\n|       |   |   |       |       -- libmaix_utils\n|       |   |   |       |           -- src\n|       |   |   |       |-- v831\n|       |   |   |       |-- v833\n|       |   |   |       -- v83x\n|       |   |   |           -- opencv4\n│   │   │   ├── maix_cv_image\n│   │   │   ├── maix_speech\n│   │   │   │   ├── Maix-Speech\n│   │   │   │   │   ├── components\n│   │   │   │   │   └── projects\n│   │   │   │   │       └── maix_asr\n│   │   │   │   └── mfcc-with-vad\n│   │   │   │       └── alsa\n│   │   │   │           └── sound\n│   │   │   └── third_party\n│   │   │       ├── apriltag\n│   │   │       ├── cJSON\n│   │   │       ├── imlib\n│   │   │       ├── libjpeg\n│   │   │       ├── sqlite3\n│   │   │       └── zbar\n│   │   └── examples\n│   │       ├── camera\n│   │       ├── display\n│   │       ├── hello-world\n│   │       ├── imlib_test\n│   │       ├── mpp_v83x_vivo\n│   │       ├── nn_mask\n│   │       ├── nn_r329_mobilenet2\n│   │       ├── nn_r329_shufflenet\n│   │       ├── nn_resnet\n│   │       ├── nn_retinaface\n│   │       ├── nn_retinaface_mdsc\n│   │       ├── nn_yolo_20class_mdsc\n│   │       ├── nn_yolo2_card_mdsc\n│   │       ├── nn_yolo2_person_mdsc\n│   │       ├── nn_yolo_number\n│   │       ├── nn_yolo_person\n│   │       ├── nn_yolo_traffic\n│   │       ├── self_learn_classifier\n│   │       ├── speech_asr\n│   │       ├── speech_mfccdo\n│   │       └── third_party_demo\n│   ├── _maix\n│   ├── _maix_camera\n│   ├── _maix_display\n│   ├── _maix_image\n│   ├── _maix_nn\n│   ├── _maix_nn_decoder\n│   ├── _maix_nn_functional\n│   ├── _maix_nn_mdsc\n│   ├── _maix_nn_new\n│   ├── _maix_speech\n│   └── _maix_vivo\n├── maix\n│   ├── _maix_vision_\n│   └── nn\n│       ├── app\n│       │   ├── classifier\n│       │   └── face\n│       └── decoder\n└── tests\n    ├── general\n    └── maix_v831\n\n68 directories, 0 files\n\n```\n\n可以得知我们开发的层级关系是对应起来的。\n\n- 在 libmaix 中开发的成果会被转化成 maixpy3 的接口，通过了底层 C/C++ 开发 components 的 examples 测试，这样就不用在 Python 解释器中开发第二遍，因为在 Python 解释器里调试是非常困难的。\n\n- 想要适配 MaixPy3 就需要在 libmaix 中移植新的平台，并且经过适配 display / camera / image / nn 等适配，目前已经适配了 V83X / R329 / desktop （无 nn 模块）等平台，按这个流程来适配一个新的芯片平台。\n\n- 如果有新功能、新的试验代码，不一定会立刻提交提供给社区用户，而是采用 libmaix examples 的方式去验证、测试、合并，这样可以处理一些不好跨平台的实现，直到大多数平台都满足要求了就可以合并回 maixpy3 的仓库代码里。\n\n- 接口采用迭代的方式去实现，比如同样的功能可以使用 C 开发也可以 C++ 开发，但最终链接回特定位置的模块是可以选择的，而这部分的选择交给 MaixPy 的 maix 模块来控制 import 的模块。\n\n目前整体框架的开发内容划分为以下几类板块：\n\n### 系统移植\n\n首先目标芯片要有一个基础的 linux 系统，[参考项目自动构建时安装的依赖](https://github.com/sipeed/MaixPy3/blob/release/.github/workflows/maixpy3_build.yml#L22-L26)。\n\n从上层桌面系统 ubuntu 来说，它至少要准备 libopencv libjpeg python3.8+ pybind11 这几个基础软件包，基于此我们可以得到一个最基础的 MaixPy3 原型开发环境。\n\n它将会协助你完成软件接口的适配与测试，如定义屏幕显示（ display ）、摄像头输入（camera）、图像处理（image）、神经网络（nn）等基础接口的通用设计。\n\n因为图像、音频处理的算法原型验证都是在桌面系统上完成的，这样像下迁移功能的时候只需要注意内存和性能优化就行，而不用考虑算法正确性、功能交互流程的设计。\n\n这里以 v831 cpu 800mhz ram 64mb 的 linux 芯片适配为例吧，回想在 2020 年底的时候我们刚拿到这个芯片是系统都进不去的程度，后来我整理了[这篇开发指导给内部的同学培训使用](https://www.cnblogs.com/juwan/p/15226245.html)，因此要基于此先将 openwrt 系统跑起来，并成功编译 python3.8 和 libopencv 作为基础环境。\n\n有了 python3 后还需要获取一些相关的 python 软件包协助软件原型验证 [envs/maix_v83x.py](https://github.com/sipeed/MaixPy3/blob/ffeb5db56e5a7481526fc6372e0be923251b613b/envs/maix_v83x.py#L245-L252) 。\n\n```\n_maix_py_modules = [\n    \"numpy\",\n    \"rpyc\",\n    \"gpiod\",\n    \"evdev\",\n    \"spidev\",\n    \"pyserial\",\n]\n```\n\n> pillow 现在已经被 imgae + libopencv 彻底取代了，主要就图像处理+屏幕显示相关的接口。\n\n此时 v831 这款芯片就具备了运行 maixpy3 的软件环境，如基础的图像处理算法模块，而其他芯片如 r329 或运行 ubuntu 的树莓派硬件同理，只是选取不同的环境配置。\n\n在继续往下开发之前，我们需要提取 系统依赖环境 gcc 和 python3 的编译环境，这样我们就可以在桌面系统（x86 linux ubuntu）上进行交叉编译了，可以参考项目主页提供的编译链[Toolchain](https://github.com/sipeed/MaixPy3#develop)。\n\n> v831 和 r329 的是从编译后的 sdk 提取出来的，但最早都是从 docker arm 的环境里编译出来的，所以适配初期至少要准备宿主机编译或实机交叉编译二选一，主要就是 linux inclue 和 lib 和 python3 解释器。\n\n### 驱动适配\n\n以上只是软件系统的基础环境，接下来底层还需要具体的硬件接口，否则软件调用接口无法真正作用在实际的硬件接口上，比如屏幕显示和摄像头输入。\n\n这时候就要来到 ext_modules/libmaix/components/libmaix/include/ 这个目录，假设上述的软件系统上不变的，只需要提供好这里所需要的接口即可。\n\n目前在这里 v831 是底层实现是闭源的，所以我们可以参考 r329 和 desktop 这两个开源的硬件实现接口。\n\n```\n|-- lib\n|   -- arch\n|       |-- desktop\n|       |   |-- libmaix_cam\n|       |   |-- libmaix_disp\n|       |   |-- libmaix_image\n|       |   -- libmaix_utils\n|       |-- r329\n|       |   |-- opencv4\n|       |       |-- libmaix_cam\n|       |       |-- libmaix_disp\n|       |       |-- libmaix_image\n|       |       |-- libmaix_nn\n|       |       |-- libmaix_nn_app\n|       |       |   |-- libmaix_nn_app_Classifier\n|       |       |   -- libmaix_nn_app_FaceRecognize\n|       |       |-- libmaix_nn_decoder\n|       |       |   |-- libmaix_nn_decoder_retinaface\n|       |       |   |-- libmaix_nn_decoder_yolo2\n|       |       -- libmaix_utils\n|       |-- v831\n|       |-- v833\n|       -- v83x\n|           -- opencv4\n```\n\n在这里，我们可以得知需要底层实现的基础模块有：\n\n- libmaix_cam\n- libmaix_disp\n- libmaix_image\n- libmaix_nn\n- libmaix_utils\n\n但并不是每个芯片平台会完整实现所有功能，比如 desktop 就没有实现 libmaix_nn ，但这并不影响它也能使用 maixpy3 的软件包，只是在编译选项的时候取消了这个模块的编译。\n\n所以会看到 desktop 桌面环境的 maixpy3 是不能 `from maix import nn` 模块的，因为没有向 python 环境提供对应的实现。\n\n在实际情况下，我们每个平台的开发者都不是一个时刻共同开发的，对于各个部件的开发我们会进行分工开发，如果是一个新功能新接口，我们会以实现的平台为参考基础给其他平台适配代码，减少重复设计和开发工作，例如鼠鼠在开发桌面环境的 camera 的时候，顺手修复了 r329 的 camera 实现，因为都是同一类接口设计下的模块。\n\n如果这个实现在其他芯片平台发现了设计不合理的情况，出现了一开始设计无法预料的情况，那也不用担心，这时候会围绕这个功能迭代设计出新的模块直到接口仍然保持一致并能够替换上去，而不是在一个设计错误的模块上不断打补丁。\n\n为了继续开发，我们仍然要继续往前走，假设底层硬件开发人员没有提供具体的硬件实现，那我们就需要提前假设好向交互的数据格式和接口，如 bytes 图像数据，又或是 rgb565 图像数据。\n\n例如摄像头会返回 raw10 yuv rgb 等图像数据，但目前我们约束当前图像之间流动的格式为 rgb ，所以在接口定义上就要求输出的是 rgb bytes 图像数据，至于底层（libmaix_cam）如何转换的我们并不关心，哪怕是虚拟的数据也可以。\n\n此时屏幕显示驱动同理，并不是所有屏幕都能吃 rgb888 的图像数据，有的是 rgb565 （r329） 有的是 argb （v831），还有得是 bgr （opencv），但如何转换就是底层（libmaix_disp）的实现问题了，比如是 rgb888 进去了自行转换成 rgb565 显示到 framebuffer 还是 rgb888 输入转换成 rgba 贴到多图层设备上（mpp vo），又或是 rgb888 输入到 cv::mat(bytes) 上 imshow 的实现都可以。\n\n以这两个模块的实现为例，说明了 maixpy3 上如何屏蔽底层的接口实现差异的设计。\n\n所以在开发新的平台和芯片的时候，不妨参考这些已有的开源实现来修改适配，目前已有的典型设计如下：\n\n- 支持硬件加速专用 mpp vivo 多媒体框架，包含显示与输入，如 v83x tina openwrt。\n\n- 通用 v4l2 video 接口，显示驱动 framebuffer 类型，如 r329 mainline armbian。\n\n- 通用 v4l2 video 接口，显示驱动 linux opencv imshow 接口，基于桌面系统（Desktop Environment）。\n\n目前摄像头和屏幕设计是均以对象的方式来设计的，如果想要得到多屏输出多摄像头输入的场合只需要根据需要提供设备路径标示符来区分就行，这只是接口设计的预留，目前还没有用上。\n\n### 图像处理\n\n在这之前已经解决了系统与驱动的适配关系，接着来到了通用的核心软件设计，解决了硬件上图像数据输入输出的问题，才能来到软件算法和数据处理的环境。\n\n在这一层只需要关注纯粹的软件代码，例如 libmaix 的 components 下的其他组件。\n\n```bash\n│   ├── libmaix\n│   │   ├── components\n│   │   │   ├── libmaix\n│   │   │   ├── maix_cv_image\n│   │   │   ├── maix_gs831\n│   │   │   ├── maix_speech\n│   │   │   └── third_party\n│   │   │       ├── apriltag\n│   │   │       ├── cJSON\n│   │   │       ├── imlib\n│   │   │       ├── libjpeg\n│   │   │       ├── sqlite3\n│   │   │       └── zbar\n│   │   └── examples\n│   │       ├── app_gs831\n│   │       ├── camera\n│   │       ├── display\n│   │       ├── hello-world\n│   │       ├── imlib_test\n│   │       ├── mpp_v83x_vivo\n│   │       ├── nn_mask\n│   │       ├── nn_r329_mobilenet2\n│   │       ├── nn_r329_shufflenet\n│   │       ├── nn_resnet\n│   │       ├── nn_retinaface\n│   │       ├── nn_retinaface_mdsc\n│   │       ├── nn_yolo_20class_mdsc\n│   │       ├── nn_yolo2_card_mdsc\n│   │       ├── nn_yolo2_person_mdsc\n│   │       ├── nn_yolo_number\n│   │       ├── nn_yolo_person\n│   │       ├── nn_yolo_traffic\n│   │       ├── self_learn_classifier\n│   │       ├── speech_asr\n│   │       ├── speech_mfccdo\n│   │       └── third_party_demo\n```\n\n- maix_cv_image 是基于 opencv 的拓展 image 实现，可以看到 maixpy3 有些接口的底层实现是直接取自 opencv 的接口设计，但 opencv 只能解决一些通用编解码格式与基础图像处理的接口，从实际的开发适配过程中会发现 openmv 的接口设计更实用，并且一些 cv 视觉算法的流程测试函数会通过这个组件使用 C++ 代码完成，对于算法开发人员来说 c++ 代码去验证算法逻辑会方便一些，至少不用到处找基础容器了。\n\n- maix_gs831 这个项目的启动入口在 app_gs831 里，在 main 中将 C 代码跳进 C++ 代码开发环境，用于提供给商业客户开发的典型示例程序，对模块分层实现了串口协议与高速扫码的分时处理，可作为落地商业项目的代码框架参考。\n\n- maix_speech 则是语音相关的接口，在这里大多数模块都是基于 c++ 实现后通过 c 符号暴露给外部调用的，这是因为用户程序如果是 c 代码则比较简单调用，但中大型程序则建议改用 c++ 代码开发。\n\n- third_party 则是引用了外部通用的 linux 库，如 apriltag、cJSON、imlib、libjpeg、sqlite3、zbar 等，可以看到它们都是跨平台通用的代码。\n\n基于此架构设计，进一步可以关注 examples 在这里是具体开发的样例代码入口，可以在这里对 maixpy3 的一些接口做测试。\n\n- camera 是一个摄像头输入输出的示例，可以通过摄像头输入图像，并显示到屏幕上，并且基于此实现了 maixpy3 的跨平台，所以 display 同理。\n\n- 而 imlib_test 是开发 openmv 传统视觉代码的测试，一些算法的移植是在桌面环境上开发完成测试后同步给底层实现的，像一些 openmv 的 find_blobs find_template find_qrcode find_apriltags 之类的函数，这些函数相比 opencv 提供的接口来说更简单更好用，而且性能更好，占用更少。\n\n- third_party_demo 而是一个第三方调用的示例，用于测试 third_party 的代码，比如 apriltag、cJSON、imlib、libjpeg、sqlite3、zbar 标准 linux 库测试。\n\n> 20210723 未来会提交有关于 lvgl + mpp vivo 的 C++ 开发的参考项目代码，满足商业更高性能低内存占用的开发场景需求。\n\n### 神经网络\n\n在一个提供了所有基础代码的开发环境上，开始做 nn 模块的适配，目前 nn 模块都是在目标芯片 + python 开发的基础上进一步向 c/c++ 的适配 load forward decode 三步骤走。\n\n而目前 nn 还处于一个不算很稳定和标准的产物，充满了碎片化，所以在设计的时候忽略了对模型精细化控制的想法，改成采用模块封装的方式，把细节都忽略，只需要调用暴露的接口，用户可以从这个 nn 模块得到某些 app 即可。\n\n拿开源的 r329 作为典型来举例，先在上位机使用 python 完成了上层的算法验证后，就要在目标芯片平台做一下模型适配和代码部署，这个目前被归纳为三个阶段，预处理阶段已经被 camera 模块按原比例 resize 成期望形状的 rgb 图像数据，中间阶段就涉及到模型的定义和推理返回结果，基于这个结果配上对应的 decode 后得到相关的结果或框的坐标，得到我们期望 AI 模型提供的结果。\n\n```bash\n|-- r329\n|   |-- opencv4\n|   -- src\n|       |-- libmaix_cam\n|       |-- libmaix_disp\n|       |-- libmaix_image\n|       |-- libmaix_nn\n|       |-- libmaix_nn_app\n|       |   |-- libmaix_nn_app_Classifier\n|       |   -- libmaix_nn_app_FaceRecognize\n|       |-- libmaix_nn_decoder\n|       |   |-- libmaix_nn_decoder_retinaface\n|       |   |-- libmaix_nn_decoder_yolo2\n|       -- libmaix_utils\n```\n\n这是底层的分层关系，接着看 maixpy3 的 nn 分层关系。\n\n```bash\n├── maix\n│   └── nn\n│       ├── app\n│       │   ├── classifier\n│       │   └── face\n│       └── decoder\n```\n\n这时候要先设计好用户的模块调用流程，例如：\n\n```python\nfrom maix import nn, camera, display\nmodel = nn.load(\"/path/to/model.bin\")\ndecode = nn.app.face()\nwhile True:\n      img = camera.camture()\n      result = model.forward(img.resize(model.input_shape))\n      face_id, box, prob = decode.run(result)\n      display.show(img.draw_box(box, prob, face_id))\n```\n\n如果上层 python 代码的流程被确定下来了，并且认为是当下最合理的结构，则底层实现需要为了这个设计而提供相应模块。\n\n如 nn.load 需要 libmaix_nn 提供基础接口，而 nn.app.face() 需要 libmaix_nn_app_FaceRecognize 提供功能，最后 decode.run 需要 libmaix_nn_decoder_retinaface 的接口。\n\n但你会发现它们是可以合并的，比如在 v831 的接口中 libmaix_nn_app_FaceRecognize 和 libmaix_nn 是放到同一个 so 下的，但这对上层逻辑是没有影响的，因为最终对同一个套文件接口实现负责就行，至于有几个 so 共同实现并不重要。\n\n因为在 python 接口的时候会被重新映射成理想的调用关系，如果底层发生了改动，也不会影响上层用户的调用逻辑，这个是必须要留意的地方，尤其是在迭代升级接口模块的时候，对用户代码来说也是无感的。\n\n至于目标芯片怎么实现，就是开发人员在 libmaix 上的具体工作了，但只要按这个结构开发，整体的代码交互逻辑就得到的了统一。\n\n可以看到 examples 里做了很多尝试，如今也在逐渐收敛这些接口的参数和用法了。\n\n```bash\n│   │   └── examples\n│   │       ├── nn_mask\n│   │       ├── nn_r329_mobilenet2\n│   │       ├── nn_r329_shufflenet\n│   │       ├── nn_resnet\n│   │       ├── nn_retinaface\n│   │       ├── nn_retinaface_mdsc\n│   │       ├── nn_yolo_20class_mdsc\n│   │       ├── nn_yolo2_card_mdsc\n│   │       ├── nn_yolo2_person_mdsc\n│   │       ├── nn_yolo_number\n│   │       ├── nn_yolo_person\n│   │       ├── nn_yolo_traffic\n│   │       ├── self_learn_classifier\n```\n\n相信未来其他芯片的代码在实现相关 AI 功能的时候都有现成的业务逻辑流程参考实现。\n\n### 开发工具\n\n在完成了底层系统的基础上，我们要回到 python 的开发问题上，我们需要一种可以满足嵌入式设备调用 python 代码并且支持实现返回运行结果（包括图片）的开发工具。\n\n这也是 maixpy3 ide 的来源，在这个开发工具上寻找了很久都没有现成的，索性就做了一个 jupyter kernel 的支持，并打包导出给用户开发和测试，也就是你下图看到的效果。\n\n![index.png](../tools/assets/index.png)\n\n这套工具内存占用大概在目标芯片中占用 11m 左右，所以要比所谓的 jupyter notebook 运行在嵌入式设备上更实际（大于 48M），就算你安装了也做不了其他事情了。\n\n为什么需要做这个呢？\n\n- 对用户的好处\n\n用户无论在哪个平台都可以使用一键安装（exe or pip）获取整套 maixpy3 python 开发环境。\n\n- 对测试的好处\n\n测试人员做过的操作和结果都可以通过 notebook 文档记录存档，有任何问题都可以向用户索要开发文档，而不是一张毫无意义的手机拍照或电脑截图，能够得到一份可以复现用户问题现场的文档，证据确诊，有图可看，有码可跑。\n\n- 对文档的好处\n\n可以合并测试的工作内容，测完功能后就写成文档，所以目前这个资料站有一半的示例文档都是直接运行文档代码得到的，不再需要人工截图，用户也不用怀疑代码是否可用，运行的代码所见即所得。\n\n- 对开发的好处\n\n有什么问题就直接发文档，其他的都不看，同样开发做了什么东西也同样直接输出文档给其他人员，这样也减少了重复无意义的测试工作，无人配合的情况下，开发可以一人分饰三人的工作内容，从开发、测试、文档提供给用户。\n\n如果没有这些开发工具的基础铺垫，一款产品到底能不能用，用户光看代码和文字描述心里是没底的。\n\n![adc-5.gif](../tools/assets/adc-5.gif)\n\n如果文档的所有代码都有运行结果和图像，那么用户会倾向于认为是自己的操作有问题，而不是怀疑代码不能运行，这一定程度上也避开了文档中很多没有意义的提问。\n\n## 一个新功能的设计、验证、开发、测试、发布的流程\n\n鼠鼠我为内部实现一个功能直到公开时定义的开发流程大致如下：\n\n1. 定义一个功能的 python 伪代码，包含加载、循环、释放等要素，确保代码可重入可多例。\n\n2. 将具体的功能代码在标准的桌面系统上经过验证，确保了移植的可行性，定义好必要的接口参数，并包含基础的单元测试，重入、边界、内存泄露等。\n\n3. 经过 libmaix 的 examples 下的开发测试后同步移植给所有 maixpy3 的支持平台，将最初 1 设计的流程代码真实跑起来后，测试的代码归档模块下的 tests 或 examples 存放好 py 的单元测试供后续的开发人员参考，后续有问题也可以翻出来复测。\n\n4. 开发与测试的工作完成后就是文档的工作，基于上述开发测试的结果，文档会分解这些代码流程到每个步骤，一步步通过文档记录和分块代码运行，确保可以复现功能并只向用户提供最简单的用法，最后将得到的所有原料作为资料引用留在文档的最底部供其他开发者参考，而用户只需要看最前面的如何最快的得到功能并调用就行。\n\n至此才会正式公开出来给到用户使用，最好大部分按这个流程来走，不然很大概率会返工。\n\n## 目前文档公开的标准\n\n- 用户文档\n\n向用户传达某个功能的最佳实践的文档，提供最简单的用法和效果，确保用户可以看到效果，建立起产品的信任感。\n\n- 测试文档\n\n针对功能接口的所有输入输出参数的覆盖性测试，方便形成 api 文档，开源软件可能会忽略这一步。\n\n- 开发文档（非必须）\n\n主要记录开发人员在完成开发内容的所有过程，用于归档和统计开发人员的开发成果，不一定可以释放出来。\n\n## 来自社区的开发过程中的参考资料\n\n如果有我没提到的地方可以跟我说，下为我们内部开发人员的一些博客和开发参考资料。\n\n- [juwan 的 linux 系统相关的](https://zzk.cnblogs.com/s?w=blog%3Ajuwan%20linux)\n\n- [neucrack ai nn 模块相关的](https://neucrack.com/c/22?type=popular)\n\n> 如果你也想向其他人分享你的开发资料和文档可以在这下面继续添加喔，"}, "/soft/maixpy3/zh/others/platform.html": {"title": "如何适配你的平台", "content": "---\ntitle: 如何适配你的平台\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 如何适配你的平台\n---\n\n> 通过【MaixPy3开发文档】可知基础的 Python3 编译、安装、测试等开发方法。\n\n本文详细地介绍了 MaixPy3 项目结构，帮助你更好的适配 MaixPy3 环境。\n\n## 2021 年的 Python 可以彻底跨平台了吗？\n\n答案是还不足够的，仍然有很多依赖底层库差异导致了 Python 模块难以跨平台兼容。\n\n虽然绝大部分软件模块（如：pil、numpy、urllib3）都支持跨平台了，但在嵌入式 linux 设备的 Python 调用硬件资源（如：video \\ audio \\ nn）的问题上，仍然不能达到理想的跨平台接口。\n\n因此 MaixPy3 是围绕一系列支持边缘 AI 的 Linux 设备来做的，短期内不会考虑所有平台（如 Android & Windows ）。\n\n## 适配 MaixPy3 流程是怎样的？\n\n除去必要 Python3 调取硬件资源的方法，在 MaixPy3 上的开发更像是自上而下的模块接口统一的工作。\n\n可以从上层软件往下要求硬件提供相关功能模块的适配。\n\n从用户角度描述常用的功能如下：\n\n- 支持显示器（display）\n- 支持摄像头（camera）\n- 支持音频录音播放（audio）\n- 支持神经网络算法（nn）\n- 支持按键、触摸、鼠标、键盘等事件（evdev）\n- 支持点灯（gpio）\n- 支持上网（network）\n- 支持访问 I2C / SPI / UART / USB 等协议外设或传感器\n\n于是适配功能的流程描述如下：\n\n1. 首先在 Linux 系统上提供上述功能模块，可以动（静）态依赖库提供，也可以系统调用提供，~~还可以直接寄存器操作~~。\n2. 接着通过更多的 Python3 拓展模块实现相应的功能，此时拥有该模块基础使用的 Python 代码。\n3. 最后在 MaixPy3 中统一存在差异的 Python 代码，屏蔽不同设备不同硬件的差异。\n\n## 以适配【显示器】为例\n\n> 由于各个产品的硬件适配程度不同，有些过程可能已经提前完成，你可以选择跳过。\n\n想要使用 Python 在屏幕上显示内容，可以先从上层 Python 代码开始描述功能，为了能够解决基本的图像处理，选择一个 Python 中经典流行通用的 PIL 图像库 [pillow](https://github.com/python-pillow/Pillow)。\n\n现在可以使用代码打开图片并显示到屏幕上了：\n\n```python\nfrom PIL import Image\nim = Image.new(\"RGB\", (640, 480), \"#FF0000\")\nim.show()\n```\n\n这时候若是从【显示器】的角度设计一个 display 模块，可以写成如下代码：\n\n```python\nfrom PIL import Image\nfrom maix import display\ndisplay.show(Image.new(\"RGB\", (640, 480), \"#FF0000\"))\n```\n\n而 `from maix import display` 的实现可以简化成如下代码：\n\n```python\nfrom PIL import Image\ndisplay = Image.new(\"RGB\", (640, 480), \"#FF0000\")\n\ndef show(img):\n  global display\n  if isinstance(img, Image.Image):\n    display.paste(img, box)\n  display.show()\n```\n\n这时候 `display` 模块的角度就是作为显示器模块，实现了同一份代码在不同类型的 Linux 设备之间产生同样的效果。\n\n![](./asserts/pil_view.jpg)\n\n在达到这样的效果验证后，就可以开始做具体的移植适配。\n\n### 准备 Linux / Python3 / pillow 等基础功能模块\n\n准备一个目标 Linux 平台上的 Python3 解释器，与之配套的还有 目标平台的 GCC 编译链与系统目录（/usr/include & /lib）相关文件。\n\n> 期间经历一系列的目标 Linux 平台的系统移植和编译操作后\n\n在确保 Linux 系统可以运行 Python 解释器后，通过 pip 下载安装 pillow 模块，验证上述 Python 实现的功能后，在 MaixPy3 的 setup.py 中给 `setup()` 函数的 `install_requires` 参数加入 `pillow` 模块。\n\n这时候用户在安装 `pip install MaixPy3` 的时候，由于 MaixPy3 依赖于 pillow 这个模块，如果安装过程中发现系统里没有，就会尝试下载编译安装 pillow 模块，但对于一些不能编译安装模块的 Linux 设备就需要系统里直接内置 pillow 模块，以减少用户的困扰。\n\n### 但运行代码后并没有效果\n\n为什么？\n\n这是因为不同平台的屏幕的显示方式（命令）有所不同，不妨从 pillow 来看看的 show 是如何工作的。\n\n```python\nfrom PIL import Image\nim = Image.new(\"RGB\", (640, 480), \"#FF0000\")\nim.show()\n```\n\n在这段代码中的 `im.show()` 最终会依赖于 [ImageShow.py](https://github.com/python-pillow/Pillow/blob/master/src/PIL/ImageShow.py) 来完成图像对象的展示。\n\n在 Linux 上是如何工作的呢？\n\n```python\n\nclass UnixViewer(Viewer):\n    format = \"PNG\"\n    options = {\"compress_level\": 1}\n\n    def get_command(self, file, **options):\n        command = self.get_command_ex(file, **options)[0]\n        return f\"({command} {quote(file)}; rm -f {quote(file)})&\"\n\n    def show_file(self, file, **options):\n        \"\"\"Display given file\"\"\"\n        fd, path = tempfile.mkstemp()\n        with os.fdopen(fd, \"w\") as f:\n            f.write(file)\n        with open(path) as f:\n            command = self.get_command_ex(file, **options)[0]\n            subprocess.Popen(\n                [\"im=$(cat);\" + command + \" $im; rm -f $im\"], shell=True, stdin=f\n            )\n        os.remove(path)\n        return 1\n\n\nclass DisplayViewer(UnixViewer):\n    \"\"\"The ImageMagick ``display`` command.\"\"\"\n\n    def get_command_ex(self, file, **options):\n        command = executable = \"display\"\n        return command, executable\n\n```\n\n可以看到 DisplayViewer 继承 UnixViewer 对象，在 show_file 的时候将图像文件缓存到临时文件（`tempfile.mkstemp()`），再通过 get_command_ex 调用 display 系统命令（程序）完成图像的显示。\n\n> 简单来说就是【在显示器上显示一张图片】的意思。\n\n![](./asserts/display_cmd.jpg)\n\n那在嵌入式 arm Linux 硬件又会是怎样的呢？\n\n在 v831 的 linux 系统中可以使用和 display 类似的 fbviewer 程序来显示一张图像。\n\n```\nroot@sipeed:/# fbviewer /home/res/logo.png\nfbv - The Framebuffer Viewer\n/home/res/logo.png\n140 x 140\n```\n\n如何注入 fbviewer 的显示接口进 pillow 模块呢？（在 `maix/__init__.py` 中有如下一段代码）\n\n```python\ntry:\n  import shutil\n  from PIL import ImageShow\n  # use fbviewer on linux\n  # os.system('ln -s /usr/sbin/fbviewer /usr/sbin/display')\n  if shutil.which(\"fbviewer\"):\n    class fbViewer(ImageShow.UnixViewer):\n      def get_command_ex(self, file, **options):\n        command = executable = \"fbviewer\"\n        return command, executable\n    ImageShow.register(fbViewer, 0)\nexcept ModuleNotFoundError as e:\n  pass\n\n```\n\n可以看到当发现系统里有 fbviewer 时就会将该类注入到 PIL 的 ImageShow 的显示接口中，又或是在系统里直接将 fbviewer 链接到 display 命令上。\n\n现在已经成功适配到具体的屏幕操作了，但这样就足够了吗？\n\n### 这样还不够，这样实现仅是完成了功能。\n\n简单分析一下，上述实现性能损耗主要发生在当图像对象进入 pillow show_file 的时候需要对其编码保存到某个临时文件（/tmp）中，然后再交给 fbviewer 去打开文件，fbviewer 对其解码后再写到 framebuffer 的设备（/dev/fb0）上。\n\n问：为什么不把图像的 rgb 数组直接写到 fb 上呢？\n\n答：没错，内部的 _maix_display 拓展模块实现是这样做的。\n\n```c++\nPyDoc_STRVAR(Display_draw_doc, \"draw()\\nDraw image(rgb888) bytes data to lcd.\\n\");\nstatic PyObject *Display_draw(V831DisplayObject *self, PyObject *args)\n{\n    PyObject *img_bytes = NULL;\n    int img_width = 0, img_height = 0;\n    if (!PyArg_ParseTuple(args, \"Oii\", &img_bytes, &img_width, &img_height))\n    {\n        return NULL;\n    }\n    if (NULL != self->disp) {\n      if (self->disp->width >= img_width && self->disp->height >= img_height) {\n          uint8_t *rgb_data = (uint8_t *)PyBytes_AS_STRING(img_bytes);\n          if (rgb_data != NULL) {\n            self->disp->draw(self->disp, rgb_data, (self->disp->width - img_width) / 2,(self->disp->height - img_height) / 2, img_width, img_height, 1);\n          }\n      }\n    }\n    Py_RETURN_NONE;\n}\n```\n\n```python\nfrom _maix_display import V831Display\n__fastview__ = V831Display(__width__, __height__)\n__fastview__.draw(img.tobytes(), __fastview__.width, __fastview__.height)\n```\n\n这就是【屏幕清屏（变黑） `dd if=/dev/zero of=/dev/fb0` 】与【显示黑色图片 `display black.bmp` 】之间存在的性能差距。\n\n至此【显示器】基本适配完成了，其他模块亦如此，但不一定每个模块都要使用这样方式进行移植，只是出于性能的考虑可以这样做。\n\n> 可以自行查阅 Linux framebuffer 相关资料了解更多。\n\n### 以 Maix 包作为通用的 Python API\n\n做完上述功能后，就要回到这里思考一个用户体验的问题（开发者也可以是用户）。\n\n如何让同一份代码在不同平台表现一致，减少用户的再次学习成本和认知成本，所以制作了一个 maix 入口模块，以减少重复实现的功能代码。\n\n> 若是不使用某个模块（maix）去约束入口代码，就会产生代码碎片化，就如同你所看到的 Linux 上各种 Python 功能模块，做同一件事，不同平台上的接口与用法都不尽相同，但你需要花费不少时间去寻找并使用，为什么不能统一常用的功能接口呢，答案肯定是可以的，但这可能需要一些时间。\n\n从摄像头获取一张图片并显示出来这样的功能，使用如下代码就可以实现这个功能，并且它在大多数平台上都是可以做到的。\n\n```python\nfrom maix import display, camera\ndisplay.show(camera.capture())\n```\n\n为了实现上述统一接口，就需要在 [maix/video.py](https://github.com/sipeed/MaixPy3/blob/main/maix/video.py) 中多次 import 直到能够匹配的平台接口，这就会产生很多肮脏的接口代码，就如下所示。\n\n```python\n\ncamera = MaixVideo()\n\ntry:\n    # use libmaix on v831\n    from _maix_camera import V831Camera\n\n    class V831MaixVideo(MaixVideo):\n\n        def __init__(self, source=\"/v831\"):\n            self.source = source\n            self.cam = None\n\n        def config(self, size=(480, 360)):\n            if self.cam == None:\n                super(V831MaixVideo, self).__init__(size)\n                self.cam = V831Camera(self.width(), self.height())\n                import time\n                time.sleep(0.2) # wait init\n                print('[camera] config input size(%d, %d)' %\n                      (self.width(), self.height()))\n\n        def read(self):\n            if self.cam == None:\n                print('[camera] run config(size=(w, h)) before capture.')\n                self.config()\n            if self.cam:\n                ret, frame = self.cam.read()\n                if ret:\n                    return frame  # bytes\n            return None\n\n        def __del__(self):\n            if self.cam:\n                self.cam.close()\n                self.cam = None\n\n    camera = V831MaixVideo()\nexcept Exception as e:\n    pass\n\ntry:\n    from cv2 import VideoCapture\n\n    class CvMaixVideo(MaixVideo):\n\n        def __init__(self, source=0):\n            super(CvMaixVideo, self).__init__((640, 480))\n            self.source = source\n            self.cam = VideoCapture(0)\n\n        def read(self):\n            ret, frame = self.cam.read()\n            if ret:\n                bgr = frame[..., ::-1]  # bgr2rgb\n                return bgr.tobytes()  # bytes\n            return None\n\n        def __del__(self):\n            self.cam.release()\n\n    camera = CvMaixVideo()\nexcept Exception as e:\n    pass\n\n```\n\n> 这样的代码并不会多次运行，只会 import 的时候载入一次。\n\n像 MaixPy3 在设计 display 和 camera 模块的时候都尽可能围绕则 pillow 和 python-opencv 模块的接口设计衍生而来的，可以看到 camera 的 MaixVideo 定义如下，是参考 opencv 结构实现的。\n\n```python\n\nclass MaixVideo():\n\n    def __init__(self, size=(640, 480)):\n        self._width, self._height = size\n\n    def width(self):\n        return self._width\n\n    def height(self):\n        return self._height\n\n    def write(self):\n        pass  # for file\n\n    def read(self):\n        return b'\\xFF\\x00\\x00' * (self._width * self._height)\n\n    def config(self, size):\n        pass\n\n    def capture(self):\n        from PIL import Image\n        tmp = self.read()\n        if tmp:\n            return Image.frombytes(\"RGB\", (self._width, self._height), tmp)\n        return None\n\n    def close(self):\n        pass  # for file\n\n```\n\n后来加入的 i2c \\ spi \\ pwm \\ gpio 也尽量以通用接口实现。\n\n但也有一些例外，如 [PyAudio](http://people.csail.mit.edu/hubert/pyaudio/) 在对接具体音频驱动设备存在 alsa 和 tinyalsa 两类接口，就需要从底层上去完成 Python 拓展 C 模块的编写，从而实现上层接口的一致，而截止 2021 年的神经网络 NN 模块实现更是千奇百怪，还难以统一。\n\n所以通过 maix 模块作为用户调用的 API 入口，重新围绕功能来抽象设计对用户友好且统一的通用接口。\n\n这样在不同平台上只需要链接不同的 Python 依赖模块即可，如 v831 链接的是 _maix_camera 模块，而 pc 上直接使用 opencv-python 模块，当然也可以是任意调用其他模块，不一定是 MaixPy3 所提供的参考模块，这取决于你的想法。\n\n## 附录：如何优化 Python 模块？（以 GPIO 为例）\n\nPython 上通用软件的接口大多都是通过 shell 接口调用系统程序完成的功能，所以在执行性能上有很大的损失。\n\n所谓经过优化实际上是通过内置代码模块的方式进行操作的，这样就减少了不必要的数据交换了。\n\n那么执行性能究竟差在哪里？除了上述说的【显示器】适配时的优化，下面再以 GPIO 的实现为例说明这个问题。\n\n如果站在使用 Python 进行的 Linux 应用编程角度，可以这样实现 GPIO 的控制。\n\n### 使用 sysfs 的接口\n\n可以在 shell 接口配置 gpio 完成输入输出、拉高拉低。\n\n```bash\nsudo su\ncd /sys/class/gpio\necho 12 > export\necho out > gpio12/direction       # io used for output\necho 1 > gpio12/value             # output logic 1 level\necho 0 > gpio12/value             # output logic 0 level\necho 12 > unexport\n```\n\n而在 Python 里可以使用 os.system() 来输入 shell 命令完成。\n\n### 使用 gpiod 的接口\n\n可以参考 [python3-gpiod](https://github.com/hhk7734/python3-gpiod) 的实现，主要它是对 /dev/gpiodchipX 设备进行操作的。\n\n```python\n\ndef gpiod_chip_open(path: str) -> Optional[gpiod_chip]:\n    \"\"\"\n    @brief Open a gpiochip by path.\n    @param path: Path to the gpiochip device file.\n    @return GPIO chip handle or None if an error occurred.\n    \"\"\"\n    info = gpiochip_info()\n\n    try:\n        fd = os_open(path, O_RDWR | O_CLOEXEC)\n    except FileNotFoundError:\n        return None\n\n    # We were able to open the file but is it really a gpiochip character\n    # device?\n    if not _is_gpiochip_cdev(path):\n        os_close(fd)\n        return None\n\n    status = ioctl(fd, GPIO_GET_CHIPINFO_IOCTL, info)\n    if status < 0:\n        os_close(fd)\n        return None\n\n    if info.label[0] == \"\\0\":\n        label = \"unknown\"\n    else:\n        label = info.label.decode()\n\n    return gpiod_chip(\n        num_lines=info.lines, fd=fd, name=info.name.decode(), label=label\n    )\n\n```\n\n可以通过 shell 接口操作 /sys/class/gpio 对象，也可以通过 `from fcntl import ioctl` 操作字符设备文件进行控制，与第一种差别不大。\n\n### 使用 mmap 的接口\n\n在 Linux 下直接读写物理地址，打开设备文件 /dev/mem 后使用 mmap 进行物理地址的映射，最后查阅数据手册获取寄存器地址读写相应的寄存器。\n\n> 节选部分代码说明意图，注意不同平台的定义和实现都不尽相同。\n\n```c++\n\nunsigned int SUNXI_PIO_BASE = 0;\nstatic volatile long int *gpio_map = NULL;\n\nint sunxi_gpio_init(void) {\n    int fd;\n    unsigned int addr_start, addr_offset;\n    unsigned int PageSize, PageMask;\n\n\n    fd = open(\"/dev/mem\", O_RDWR);\n    if(fd < 0) {\n        return SETUP_DEVMEM_FAIL;\n    }\n\n    PageSize = sysconf(_SC_PAGESIZE);\n    PageMask = (~(PageSize-1));\n\n    addr_start = SW_PORTC_IO_BASE & PageMask;\n    addr_offset = SW_PORTC_IO_BASE & ~PageMask;\n\n    gpio_map = (void *)mmap(0, PageSize*2, PROT_READ|PROT_WRITE, MAP_SHARED, fd, addr_start);\n    if(gpio_map == MAP_FAILED) {\n        return SETUP_MMAP_FAIL;\n    }\n\n    SUNXI_PIO_BASE = (unsigned int)gpio_map;\n    SUNXI_PIO_BASE += addr_offset;\n\n    close(fd);\n    return SETUP_OK;\n}\n\n```\n\n然后编写相应的 Python 拓展 C 模块调用上述接口。\n\n```c++\n\n#define PD0    SUNXI_GPD(0)\n#define PD1    SUNXI_GPD(1)\n#define PD2    SUNXI_GPD(2)\n#define PD3    SUNXI_GPD(3)\n#define PD4    SUNXI_GPD(4)\n#define PD5    SUNXI_GPD(5)\n#define PD6    SUNXI_GPD(6)\n#define PD7    SUNXI_GPD(7)\n#define PD8    SUNXI_GPD(8)\n#define PD9    SUNXI_GPD(9)\n#define PD10    SUNXI_GPD(10)\n#define PD11    SUNXI_GPD(11)\n#define PD12    SUNXI_GPD(12)\n#define PD13    SUNXI_GPD(13)\n#define PD14    SUNXI_GPD(14)\n#define PD15    SUNXI_GPD(15)\n#define PD16    SUNXI_GPD(16)\n#define PD17    SUNXI_GPD(17)\n#define PD18    SUNXI_GPD(18)\n#define PD19    SUNXI_GPD(19)\n#define PD20    SUNXI_GPD(20)\n#define PD21    SUNXI_GPD(21)\n#define PD22    SUNXI_GPD(22)\n#define PD23    SUNXI_GPD(23)\n#define PD24    SUNXI_GPD(24)\n#define PD25    SUNXI_GPD(25)\n#define PD26    SUNXI_GPD(26)\n#define PD27    SUNXI_GPD(27)\n\n#define MISO    SUNXI_GPE(3)\n#define MOSI    SUNXI_GPE(2)\n#define SCK     SUNXI_GPE(1)\n#define CS      SUNXI_GPE(0)\n\nstatic int module_setup(void) {\n    int result;\n\n    result = sunxi_gpio_init();\n    if(result == SETUP_DEVMEM_FAIL) {\n        PyErr_SetString(SetupException, \"No access to /dev/mem. Try running as root!\");\n        return SETUP_DEVMEM_FAIL;\n    }\n    else if(result == SETUP_MALLOC_FAIL) {\n        PyErr_NoMemory();\n        return SETUP_MALLOC_FAIL;\n    }\n    else if(result == SETUP_MMAP_FAIL) {\n        PyErr_SetString(SetupException, \"Mmap failed on module import\");\n        return SETUP_MMAP_FAIL;\n    }\n    else {\n        return SETUP_OK;\n    }\n\n    return SETUP_OK;\n}\n\nstatic PyObject* py_init(PyObject* self, PyObject* args) {\n\n    module_setup();\n\n    Py_RETURN_NONE;\n}\n\nPyMethodDef module_methods[] = {\n    {\"init\", py_init, METH_NOARGS, \"Initialize module\"},\n    {\"cleanup\", py_cleanup, METH_NOARGS, \"munmap /dev/map.\"},\n    {\"setcfg\", py_setcfg, METH_VARARGS, \"Set direction.\"},\n    {\"getcfg\", py_getcfg, METH_VARARGS, \"Get direction.\"},\n    {\"output\", py_output, METH_VARARGS, \"Set output state\"},\n    {\"input\", py_input, METH_VARARGS, \"Get input state\"},\n    {NULL, NULL, 0, NULL}\n};\n\n```\n\n这样与上述实现 display 模块到优化处理的思路是相通的，目的都是减少不必要的接口之间的数据交换达到最终优化的目的。\n\n### 总结\n\n无论是哪种方法本意想通过抽象封装的通用接口来解决不同硬件上的差异，但有时会因为性能和内存的问题，只能放弃抽象直接访问底层寄存器硬件以提高性能。\n\n> 上述接口的操作都是处于 linux 用户空间进行的，使用 Python 和 C 访问 /sys/class/gpio 设备在程序逻辑上并无区别，但从执行代码段和传递变量消耗的角度来看，越靠近底层的实现执行效率自然越高，通过 Python 拓展模块实现的 mmap 映射操作相对于直接使用 C 代码实现而言，两者性能差异几乎可以忽略不计，所以 Python 程序也不一定会性能低下，主要还是取决于具体的实现方式。\n\n如果还想继续提高性能，就需要把寄存器操作下到内核空间了，可能这对于一些用户来说并不是必要的，例如用户点灯相对于系统而言是低频操作，而模拟 SPI 通信需要控制 GPIO 翻转则是高频操作，而从用户的角度来说，实现这个点灯功能（低频操作）对性能的要求不敏感，可以不做优化。\n\n因此要根据硬件的实际情况，在性能与功能之间选择一个折衷的实现。"}, "/soft/maixpy3/zh/install/index.html": {"title": "如何安装 MaixPy3 ", "content": "---\ntitle: 如何安装 MaixPy3 \nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 如何安装 MaixPy3 \n---\n\n通常来说，任何支持 Python3 的设备上都可以通过 `pip3 install maixpy3 --upgrade` 来安装 MaixPy3 作为模块入口使用，但由于一些嵌入式设备和不同平台限制，所以在这些平台上需要适配。\n\n适配进度请查阅 [MaixPy3#progress](https://github.com/sipeed/MaixPy3#progress) 。\n\n> 由于 Windows 的特殊性，不鼓励用户去折腾 Windows 的编译与安装。"}, "/soft/maixpy3/zh/install/install.html": {"title": "MaixPy3 如何获取、安装、使用？", "content": "---\ntitle: MaixPy3 如何获取、安装、使用？\nkeywords: linux, MaixII-Dock, MaixSense, 安装MaixPy3\ndesc: maixpy doc: linux_x86_64 MaixPy3 如何安装？\n---\n\n## 目前 MaixPy3 适配的平台\n\n目前 MaixPy3 用于 Linux 平台，为用户提供了板子的 Python 编程基础模块、如摄像头、屏幕、I2C等传感器外设的用法、以及后续需要的图像处理、传统算法、AI 算法模块等相关功能模块，未来会陆续适配其他低端嵌入式芯片平台。\n\n- [Linux Desktop](https://github.com/sipeed/MaixPy3)\n\n- [MaixII-Dock](/hardware/zh/maixII/M2/resources.html)\n\n- [MaixSense](/hardware/zh/maixII/M2A/maixsense.html)\n\n> 2022年07月11日 MaixPy3 不支持 Windows MacOS Android 等操作系统。\n\n## 在 Linux Desktop 上使用 pip3 安装 MaixPy3\n\n> Linux Desktop 特指带桌面的 Linux 系统，如 ubuntu 、 debian 、Raspberry Pi OS 等。\n\n先在终端执行下面命令来配置 MaixPy3 所需要的 libjpeg pybind11 gcc libopencv 等基础依赖库。\n\n```bash\nsudo apt update && sudo apt install libjpeg-dev gcc python3-pybind11 libopencv-dev -qq -y && wget http://mirrors.kernel.org/ubuntu/pool/main/libf/libffi/libffi6_3.2.1-8_amd64.deb && sudo apt install ./libffi6_3.2.1-8_amd64.deb -qq -y\n```\n\n然后安装或更新 maixpy3 依赖包。\n\n```bash\npip3 install maixpy3 --upgrade\n```\n\n若是 MaixPy3 包安装后即可在终端运行代码检查版本号。\n\n```python\njuwan@juwan-n85-dls:~$ python3\nPython 3.8.10 (default, Nov 26 2021, 20:14:08)\n[GCC 9.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import maix\n>>> maix.\nmaix.nn       maix.signal   maix.version\n>>> maix.version\n'0.5.1'\n>>>\n```\n\n可以接好摄像头设备(/dev/videoX)后，在设备上输入下面命令来测试拍照\n\n```python\nfrom maix import camera, display\ndisplay.show(camera.capture().draw_string(0, 0, \"hello world!\"))\n```\n\n下面为实拍图。\n\n![](./asserts/ubuntu.png)\n\n通常来说，像树莓派 2B 这类拥有桌面环境的 linux 设备也是可以通过 pip 进行安装 Linux Desktop 分支的，使用效果一样。\n\n![](./asserts/rpi2b.png)\n\n## MaixII-Dock (m2dock) 安装与更新 MaixPy3\n\nMaixII-Dock 需要[烧录官方提供预置 MaixPy3 的 Openwrt 镜像](https://wiki.sipeed.com/hardware/zh/maixII/M2/flash.html)，你也可以手动 pip 安装更新 whl 软件包。\n\n![](./asserts/V831.jpg)\n\n## MaixSense 安装 MaixPy3\n\nMaixSense 需要[烧录官方提供预置 MaixPy3 的 Armbian 镜像](https://wiki.sipeed.com/hardware/zh/maixII/M2A/flash_system.html)，你也可以手动 pip 安装更新 whl 软件包。\n\n```shell\nroot@maixsense:~# pip3 install maixpy3\n\nRequirement already satisfied: maixpy3 in /usr/local/lib/python3.9/dist-packages (0.3.4)\nRequirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from maixpy3) (8.1.2)\nRequirement already satisfied: zbarlight in /usr/local/lib/python3.9/dist-packages (from maixpy3) (3.0)\nRequirement already satisfied: evdev in /usr/local/lib/python3.9/dist-packages (from maixpy3) (1.4.0)\nRequirement already satisfied: spidev in /usr/local/lib/python3.9/dist-packages (from maixpy3) (3.5)\nRequirement already satisfied: pyserial in /usr/local/lib/python3.9/dist-packages (from maixpy3) (3.5)\nRequirement already satisfied: rpyc in /usr/local/lib/python3.9/dist-packages (from maixpy3) (5.0.1)\nRequirement already satisfied: gpiod in /usr/local/lib/python3.9/dist-packages (from maixpy3) (1.5.0)\nRequirement already satisfied: plumbum in /usr/local/lib/python3.9/dist-packages (from rpyc->maixpy3) (1.7.0)\n\nroot@maixsense:~# python #运行python\nPython 3.9.2 (default, Feb 28 2021, 17:03:44)\n[GCC 10.2.1 20210110] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> from maix import camera,display\n>>> while True:\n···    display.show(camera.capture())\n```\n\n输出以上信息则是代表安装好了，以下为实拍图。\n![](./asserts/R329.jpg)\n\n## 其他 Linux 芯片平台\n\n目前还没有精力适配其他平台的软件包，但大都不难适配，主要的适配工作量在 nn 模块（神经网络方面的接口），而其他都是通用的。"}, "/soft/maixpy3/zh/tools/maixsense.html": {"title": "使用 MaixPy3 IDE 连接 MaixSense", "content": "---\ntitle: 使用 MaixPy3 IDE 连接 MaixSense\nkeywords: Jupyter, MaixPy3, Python, Python3\ndesc: maixpy doc: 在 MaixII-Sense 平台上使用\n---\n\n---\n\n|     时间      | 负责人 |   更新内容   |\n| :-----------: | :----: | :----------: |\n| 2022年4月24日 | wonder | 增加一点细节 |\n| 2022年2月28日 |  Rui   | 编写连接文档 |\n\n> Maixsense 仅支持通过网络连接到 MaixPy3 IDE\n\n在 MaixSense 上使用 MaixPy3 ，需要烧录内置 MaixPy3 的 armbian 系统，并且连接到网络\n\n其中烧录系统可以参考 [点我](./../../../../hardware/zh/maixII/M2A/flash_system.html)。系统用该选择带有 MaixPy3 的 armbian 镜像。\n\n## 设置 wifi\n\n对于新烧录的系统需要先使用串口来设置 wifi 连接。\n\n使用 串口 连接板子，然后这里以 mobaxterm [[点我查看简述](./../../../../hardware/zh/maixII/M2/tools/mobaxterm.html)]这个软件为例\n\n![](./assets/mobaxterm-serial-4.png)\n\n在「session setting」 对话框里选择【serial】，设置好波特率为 115200，点击【OK】后就进到如下页面\n\n![](./assets/mobaxterm-serial-5.png)\n\n> 如果提示用户名和密码的话。那么用户名和密码都是`root`。\n> 注意的是输入密码的时候是没有输入显示的，因此只管输入就行。\n\n连接板子后可以参考 [上手使用](./../../../../hardware/zh/maixII/M2A/Usages.html) 来基本用一下。\n\n如果连接软件后终端界面没有任何显示，尝试按一下回车，看看有没有信息显示出来。这是因为板子已经启动完毕，串口连接迟了。\n\n## MaixPy3 IDE 连接\n\n### 准备\n\n- 烧录好带有 MaixPy3 的 Armbian 系统\n- 连接网络进行 MaixPy3更新，确保 MaixPy3 的版本大于 0.3.4。\n\n在 linux 终端使用 `hostname -I | awk '{ print $1 }'` 来查看连网后的IP地址\n\n![](./assets/maixsense-display-ip.png)\n\n从上面收到的信息可以看到本次的 IP 地址为 192.168.0.23 .\n\n没有显示IP的话重新设置连接网络\n\n### 连接\n\n在板子终端执行下述命令(需要确认系统镜像是20220516之后的版本)来启动板子上的远程 RPyc 服务。\n\n```bash\nmaixpy3.sh\n```\n\n以后每次使用都执行一次即可\n\n<details>\n  <summary><font color=\"#4F84FF\">点开查看正常运行样子</font></summary>\n  <img src=\"./assets/maixpy3-bash.png\">\n</details>\n\n在电脑启动 MaixPy3 IDE，新建代码区，运行下面的连接代码。\n\n- 电脑在 IDE 中新建代码区并执行\n\n```python\n$connect(\"192.168.0.23\")   # 手动更改为板子的IP\nimport platform\nprint(platform.uname())\n```\n\n<details>\n  <summary><font color=\"#4F84FF\">点开查看相关操作</font></summary>\n  <img src=\"./assets/r329-start/1.jpg\">\n  <img src=\"./assets/r329-start/2.jpg\">\n  <img src=\"./assets/r329-start/3.jpg\">\n  <img src=\"./assets/r329-start/4.jpg\">\n  <img src=\"./assets/r329-start/5.jpg\">\n  <img src=\"./assets/r329-start/6.jpg\">\n</details>\n\n- 启动 MaixPy3 IDE 的时候，会弹出一个 adb 终端窗口。对于 R329 我们是使用串口和板子连接的，因此可以关掉它。\n\n> 今后使用出现 AIPU_load_graph_helper: UMD fails in allocating buffers 错误。\n> 重新使用 `maixpy3.sh` 命令就可以了\n\n具体相关使用方法可以参考[使用 MaixPy3 IDE 连接 MaixII-Dock](./0.MaixII-Dock.html),其中的**如何运行代码**和**首次尝试**都可以参考。\n\n- 另外因为板子是使用无线网络连接的 IDE，所以在 IDE 里面需要先指定一下板子IP\n\n### 开机自启\n\n目前 R329 镜像是 armbian linux 系统。\n\n可以通过修改 /etc/rc.local 来启动自己想要开机运行的脚本\n\n具体方法可以自行搜索配置 linux 开机脚本方式来启动想要的运行脚本。"}, "/soft/maixpy3/zh/tools/maixpy3-ide-overview.html": {"title": "MaixPy3 概述", "content": "# MaixPy3 概述\n\n## What?\n\n\n## Why?\n\n\n## How?"}, "/soft/maixpy3/zh/tools/MaixPy3_IDE.html": {"title": "安装 IDE", "content": "---\ntitle: 安装 IDE\nkeywords: MaixPy3 IDE\ndesc: maixpy doc: 如何连接并使用 MaixPy3 的 IDE\n---\n\n- 修改与2022年七月七日\n\n>下载站链接：[下载 MaixPy3 IDE ](https://dl.sipeed.com/shareURL/MaixII/MaixPy3-IDE)\n>百度网盘下载链接：链接：[点我](https://eyun.baidu.com/s/3htTXfaG#sharelink/path=%2F%E4%B8%8B%E8%BD%BD%E7%AB%99%E6%96%87%E4%BB%B6%2FMaixII%2FMaixPy3-IDE&parent_path=%2F%E6%B7%B1%E5%9C%B3%E7%9F%BD%E9%80%9F%E7%A7%91%E6%8A%80%E6%9C%89%E9%99%90%E5%85%AC%E5%8F%B8)\n\n## 为什么要使用 IDE ？\n\n在没有 IDE 的时候，我们是这样编程的；这是每一只远古程序猿都必备的开发技能。\n\n<img style=\"width:100%;height:auto;min-width:600px;min-height:400px;\" src=\"./assets/4in1.jpg\" >\n\n上述命令行的编程方式是上世纪 80 年代流行的开发方式，建议在有一定的 linux 基础后再来使用会比较好。\n但 2022 现代 IDE 工具的重点应该是传达出 Python 代码所运行的结果或效果。\n\n<img style=\"width:100%;height:auto;min-width:600px;min-height:400px;\" src=\"./assets/python_cmd.gif\">\n\n如果你是一名开发者，你要如何教会初学者使用你的代码？像你一样使用命令行敲出来看实际的效果吗？\n- **对初学者来说，这一定是一场灾难，所以我们需要 IDE 来结束这一场悲剧。**\n\n## 那么 MaixPy3 IDE 是什么?\n\n它是一套基于 [jupyter](https://jupyter.org/) 实现的 Python3 集成开发环境，意在帮助用户通过电脑编写 Python 代码或阅读运行他人提供的 jupyter notebook 文档后，接上硬件后点击【▶ 运行】可以实时呈现如下效果图。\n\n![example](./assets/index.png)\n\n为了方便新入门的同学进行学习，在 Jupyter 文档中你可以单步执行代码，保留输出的结果，还能将屏幕显示的内容展示出来。\n\n软件具备的特点如下：\n\n- 通过 TCP/IP 连接开发板，支持在本机运行 Python 代码，实时反馈开发板的运行结果或图片\n- 通过 jupyter notebook 文档可以保存每一次的运行结果，方便知识的传播\n- 继承 ipython 实现 Python 语法的高亮和补全功能（可以按 tab 进行补全或提示）\n\n## 如何安装 MaixPy3 IDE ？\n\n> **软件安装方法和注意事项要严格看本文说明（大佬鼠宣）**\n\nMaixPy3 IDE 的构成主要如下：\n\n- 一个在后台运行的托盘程序。\n- 一个 jupyter 服务程序。\n\n用户使用与排查流程主要如下：\n\n- 确认所用的系统平台，确定安装方法\n- 确认所用的开发硬件，查看对应的系统配置方法\n- 确认系统防火墙没有阻止 TCP 的 18811 和 18812 端口，不清楚就关闭网络防火墙或安全软件\n- 确认硬件的 MaixPy3 的 Python 包和电脑上的 IDE 版本为 0.4.0 以上\n- 确认 所用硬件的 连接和配置方法，确认没有被杀毒程序给阻止且已经授予程序权限\n- **若是仍然出现问题，将上述流程打包起来反馈给[社区](https://bbs.sipeed.com/)并@管理员解决**\n\n### 关于 Windows 平台的安装方法\n\n在文章开头处下载即可。\n\n注意事项:\n\n- 只支持 Windows7 或 Windows10 32 位以上的系统\n- 需要内核为Chromium的浏览器,比如 [Microsoft Edge](https://www.microsoft.com/zh-cn/edge) 或 Chrome。\n- Windows11 测试样本极少，需要慎重选用\n\n可以看下面的视频来看具体操作：\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?bvid=BV1SS4y1q7QX&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n补充说明：\n- 安装完 IDE 后务必安装弹出窗口的驱动\n- 记得像视频中一样卸载掉对应的 ADB 驱动\n\n> 若下载站下载很慢，可以在文章开头处获取百度云链接\n\n2022年1月15日 收到用户的反馈点\n\n曾经有自己手动配置过环境的同学，需要在删除你系统的 `C:\\Users\\（改成你电脑的用户名）\\AppData\\Roaming\\jupyter\\kernels\\rpyc` 防止调用核心时调用了过去的旧核心导致错误发生。\n\n### 关于 Mac 和 linux 其他平台的安装方法\n\n如果你不喜欢提供的 pyinstaller 打包的版本，可以自行配置 python 、jupyter 、ikernel 等基础环境，自行 pip 手动安装和配置环境。\n但是需要你具备一定的 python 基础，请阅读 [rpyc_ikernel](https://github.com/sipeed/rpyc_ikernel) 完成安装即可，手动安装脚本的方法如下：\n\n```bash\npip install rpyc_ikernel\npython -m rpyc_ikernel.install\n```\n\n此时执行 `jupyter notebook` 或 lab 即可。\n\n## MaixPy3 IDE 界面介绍\n\n下面介绍一下软件的工作区域和环境。\n\n### 程序主界面介绍\n\n![IDE_2](./assets/IDE_2.png)\n\n1. 为文件选择区，点击即可进入 Jupyter 文档中\n2. 为文件上传，是将文件上传到 MaixPy3 IDE 的工作区当中，并不是将文件上传到开发板中\n3. 为新建文件或者是文件夹\n4. 退出 MaixPy3 IDE。直接关闭浏览器的话 MaixPy3 IDE 是还会在后台中运行的\n\n---\n\n#### 新建文件\n\nMaixPy3 IDE （jupyter） 新建文件(上图中的2)的时候，可以选择多个 Python 执行核心。\n\n![IDE_2](./assets/core.png)\n\n1. 选择 Python3 的话此时里面的相关代码是运行在本机上的。\n2. 选择 RPyc-python，此时相关代码运行在板子上\n\n### jupyter notebook 文档界面\n\n![](./assets/IDE_3.png)\n\n1. 单元格工具栏，可以对单元格进行复制、粘贴、运行、停止等操作\n2. 当前单元格的属性，可以切换单元格属性为 代码 或者 markdown\n3. 显示当前文档运行代码属性单元格时所使用的内核\n4. 蓝色边框表示单元格处于命令模式。绿色边框表示单元格为编辑模式\n\n<details><summary><font color=\"#4F84FF\">Jupyter 用法简介</font></summary>\n与 vim 类似，jupyer 有命令模式和编辑模式\n<li>\n通用：<br>\n&emsp;&emsp;Shift+ Enter ：运行单元格，且以命令模式切换到下一个单元格<br>\n&emsp;&emsp;Ctrl + Enter ：运行单元格，且进入命令模式\n</li>\n<li>\n编辑模式中：<br>\n&emsp;&emsp;Esc ：进入命令模式\n</li>\n<li>\n命令模式中：<br>\n&emsp;&emsp;h :打开帮助<br>\n&emsp;&emsp;Enter :进入编辑模式<br>\n&emsp;&emsp;x :剪切单元格<br>\n&emsp;&emsp;c :复制单元格<br>\n&emsp;&emsp;v :粘贴单元格<br>\n&emsp;&emsp;dd :删除整个单元格<br>\n&emsp;&emsp;ii :终止运行\n</li>\n</details>\n\n借助于 notebook 文档，你可以看到编辑者的文本描述、代码和相关运行结果，非常适合新人边看边学\n\n编辑者文本描述\n![文本描述](./assets/IDE_4.png)\n\n相关代码实例，直接点击运行就可以看到\n![实例代码](./assets/IDE_5.png)\n\n图片展示结果，直接点击运行可以复现\n![运行结果](./assets/IDE_6.png)\n\n这也是 jupyter 的魅力之一吧，有兴趣的话也可以直接分享 ipynb 文件 来向他人展示你的代码\n\n---\n\n**注意 MaixPy3 IDE 的 jupyter notebook 的服务是运行在主机上的，因此所有相关文件和工作区都是在本机上的**\n\n## [附录]遇到问题应该如何解决？\n\n为了降低初学者的搭建和上手时间，我们做了大量的努力，但仍然无法完全解决每个人因环境差异而可能导致的不同问题。\n\n所以一些常见问题我们已经整理好，阅读过后本文后仍然出现无法解决的问题，就可以从左侧的导航中获取 [MaixPy3 的常见问题与解决办法](/soft/maixpy3/zh/question/maixpy3_faq.md) 。\n\n为了防止一些**不了解具体实现，也不看文档，但又自作主张的同学出现的问题**，大佬鼠在这里特别说明一下 IDE 的工作框架和可能遇到的问题，谨记！没有绝对完美且不出错的软件，如果有那一定是他本机上的软件。\n\n---\n\n### IDE 连接原理\n\n这里我们以典型硬件 V83X 和 R329 的为例。\n\n因为 V83X 因为支持 usb adb forward 功能，可以通过转发 TCP/IP 地址进行 IP 映射的端口绑定；而 R329 只能通过 TCP/IP 地址进行连接。但从设计上来说 IDE 只支持 TCP/IP 地址的连接方法，所以要确保 TCP 的远程调用端口 18811 和 18812 图传端口没有被防火墙阻止，那么如何确认呢？\n\n最起码的就是互相 ping 对方的 IP 确保该 IP 下的通路可行。如果 IP 确定是可以连通的，则我们需要确定远程调用的服务是否存在。\n\n通常里面需要一个 rpyc + mjpg 的服务去维持远程调用，如果是 V83X 则会自动经过 adb shell 完成调用 [实现在 github 这里](https://github.com/sipeed/rpyc_ikernel/blob/master/rpyc_ikernel/adb.py#L509-L539)；如果是 R329 则需要手动启动该服务，或配置成开机自动启动，从而避免下次手动启动。\n\n如果遇到 IDE 链接失败，就先使用板子所运行的 linux 系统中的 `ps` 命令检查维持 IDE 链接的 Python 程序（`python3 -c from maix import mjpg;mjpg.start();`）是否存在（如 S 表示正在运行）。可以参考下图中的命令\n\n![](./assets/ps.png)\n\n若是通道+服务均正常，那 IDE 就一定可以连上，还需要注意的就是 IDE 默认连接的 IP 地址是 [localhost](https://baike.baidu.com/item/localhost/2608730)；如果你本机的网卡环境比较混乱，可能你需要在 IDE 里手动指定一下 IP 才能确保正确连接上，如：`$connect(\"192.168.0.23\")`。\n\n下图为 IDE 通过无线网在 Maixsense 上运行代码的结果，同样也适用于在 V831 上。只是说明一下可以指定 IP 然后通过无线运行（需要确定板子上已经有`python3 -c from maix import mjpg;mjpg.start();`在运行，没有的话可以手动执行一下 `python3 -c \"from maix import mjpg;mjpg.start();\"`）\n\n<img src=\"./assets/r329-start/6.jpg\">\n\n## 结语\n\nMaixPy3 IDE 依赖于 [jupyter](https://github.com/jupyter/jupyter)、[rpyc_ikernel](https://github.com/sipeed/rpyc_ikernel)、[MaixPy3](https://github.com/sipeed/MaixPy3) 开源仓库实现，目的是为了让新用户更直观地运行代码，感兴趣的可以自行了解。\n\n其实所谓的 IDE 就是 jupyter 的打包版本，知道了工作机制后，还不赶紧用起来？"}, "/soft/maixpy3/zh/index.html": {"title": "What is MaixPy3?", "content": "---\ntitle: What is MaixPy3?\nkeywords: Maixpy3 官方文档\ndesc: maixpy doc: MaixPy3 是什么？能做什么？\n---\n\n中国的 [Sipeed 开源组织](https://github.com/sipeed) 在 2020 年底推出了 [MaixPy3](https://github.com/sipeed/MaixPy3) 开源软件，这是一款基于 [linux cpython](https://github.com/python/cpython) 的 Python3 软件开发工具包（SDK），借助开源 Python 编程语言实现跨平台统一和简化 Linux 嵌入式设备上开发 AIoT （人工智能物联网） = AI（人工智能） + IoT（物联网）应用，意在打造可落地的视觉 AI 应用生态，帮助更多人了解、使用 AI 技术来解决实际问题，推进全球边缘 AI 的落地化进程。\n\n## MaixPy3 能做什么？\n\n> 以下内容均是社区用户基于 MaixPy3 开发的成果分享，本文档不提供他人的二次开发成果。\n\n### MaixPy3 开源项目自述\n\n- 大佬鼠自述：MaixPy3项目开发的心路历程\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=639532689&bvid=BV1SS4y1q7QX&cid=734642901&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n> 在 2022年01月21日 该视频只说了为什么要做这款开源产品系列，此时还没有做【在线 AI 训练】和【突出产品亮点】，最终是**想要让初学者【学会视觉 AI 应用】和【开发出可落地的产品】**\n> 在 2022年06月01日 儿童节的时候【在线 AI 训练】已公开集标注训练部署一体的服务。\n\n### M2DOCK 产品功能速览\n\n- M2DOCK：国产全志 V831 开源 人工智能 神经网络 视觉 AI Python开发板\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=298543445&bvid=BV1sF411u7xb&cid=586467021&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n### M2DOCK 电赛数字送药小车\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=258696230&bvid=BV1Wa411D7DL&cid=779040049&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n### M2DOCK V831 车牌识别\n\n- 社区是检测后联网识别，官方是检测后本地识别。\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=854443414&bvid=BV1M54y1o7YH&cid=730413361&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=941105171&bvid=BV1zW4y117U4&cid=775859102&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n### M2DOCK 冰箱水果识别系统\n\n- 简易水果识别系统。基于pytorch的yolo训练模型，移植到v831，app和开发板实时显示。\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=853065689&bvid=BV1sL4y157us&cid=572354654&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n### yolov2_20classes by Maix ⅡDock(V831)\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=594688527&bvid=BV16q4y1i7rS&cid=546750387&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n### M2DOCK 人脸和口罩检测\n\n- ［maixpy3] 831 脸子姐姐的毕设人脸口罩识别\n\n<p align=\"center\">\n    <iframe src=\"//player.bilibili.com/player.html?aid=467818856&bvid=BV1X5411S7F6&cid=713976242&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n## 本开源项目适用于以下人群：\n\n1. 想要了解与学习应用视觉 AI 开发的学生、爱好者、创客等。\n2. 想要应用视觉 AI 功能解决问题，但不想浪费生命在底层原理实现的同学们。\n3. 需要对 Python AI 教学、电赛毕设、视觉应用等功能评估与验证的同行们。\n4. 有过 opencv 、openmv 、 maixpy 使用基础的老朋友们！！！\n\n## 本开源项目建议具备的背景知识\n\n- 有过 [Python 语言](./origin/python.html)编程基础，了解基本语法，如面向对象、交互解释等概念。\n\n- 有过嵌入式、单片机的基本概念，了解 IO 口、电压、串口、外设等概念。\n\n- 有使用过 maixpy K210 AI 开发板的基础（与上代[MaixPy](https://github.com/sipeed/MaixPy)开源产品联动）。\n\n当你有这些基础概念后，你可以减少很多犯错的次数，避免踩到一些不必要的坑。\n\n## [MaixPy3](https://github.com/sipeed/MaixPy3) star-history\n\n![GitHub forks](https://img.shields.io/github/forks/sipeed/maixpy3.svg?style=social) ![GitHub stars](https://img.shields.io/github/stars/sipeed/maixpy3.svg?style=social) ![GitHub watchers](https://img.shields.io/github/watchers/sipeed/maixpy3.svg?style=social)\n\n<iframe style=\"width:100%;height:auto;min-width:600px;min-height:400px;\" src=\"https://star-history.com/embed?secret=#sipeed/MaixPy3&Date\" frameBorder=\"0\"></iframe>\n\n> 快来与我们一起同行吧！ “Life is short. You need Sipeed.”"}, "/soft/maixpy3/zh/usage/Audio/play_mp4.html": {"title": "MaixPy3 播放视频", "content": "# MaixPy3 播放视频\n\n| 更新时间 | 负责人 | 内容 | 备注 |\n| --- | --- | --- | --- |\n| 2021年12月4日 | Rui | 初次编写文档 | ---- |\n| 2022年1月18日 | dalaoshu | 修改文档，增加效果图 | ---- |\n| 2022年4月24日 | Coty | 修改视频播放代码,修改了提供视频的尺寸 | ---- |\n\n实际上它是由 ffmpeg + pyav 编译而来，它们分别是什么呢？\n\nFFmpeg 是一套可以用来记录、转换数字音频、视频，并能将其转化为流的开源计算机程序。\n\nhttps://pyav.org/docs/develop/\n\nPyAV 是一个用于 FFmpeg 的 python 绑定。通过容器、流、包、编解码器和帧直接和精确地访问媒体。它公开了一些数据的转换，并帮助您从其他包(例如 Numpy 和 Pillow )获取数据。\n\n目前测试的视频格式有 mp4 和 avi，其他格式还没有进行测试，以下是我们提供的测试视频供确认效果。\n\n<p align=\"center\">\n  <iframe src=\"//player.bilibili.com/player.html?aid=717126108&bvid=BV1dQ4y1f7RN&cid=385731209&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\" style=\"max-width:640px; max-height:480px;\"> </iframe>\n</p>\n\n## 如何播放视频？\n\n这里使用的是转换后的 output_240_240.mp4 [测试视频](https://dl.sipeed.com/shareURL/MaixII/MaixII-Dock/example)，从这里获得视频后存放到 Linux 系统的 root 目录中，将 `path_to_video` 的参数修改成所存放视频路径，如：`'/root/output_240_240.mp4'`，其他视频同理，需要注意的是 v831 的性能很弱，可能最高就播放到软解 h264 30fps 了，硬解资源不被 FFmpeg 所提供。\n\n> ffmpeg 转换命令 ffmpeg -r 30 -i badapple_240_60fps.mp4 -vf scale=240:240,setdar=1:1 output.mp4\n\n```python\nimport pyaudio, av, os\nfrom maix import display, camera, image\n# ffmpeg -r 30 -i badapple_240_60fps.mp4 -vf scale=240:240,setdar=1:1 output.mp4\n# adb push ./output.mp4 /root/\npath_to_video = '/root/output_240_240.mp4'\nif os.path.exists(path_to_video):\n    try:\n        container = av.open(path_to_video)\n        ai_stream = container.streams.audio[0]\n        vi_stream = container.streams.video[0]\n        fifo = av.AudioFifo()\n        p = pyaudio.PyAudio()\n        ao = p.open(format=pyaudio.paFloat32, channels=1, rate=44100, output=True)\n        for frame in container.decode(video=0, audio=0):\n            if 'Audio' in repr(frame):\n                frame.pts = None\n                fifo.write(frame)\n                for frame in fifo.read_many(4096):\n                    ao.write(frame.planes[0].to_bytes())\n            if 'Video' in repr(frame):\n                img = image.load(bytes(frame.to_rgb().planes[0]), (vi_stream.width, vi_stream.height))\n                display.show(img)\n    finally:\n        ao.stop_stream()\n        ao.close()\n        p.terminate()\n```\n\n## 如何录制视频？\n\n2022年07月26日 根据 pyav.org 文档加源码整理如下代码，从录制编码到播放（x264 支持手机预览但解码性能低），注意该代码只在 av 9.2.0 版本的库可用，需要更新 0.5.2 系统底包的（av 8.0.3）喔（这种需要编译的包 pip 是得不到的）。\n\n```python\n\nfrom maix import display, image\n\npath_to_video = 'test.mp4'\n\nimport av\n\nduration, fps = 4, 10\ntotal_frames = duration * fps\ncontainer = av.open(path_to_video, mode='w')\nstream = container.add_stream('h264', rate=fps) # h264 or mpeg4\nstream.width = 320\nstream.height = 240\nstream.pix_fmt = 'yuv420p'\n\nfor frame_i in range(total_frames):\n    img = image.new(size=(stream.width, stream.height), mode='RGB')\n    import time\n    img.draw_string(0, 0, str(time.time()), 2)\n    frame = av.VideoFrame(img.width, img.height, 'rgb24')\n    frame.planes[0].update(img.tobytes())\n    for packet in stream.encode(frame):\n        container.mux(packet)\n\n#Flush stream\nfor packet in stream.encode():\n    container.mux(packet)\n\n#Close the file\ncontainer.close()\n\n#Play the video\n\ncontainer = av.open(path_to_video)\nstream = container.streams.video[0]\nfor frame in container.decode(video=0):\n    if 'Video' in repr(frame):\n        img = image.load(bytes(frame.to_rgb().planes[0]), (stream.width, stream.height))\n        display.show(img)\n```\n\n试试吧，可以看到录制的是我们填充的 image 图像，打印了时间字符串。"}, "/soft/maixpy3/zh/usage/Audio/audio.html": {"title": "音频操作", "content": "---\ntitle: 音频操作\nkeywords: 音频操作, MaixPy3, Python, Python3\ndesc: maixpy doc: 音频操作\n---\n\nMaixPy3 关于音频相关操作采用的是PyAudio 库，PyAudio 为跨平台音频 I/O 库 PortAudio 提供了 Python 绑定，帮助用户轻松地在各种平台上播放和录制音频。\n\n[pyaudio 官方文档](http://people.csail.mit.edu/hubert/pyaudio/docs/)\n\n## 录音操作\n\n```python\nimport pyaudio\nimport wave\n\nCHUNK = 1024\nFORMAT = pyaudio.paInt16\nCHANNELS = 2\nRATE = 44100\nRECORD_SECONDS = 5\nWAVE_OUTPUT_FILENAME = \"test.wav\"\n\np = pyaudio.PyAudio()\n\nstream = p.open(format=FORMAT,\n                channels=CHANNELS,\n                rate=RATE,\n                input=True,\n                frames_per_buffer=CHUNK)\n\nprint(\"* recording\")\n\nframes = []\n\nfor i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n    data = stream.read(CHUNK)\n    frames.append(data)\n\nprint(\"* done recording\")\n\nstream.stop_stream()\nstream.close()\np.terminate()\n\nwf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\nwf.setnchannels(CHANNELS)\nwf.setsampwidth(p.get_sample_size(FORMAT))\nwf.setframerate(RATE)\nwf.writeframes(b''.join(frames))\nwf.close()\n```\n\n## 播放音频\n\n```python\nimport pyaudio\nimport wave\nimport sys\nCHUNK = 1024\nwf = wave.open(r'test.wav', 'rb')#(sys.argv[1], 'rb'\np = pyaudio.PyAudio()\n\nstream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n                channels=wf.getnchannels(),\n                rate=wf.getframerate(),\n                output=True)\n\ndata = wf.readframes(CHUNK)\n\nwhile len(data) > 0:\n    stream.write(data)\n    data = wf.readframes(CHUNK)\n\nstream.stop_stream()\nstream.close()\n\np.terminate()\n```"}, "/soft/maixpy3/zh/usage/Audio/speech.html": {"title": "", "content": ""}, "/soft/maixpy3/zh/usage/train_AI/train_yolov2.html": {"title": "目标检测本地训练教程", "content": "# 目标检测本地训练教程\n\n| 时间 | 负责人 | 更新内容 | 备注 |\n| --- | --- | --- | :---: |\n| 2022年1月22日 | dianjixz | 编写初稿文档 | 训练教程只能在 Linux 系统中运行，<br>并只能部署到 MaixII-Dock 开发板上运行，<br>文档还需要二次整理 |\n| 2022年3月11日 | rui | 添加注意事项 | ---- |\n\n> Windows 系统暂不支持！\n> 临时工程文件获取：https://github.com/dianjixz/v831_yolo\n\n## 基本概念\n目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。\n\n计算机视觉中关于图像识别有四大类任务：\n\n（1）分类-Classification：解决“是什么？”的问题，即给定一张图片或一段视频判断里面包含什么类别的目标。\n\n（2）定位-Location：解决“在哪里？”的问题，即定位出这个目标的的位置。\n\n（3）检测-Detection：解决“在哪里？是什么？”的问题，即定位出这个目标的位置并且知道目标物是什么。\n\n（4）分割-Segmentation：分为实例的分割（Instance-level）和场景分割（Scene-level），解决“每一个像素属于哪个目标物或场景”的问题。\n\n![](./dnn/yolo.png)\n\n> 了解更多可以查看 CSDN 博文：<https://blog.csdn.net/yegeli/article/details/109861867>\n\n## 准备\n\n获取训练工程文件\n\n    git clone https://github.com/dianjixz/v831_yolo.git\n\n### 数据集制作与使用  \n\nYOLOv2 默认使用 voc 格式的数据集,文件夹取名为 custom 放到 data 目录下, 比如:\n~~~ bash\n#voc格式的yolo训练数据集\n├── custom    #数据集文件夹名\n│   ├── Annotations\t\t#标注文件\n│   ├── ImageSets\t\t#训练参数划分\n│   │    └── Main\n│   │         ├── train.txt\n│   │         └── val.txt/\n│   ├── JPEGImages\t\t#训练图片\n~~~\n\ntrain.txt 和 val.txt 中, 每一行是一个数据(图像)名, 路径相对于 `JPEGImages`\n~~~ bash\ntrain.txt 写着用于训练的图片名称\nval.txt  写着用于验证的图片名称\n~~~\n**修改配置**\n\n修改 `data/custom.py` 中的 `CUSTOM_CLASSES` 变量为正确的 `labels`\n~~~ python\nCUSTOM_CLASSES = [\n    \"mouse\",\n    \"sipeed_logo\"\n]\n~~~\n\n### 训练开始\n\n~~~ bash\npython3 train.py -d custom --cuda -v slim_yolo_v2 -hr -ms\n~~~\n\n[//]: # \"或者安装好horovod, 然后多卡训练\"\n[//]: # \"~~~ bash\"\n[//]: # \"horovodrun -np 4 python train.py -d custom --cuda -v slim_yolo_v2 -hr -ms\"\n[//]: # \"~~~\"\n\n训练完成后会在 weights/custom/slim_yolo_v2 目录下生成训练中保存的参数\n\n### 导出模型\n\n~~~ bash\npython3 test.py -d custom -v slim_yolo_v2 --trained_model weights/custom/slim_yolo_v2/slim_yolo_v2_1000.pth --visual_threshold 0.3 -size 224 --export\n~~~\n\n运行导出模型命令后会在 out 目录下生成 test 测试图片效果和模型文件,模型转换请参考上面模型转换章节.\n\n### 模型部署  \n等待模型转换完成,下载转换好的模型文件.  \n得到的 *.param 和 *.bin 文件就是部署在 MaixII-Dock 上的文件.  \n打开事例代码,替换模型文件名,分类标签和模型加载参数,然后运行即可. \n~~~ python\n#检测示例代码\n\nfrom maix import nn, camera, image, display\nfrom maix.nn import decoder\nimport time\n\nmodel = {\n    \"param\": \"/root/yolov2_int8.param\",\n    \"bin\": \"/root/yolov2_int8.bin\"\n}\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (7, 7, (1+4+2)*5)    #输出参数修改,修改格式 (7 ,7 , (1 + 4 + \"类别数量\" ) * 5)\n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\n\nlabels = [\"mouse\",\"sipeed_logo\"]            #分类标签\nanchors = [1.19, 1.98, 2.79, 4.59, 4.53, 8.92, 8.06, 5.29, 10.32, 10.65]\n\nm = nn.load(model, opt=options)\nyolo2_decoder = decoder.Yolo2(len(labels), anchors, net_in_size=(options[\"inputs\"][\"input0\"][0], options[\"inputs\"][\"input0\"][1]), net_out_size=(7, 7))\n\nwhile True:\n    img = camera.capture()\n    AI_img = img.copy().resize(224, 224)\n    out = m.forward(AI_img.tobytes(), quantize=True, layout=\"hwc\")\n    boxes, probs = yolo2_decoder.run(out, nms=0.3, threshold=0.3, img_size=(options[\"inputs\"][\"input0\"][0], options[\"inputs\"][\"input0\"][1]))\n\n    if len(boxes):\n        for i, box in enumerate(boxes):\n            class_id = probs[i][0]\n            prob = probs[i][1][class_id]\n            disp_str = \"{}:{:.2f}%\".format(labels[class_id], prob*100)\n            img.draw_rectangle(box[0], box[1], box[0] + box[2], box[1] + box[3], color = (255, 255, 255))\n            x = box[0]\n            y = box[1] - 20\n            if y < 0:\n                y = 0\n            img.draw_string(x, y, disp_str, color = (255, 255, 255))\n\n    display.show(img)\n\n\n~~~\n\n运行效果图:\n\n![](./dnn/yolo_test.jpg)\n\n\n> 参考：\n>csdn 博客：https://blog.csdn.net/yegeli/article/details/109861867"}, "/soft/maixpy3/zh/usage/train_AI/train_resnet.html": {"title": "图像分类模型训练过程", "content": "# 图像分类模型训练过程\n\n| 时间 | 负责人 | 更新内容 | 备注 |\n| --- | --- | --- | :---: |\n| 2022年1月22日 | dianjixz | 编写初稿文档 | 训练教程只能在 Linux 系统中运行，<br>并只能部署到 MaixII-Dock 开发板上运行，<br>文档还需要二次整理 |\n\n\n> 临时工程文件获取：https://github.com/dianjixz/v831_restnet18\n\n\n图像分类主要采用 resnet18 网络结构，使用 Pytorch 框架进行搭建，再将经过**训练、转换和量化**后的模型文件部署到 MaixII-Dock 上。\n\n## 准备\n\n### 获取训练工程文件\n\n可以在 GitHub 上下载压缩包或者是通过 git 克隆到本地\n\n    git clone https://github.com/dianjixz/v831_restnet18.git\n\n工程文件结构介绍：\n```shell\n├── classes_label.py                            #分类标签\n├── classifier_resnet_test.py                   #测试程序\n├── classifier_resnet_train.py                  #训练程序\n├── convert.py                                  #模型转换程序\n├── convs_data                                  #存放经过 onnx2ncnn 转换之后的模型文件\n├── data                                        #训练数据文件夹\n├── out                                         #训练模型输出文件夹，每隔一定训练周期输出一个模型参数\n└── test                                        #测试数据集文件夹（不分类别）\n```\n\n### 数据集的制作与使用\n\n使用手机或者 MaixII-Dock 来对物品进行拍摄，将拍摄的图片进行导出，按照类别分类到文件夹中，并以类别名来命名对应的文件夹。图片最好是以数字进行命名，这样可以减少一些奇怪的 BUG。\n\n> 注意！数据集中图片的分辨率需要是 224*224\n\n数据集 data 文件夹结构\n~~~ c\n── mouse\n│   ├── 1.jpg\n...\n├── sipeed_logo\n│   ├── 1.jpg\n...\n...\n~~~\n\n将整理好的数据集，复制到训练工程文件中的 data 文件夹下，将 classes_label.py 里面的 labels 值修改成 data 下的文件夹名字。\n\n- 例如：data文件夹内为\n![resnet-data](./dnn/resnet-data.png)\n则将 `classes_label.py` 修改成\n    ```python\nlabels = [\"mouse\",\"sipeed_logo\"]\n    ```\n\n## 训练:\n\n训练的相关参数，在工程中的 classifier_resnet_train.py 文件里面，可以根据自己的需要进行修改，不懂怎么修改的，就保持默认就好了\n\n~~~ python\ndataset_path = \"data\"\t\t#训练集的路径\nval_split_from_data = 0.1 # 10%\t\t#学习率\nbatch_size = 4\t\t\t\t\t\t#训练批次\nlearn_rate = 0.001\t                #学习率\ntotal_epoch = 100\t\t\t\t\t#训练循环，总共需要训练100个循环\neval_every_epoch = 5\t\t\t\t#每个循环训练次数\nsave_every_epoch = 20\t\t\t\t#多少个循环保存一次\ndataload_num_workers = 2\t\t\t\ninput_shape = (3, 224, 224)\t\t\t#输入尺寸\ncards_id = [0]\t\t\t\t\t#使用的训练卡\nparam_save_path = './out/classifier_{}.pth'\t#参数保存路径\n~~~\n\n开始训练，在 resnet18 工程文件夹目录下运行\n\n~~~ bash\npython3 classifier_resnet_train.py\n~~~\n\n训练完成后，会在工程目录下生成一个 out 文件夹，在 out 文件夹下存放着训练过程中保存的训练参数。\n\n例如：\n~~~ bash\n.\n├── classifier_99.pth             #训练过程中保存的参数\n├── classifier_final.pth          #训练完成后保存的参数\n└── classifier.onnx               #生成的onnx深度学习网络文件\n~~~\n\n## 测试\n\n> 到这里，windows 的用户就需要在训练工程文件夹中打开 wsl\n\n准备好你的测试图片，注意和数据集中的图片尺寸一样。新建一个 test 目录，并放在该目录下。   \n\n进行模型测试\n\n    python3 classifier_resnet_test.py images_folder_path model_param_path\n\n- images_folder_path ：测试图片文件夹的路径\n- model_param_path ：模型文件的路径，一般都是在 out 文件夹下\n\n> 在该命令中会调用用户环境中的 onnx2ncnn 转换工具，请确保[训练环境](./ready.html)已经搭建好了。  \n\n运行完测试后，会生成 ncnn 模型和 ncnn 模型参数。\n\n## 模型量化 \n\n生成的 ncnn 模型此时还无法被 v831 直接使用，需要使用 [MaixHub](https://maixhub.com/modelConvert) 在线模型转换工具进行量化模型，转换成 MaixII-Dock 可以直接使用的 awnn 模型\n将一下内容整合到一个压缩包中：\n- 创建为 images 的文件夹，内容一些校正图片，可考虑直接采用训练中的验证数据集，并务必保证校正时图像的预处理方式与训练和部署时一致。（数量在50张左右）\n- 将训练结束之后得到的模型文件一个 xxx.bin 和一个 xxx.param。\n- 压缩包内文件结构如图：\n    ![resnet-zip](./dnn/resnet-zip.png)\n> 注意：确保 images 目录内没有混入其他文件，否则会导致模型量化错误。\n\n注册登录并激活账号后,上传你的压缩包等待模型转换任务完成。\n\n## 模型部署  \n\n等待模型转换完成,下载转换好的模型文件。将得到的 *.param 、 *.bin 和训练工程中 classes_label.py 文件放传到 MaixII-Dock 的 U盘中。\n\n将以下代码复制到开发板上即可使用\n\n~~~ python\nfrom maix import nn, display, camera, image\nfrom root.classes_label import labels    #分类标签,根据个人需求自行替换\nimport time\n\nmodel = {\n    \"param\": \"/root/restnet18_int8.param\",        #模型文件,需要替换成自己训练的模型路劲\n    \"bin\": \"/root/restnet18_int8.bin\"\n}\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    },\n    \"outputs\": {\n        \"output0\": (1, 1, len(labels))           \n    },\n    \"first_layer_conv_no_pad\": False,\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n}\n\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nwhile True:\n    img = camera.capture()\n    AI_img = img.copy().resize(224, 224)\n    t = time.time()\n    out,  = m.forward(AI_img, quantize=True)\n    t = time.time() - t\n    print(\"-- forward time: {}s\".format(t))\n    msg = \"{}%: {}\".format(int(out.max() * 10), labels[out.argmax()])\n    print(msg)\n    img.draw_string(0, 0, msg, color = (255, 0, 0))\n    display.show(img)\n~~~\n\n运行效果：  \n![](./dnn/restnet_img.jpeg)"}, "/soft/maixpy3/zh/usage/train_AI/ready.html": {"title": "本地训练环境搭建", "content": "# 本地训练环境搭建\n\n| 时间 | 负责人 | 更新内容 | 备注 |\n| --- | --- | --- | :---: |\n| 2022年1月22日 | Rui | 编写初稿文档 | ---- |\n\n\n由于训练需要用到显卡，关于安装显卡驱动、CUDA、CUDNN 请自行百度查阅安装，本文不做详细说明。（ADM 显卡或者无显卡的，可以使用 CPU 进行训练）\n\n## 安装 Python 软件包\n\n本地训练时使用 Python 进行搭建的，需要在电脑上安装 Python，请自行[百度 Python ](https://www.baidu.com/s?ie=UTF-8&wd=python)如何安装。我们所有的训练工程都是使用 PyTorch 框架进行搭建（wsl中同样要安装以下的 Python 软件包） \n\n需要安装以下 Python 软件包：\n- PyTorch\n- torchsummary\n- pycocotools\n- opencv\n\n> 下载安装包，可以通过在安装指令后面添加 `-i https://mirrors.ustc.edu.cn/pypi/web/simple` 来使用中科大 pypi 源进行加速\n\n### 安装 PyToch\n\n进入 Pytorch 下载帮助[页面](https://pytorch.org/get-started/locally/)，根据自己所用系统的环境情况，选择对应的 CUDA 版本和安装包的类型，这里所选用的是 CUDA 10.2、 Linux 系统、稳定版、pip包（30 系列显卡只能使用11.2以上的版本）\n![pytoch-install](./../asserts/pytorch-install.png)\n\n    pip3 install torch torchvision torchaudio\n\n\n### 安装 torchsummary、pycocotools\n\n然后再通过 pip 进行安装 torchsummary\n\n    pip3 install torchsummary pycocotools\n\n### 安装 Opencv\n\n    pip3 install opencv-python opencv-contrib-python\n\n##  onnx2ncnn 模型转换工具\n\nPyTorch 不能直接将模型导出成 ncnn 格式，需要使用 onnx2ncnn 转换工具进行转换，需要用户自行去编译出对应的可执行文件。具体的编译步骤如下\n\n1. 安装编译环境所需要用到的软件\n\n    ```shell\n    sudo apt install build-essential git cmake libprotobuf-dev protobuf-compiler libvulkan-dev vulkan-utils libopencv-dev\n    ```\n\n    > 下载慢的可以通过更换镜像源或者是使用代理进行下载，但是 wsl 中使用非官方的镜像源可能会导致部分软件不能下载\n\n2. 需要先拉取整个 ncnn 转换工具的工程到任意文件夹下\n\n    ```shell\n    git clone https://github.com/Tencent/ncnn.git\n    cd ncnn\n    git checkout a03c1353193a172bfc22481195704731f75148d9\n    ```\n\n3. 工程编译初始化\n\n    ```shell\n    cd ncnn\n    git submodule update --init\n    ```\n\n4. 开始编译\n\n    ```shell\n    mkdir build\n    cd build\n    cmake -DCMAKE_BUILD_TYPE=Release -DNCNN_VULKAN=ON -DNCNN_SYSTEM_GLSLANG=OFF -DNCNN_BUILD_EXAMPLES=ON ..\n    make\n    ```\n    \n编译结束之后，可以在 ncnn/build/tools/onnx 目录下，能得到 **onnx2ncnn** 模型转换工具，执行以下命令添加到系统的环境变量中\n\n    sudo nano ~/.bashrc\n\n打开.bashrc文件之后，将下面这句代码添加到最后一行\n\n```shell\nexport PATH=$PATH:`pwd`/tools/onnx\n```\n\n## 文章参考\n\n* 显卡驱动安装：https://neucrack.com/p/252\n* opencv 多版本共存：https://neucrack.com/p/349"}, "/soft/maixpy3/zh/usage/train_AI/information.html": {"title": "深度神经网络（DNN）基础知识", "content": "---\ntitle: 深度神经网络（DNN）基础知识\nkeywords: maixpy, k210, AIOT, 边缘计算\ndesc: maixpy doc: 深度神经网络（DNN）基础知识\n---\n\n\n这里介绍使用 MaixPy AI 相关功能需要了解的知识，让你能够理解后面的内容， 不在本篇中深入介绍。\n\n## 如何解决一个问题--引出机器解决问题\n\n一个问题， 通常分为 **输入** 和 **输出（结果）**\n\n比如： \n坐标系中的一条直线如下， 上面的数据点值是已知的:\n![y=kx+b](./dnn/ykxb.jpg)\n\n现在提问，假如数据点规律不变， 输入一个 x 坐标 20, y 的值是多少？\n按照大家的知识，都知道这是一个一元一次方程(`y = kx + b`能解决的， 带入两个点的值，算出方程为`y = 3x + 10`, 那么当 `x=20`, `y` 的值为`70`， 于是输入是`20`, 输出是`70`。\n\n这里就是 输入(`20`) + 算法（一元一次方程） = 输出（`70`）， 这就是我们在解决一个问题时的基本方法， 所以关键就是找到这个符合这条线段上数据点规律的一个算法。\n\n人类很强大，会从这些数据中归纳总结学习，最终得到了这个算法（方程），然后其他的人直接使用这个算法就可以快速用于解决同类问题，那么，有没有一种方法， 让机器自动去寻找这个算法呢？\n\n\n## 如何让机器总结出算法\n\n要让机器自动总结出算法，即机器学习（ML，Machine Learning）， 我们先看看，人类是如何得到这个算法（方程）的。\n\n* 步骤1： 首先，有大量数据点，然后人类根据这些数据点发现了直线都符合`y = kx + b`这个适应所有直线的算法, 但是发现，这里面有两个未知数`k`和`b`, 这就是适应任何直线的参数\n* 步骤2： 然后具体的是什么样的直线，因为方程有两个未知数，即参数，将实际的两个数据点带入这个方程，得到了`k = 3`和`b = 10`\n* 步骤3： 然后我们用在步骤2中没有用到的在线上的数据点，去试试这个算法（方程）是否正确，最终发现都验证正确\n* 步骤4： 然后要通过`x`的值知道其它的点的`y`的值，只需要代入`y = 3x + 10` 即可\n\n\n那么，机器学习是不是也可以利用这个步骤来做呢？\n\n* 我们认为地设计一个算法结构， 加入我们碰巧直接设计成了`y = kx + b`， 我们给具体的直线留下了两个参数，我们暂且称呼这个结构叫 **模型结构**，因为有未知参数，我们称之为未训练的模型结构。其中`x`称为**输入**, `y`称为**输出**\n\n* 现在，我们将我们这条直线的的几个点代入到这个方程，  我们称这个过程为 **训练**，得到`y = 3x + 10` 这个算法， 已经没有未知参数了， 我们现在称它为**模型** 或者 训练好的模型，其中`k b`是模型内的参数，`y = kx + b`是这个模型的结构。 而带入训练的数据点，就叫做**训练数据**，它们的统称就叫**训练数据集**\n\n* 然后，我们使用几个在 训练 过程中没有用到的在线段上的数据点作为输入，代入这个模型进行运算，得到结果，比如 `x = 10`, 得到`y = 40`, 然后对比输出值是否与预期相符，这里我们发现`x = 10, y = 40` 确实是在图中这条直线上的， 并且训练时没有使用这个点，说明我们得到的模型在此次核验中通过，这个过程叫 **验证**， `x = 10, y = 40` 这个数据叫验证数据。 如果我们用多组数据去验证这个模型， 这些数据的统称就叫**验证数据集**\n\n* 现在， 我们获得了一个**模型**，并且用**验证数据集**对这个模型进行了验证，貌似也是很准确了，那我们就可以假设这个模型基本满足了我们以后有一个`x`， 要求着图中线上任意一点的`y`值，都可以输入`x`给出这条直线上对应点的`y`坐标。 这个过程我们其实是在**使用模型**了，这个过程称之为**推理**\n\n其实这就算是机器学习了， 我们人类需要的事就是设计`y = kx + b`这个结构，以及给出**训练数据集**和**验证数据集**，经过**训练**和**验证**得到一个我们认为可用的模型，然后使用`输入 + 模型`就可以得到认为的正确`输出（结果）`了。\n\n\n\n## 什么是深度神经网络？\n\n深度神经网络（DNN）是机器学习（ML）领域中的一种技术。\n\n前面说了一个比较简单的例子， 根据一条直线数据来预测直线上的任何一个点， `y = kx + b`这个结构是人为设计的， 很简单，当用于复杂的数据，发现它就不适用了，比如“这张图片里面是球还是玩具”\n\n![小球](./dnn/ball.jpg) ![玩具](./dnn/toy.jpg)\n\n前面为了模型能够存下一条直线的信息， 用了结构`y = kx + b`，直线的特征都存在模型里面了。\n现在用来存一张图的特征，光是`y = kx + b`这个线性结构， 以及`k 和 b`两个参数显然无法满足了， 需要设计一个更好的结构， 这时 **神经网络** 就出现了， 一种网状结构，能更好地记住图片的特征信息， 而这个网状结构又是多层的，也就是有深度的，所以称之为深度神经网络（DNN， deep neural network）， 所以说 DNN 是一种网络结构，是为了实现机器学习的一种手段。 每一层由多个节点组成， 如下图， 一个 DNN 包含了 **输入层**， **隐藏层**， **输出层**， 这里隐藏层由三层组成（`A[1], A[2], A[3]`层），但是统称隐藏层：\n\n![深度神经网络](./dnn/dnn.jpg)\n\n**输入层**：\n图中就是一个深度神经网络结构， `x` 是输入， 比如`x`这里可以是图片, 输入有多个节点，每个节点可以是一个像素点值， 这里输入层画了 7 个节点， 加入我们有一张图片是 `10 x 10`的分辨率，则输入层共需要 `100` 个节点。\n这里输入层是一个一维结构，实际情况可能有多维结构， 比如输入如果是一张灰度图片，分辨率`3x3`，这其实是一个二维结构，即两行两列的矩阵（关于矩阵的概念请自行学习，或者暂且理解成二维数组），比如：\n```\n[[109  138  110]\n [220  37   166]\n [32   243  67]\n]\n```\n每个像素点的值取值范围∈[0, 255]，然后我们将其平铺后变成共 9 个数据的一维数组给输入层\n```\n[109 138 110 220  37 166  32 243  67]\n```\n\n> 另外， 一般也会将输入层的值归一化到范围`[0, 1]`\n\n如果是一张彩色图片，那就是三维，即`高、宽、颜色通道`，颜色通道比如`RGB`三个颜色通道，即，输入有形状（包含了维度和每个维度的数据数量），比如上面的一维输入形状为`(9)`，其它图像通常以`(高，宽，通道数)`来表示形状，比如`(10, 10, 3)`表示分辨率`10 x 10`， 并且有三个颜色通道， 比如`RGB`。\n\n这里为了入门好理解，原理只介绍一维的情况\n\n\n**输出层**：\n`y` 是输出，这里输出有两个值，你可以理解成就是 MaixPy 的两个浮点值的 `list` `[Y1, Y2]`， `Y1`是`是小球的概率`，值∈[0, 1], `Y2`是`是玩具的概率`。 所以最终我们使用这个模型，  就是给它一张图片， 机器按照这个模型规定的结构和算法进行计算后得到一个 `list`， 我们根据这个输出的值就知道图中是什么东西了。\n\n**隐藏层**:\n连接输入层和输出层的隐藏层，以及中间的连接，负责了将输入数据推算成合理的输出值。\n\n\n## 中间休息，总结\n\n到现在为止， 你知道， **模型**是什么：就是一组数据结构，保存了一个网络的形状，以及里面的参数， 通常，这个模型的数据可以被保存成文件，比如`.h5 .tflite .kmodel`等文件，都是用来阐述这个模型的形状结构和参数，只不过是不同软件使用。\n人们只需要设计模型结构以及参数，用来解决一类问题，比如常见的物体分类， 比如就是上面说的区分一张图里面是小球还是玩具。\n这个模型里面有很多参数，具体在需要识别物体的时候，使用已知分类的数据集让机器自动训练得出一套合适的模型参数。\n然后我们就可以输入数据，让通过模型推理出来输入的数据时什么类别了。\n\n所以， 如果我们不需要训练模型，直接使用别人训练好的模型，只需要：\n* 确认需求，找到现成的模型，因为模型已经是训练好的了，输入和输出的形状的含义都已经定了\n* 确认模型的输入形状，比如模型输入分辨率`10x10`的彩图，则使用时需要将符合要求的图片传个输入层\n* 确认输出层的含义，比如前面说的识别小球和玩具，最后输出是分别代表是该物体的概率的 list， 比如 `[0.9, 0.1]`, 第一个值代表是小球的概率，那我们就知道这张图里有 90% 的概率是小球， 只有 10% 的概率是玩具\n* 将模型放到推理程序进行运行。 具体用什么程序先不着急，会在下一章介绍\n\n到这里，应该大致上明白了以下东西：\n* 什么是机器学习\n* 什么是深度神经网络（简单概念）\n* 模型是什么\n* 什么是输入层，输出层，在上面举例的分类应用中分别表示什么含义，层形状是什么样的\n* 到此为止，我可能还不知道什么是模型训练\n* 如果我需要一个模型，我知道如何确认需求\n\n所以，**如果你只希望能够使用模型，不需要训练，到此即可**， 也不需要知道模型有些什么具体的东西，你就把它当成一个**黑盒工具箱**使用即可。 如果想要更深的了解，请继续看下面的内容。\n\n\n\n## 继续：深度神经网络（续\n\n既然设计了多层设计，那我们继续深入：\n\n**数据流** ， **权重**， **偏置**:\n在模型进行推理时，数据从输入层流动到输出层，就是这些网状箭头的方向（第三节网状图），每个箭头前一层到后一层的计算可以用一个熟悉的公式:`y = wx + b`, 称`w`为**权重**(weight), `b`为**偏置**（bias）, 注意是每个箭头都有一个单独的`w, b`, 也就是说后一层节点的值等于前一层节点经过这个公式计算过后的值， 后一层的节点有多个前一层节点指向，那就等于所有前一层节点的值经过这个公式计算后的值的和。\n就这样经历了无数次运算后，结果终于在输出层以一个值的形式出现了，整个推理也就完成了\n\n**激活函数**：\n\n上面的模型虽然可以通过输入得到结果，但是会发现，所有层计算都是线性函数，那么不管有多少层，整体其实还是一个线性函数，即`y0 = w1x + b1` + `y = w2y0 + b2` ==> `y = w2(w1x + b1) + b2` ==> `y = w2w1x + w2b1 + b2`, 其实还是一个线性函数，那么多层的意义就没有了，于是我们需要在中间加入非线性函数，让网络内部更加复杂一点， 于是就在每个节点上做手脚， 在每个节点输出数据前，先对其用一个非线性函数运算，比如`sigmod`或者`relu`函数，别听到名字害怕，其实很简单，看下图, 总之就是 x 和 y 不成线性关系：\n![sigmod](./dnn/sigmod.jpg) ![relu](./dnn/relu.jpg)\n\n即到现在为止， 除了输入层，所有节点输出的值都需要经过`Sigmod(∑(Wn * x + Bn))`, 输出一个浮点数值\n\n**softmax**:\n\n输出层在最后输出的时候，因为前面的运算，值的范围不是很统一，虽然我们可以同过比大小，值最大的即认为是答案，但是为了统一而且可以直观地知道每个类别的可能性（另外也为了训练的准确性，这里不讲），正如前面讲到，我们最后输出的一个类别的概率，取值范围∈[0, 1]， 且所有输出的值和为`1`，所以在输出层后面对输出层的所有值进行处理，公式为\n![softmax](./dnn/softmax.jpg)\n\n到此，从输入到输出的推理过程就结束了\n\n## 深度神经网络训练\n\n前面简单介绍了深度神经网络的结构组成， 以及从输入层到输出层的正向过程，在我们使用模型时，就是这个正向过程。\n那么，模型定好了，里面的参数（比如`w,b`）都是随机的值，怎么让它自动训练得到模型中参数的值呢？ 在前面我们讲到， 使用一些我们已知结果的数据输入，来得到参数，同样地，这里我们也输入已知结果的数据，得到第一次的输出结果\n\n**判定输出正确性(accuracy)（或者说误差/loss）** 和 **损失函数**：\n\n在输出层得出结果，比如得到了`[0.6, 0.4]` 代表是小球的概率`0.9`, 是玩具的概率`0.1`, 但是因为是已知答案的数据， 实际正确答案是`[1.0, 0.0]`, 这明显不符合要求。\n所以我们得出正确答案和推算的答案的误差为： `[0.4, -0.4]`, 但是发现一个问题就是这个误差值的范围不太好看，要是误差的取值范围∈`[0, ∞]` 就好了。 在高中数学中有个函数`y = log10(x)`, 坐标图如下：\n![log10](./dnn/log10x.jpg)\n发现`x`取值∈`[0, 1]`时， `-y`的取值刚好∈`[0, ∞]`， 而我们的输出结果也刚好∈`[0, 1]`！ 所以，我们直接这样计算误差： `error = -log10(输出)`， 也就是输出越接近`1`，误差就越接近 `0`，这种方法称之为`交叉熵损失（CEE, Cross Entropy Error)`， 除了这种方法还有其它的比如均方误差（MSE，Mean Squared Error）等\n\n至此，我们知道了现在结果和实际结果的误差\n\n**误差的反向传播** 和 **参数优化（权重更新）**：\n因为模型的参数还不符合我们的预期， 那我们需要对参数进行修正，我们使用反向传播的方式。\n前面我们得出了误差， 因为参数不够正确， 我们用这个误差去修改模型中的参数，来达到微调模型内参数的效果。 就好像你在开一个水龙头， 水打了（即误差大了），就把开关拧紧一点，小了就拧松一点，对其做调整。\n就像我们正向推算一样，这次换成了反向，从后往前，可以得到在每个节点处的误差值，然后再根据一定的学习率去更新模型内参数。这里暂时就不仔细展开讲了。\n\n总之，经过一轮反向的调整参数之后，得到了新的模型\n\n**衡量模型好坏：训练集误差和验证集误差**：\n\n我们使用训练数据集里面的数据反复去进行正向推理得出误差，然后反向调整这个过程，在使用完训练数据集后，可能会得到误差比较小，但是这只能说明这个模型对这批数据来说比较准确，换一些新的数据可能就不准确了，所以我们要用一些训练集里没有的数据去**验证**模型的效果：\n我们使用 **验证数据集** 去正向推算，得到误差，因为验证数据集没有参与训练，也就是说现在模型的参数和验证数据集没有任何关系，我们用这个得到的误差来恒定这个模型的好坏，误差越小则认为效果越好\n\n**多次迭代**：\n\n如果将所有数据集训练完了，发现误差依然很大，那么可以用多次训练的方法来继续训练，即**多次迭代**，每次迭代完成后都用 验证数据集 去验证效果如何， 如果训练集的误差和验证集的误差都足够小，我们就可以暂且认为模型已经有不错的效果了。\n\n**测试集**：\n这时，我们就可以用又一批新的数据去测试我们的模型效果如何，因为这是全新的数据，没有参与到训练也没参与到验证（即确定什么时候停止训练），理论上更有公信力。如果测试误差较小，那么训练就算成功了\n\n**优化训练**：\n如果最终效果不太好， 有很多地方可以调整， 比如\n* 训练迭代的次数，并不是越多越好，过多的在一批数据集上训练可能导致模型只对这批数据有效，泛化能力不够， 也就是**过度拟合**\n* 每次训练的学习率也可以调整\n* 检查数据集，是否有一些影响分类的数据存在\n* 优化网络结构，不管是输入输出还是内部结构和参数，根据不同的数据和任务可以有更优的设计，也叫**特征工程**\n\n\n## 说在最后\n\n到这里，应该大致上明白了以下东西：\n* 什么是机器学习\n* 什么是深度神经网络\n* 模型是什么\n* 什么是输入层，输出层，在上面举例的分类应用中分别表示什么含义，层形状是什么样的\n* 什么是训练，有什么作用\n* 数据训练集，验证集，测试集分别是什么，用在什么地方，需要注意什么\n* 衡量模型好坏的标准是什么\n\n如果还不明白的，可以再仔细理解一遍，或者查阅相关资料，如果你发现有更好的阐述方法，欢迎按照左边目录的文档贡献方法参与贡献\n\n\n## 修改记录\n\n| 日期 | 作者 | 备注 |\n| ---  | ---- | --- |\n| 2020.11.17 | [neucrack](https://neucrack.com) | 初始版本，根据 MaixPy 的需要介绍深度神经网络基本概念，初稿 |"}, "/soft/maixpy3/zh/usage/train_AI/data.html": {"title": "制作数据集", "content": "# 制作数据集\n\n目前提供了 Resnet18 和 yolov2 的训练方式，就需要制作这些网络模型的数据集\n\n## Resnet18 数据集\n\nResnet18 是一个用来分类的网络模型，只能将输入的图像分类到一个类别，所以数据集相对于简单很多。\n\n1. 将需要进行分类识别的物品进行多个角度和多种背景环境下进行拍摄\n2. 把图片转换成大小为**224\\*224**\n3. 新建以类别命名的文件夹\n4. 将转换好的图片存放到对应的文件夹中\n\n![resnet-data](./dnn/resnet-data.png)\n\n## YoloV2 数据集\n\nYoloV2是用来目标检测的网络模型结构，所以制作数据集的时候需要对图像做对应的标记。\n\n### 准备\n1. 下载[labelImg](https://github.com/tzutalin/labelImg/releases)\n2. 把需要标注的图片转换成 **224\\*224**，并分类到各自的文件夹中\n3. 新建一个文件夹来用于存放标记信息\n\n### 开始标注\n\n将下载的压缩包，解压到任意的位置，但是路径不能有中文，否则会启动不了。运行后界面如下：\n\n![](./../asserts/labelimg.png)\n\n选择左边的 `Open Dir` 中打开图片的文件夹，选择 `Change Save Dir` 进行保存标记信息。选择左边的命令按钮 Create nRectBox，也可以用快捷键 W 创建一个新的物体标注包围矩形。\n\n![](./../asserts/labelimg-1.png)\n\n在右边选择所框选的类别，通过快捷键 S 保存标记的内容，会在保存路径下生成一个xml文件，每个图形对应一个文件，里面记录了物体的包围矩形的位置。使用快捷键 D 切换到下一张图片继续标记，同一张图片中，可以同时标记多个类别。\n\n> 如果不能使用 A、D切换图片的时候，这是图片的路径包含了中文导致的。\n> 可以通过在 View 中打开自动保存模式，发现不能自动保存的时候，需要手动重启一下自动保存模式"}, "/soft/maixpy3/zh/usage/train_AI/test.html": {"title": "将自己的神经网络部署的到 MaixII-Dock 上", "content": "# 将自己的神经网络部署的到 MaixII-Dock 上\n\n| 时间 | 负责人 | 更新内容 | 备注 |\n| --- | --- | --- | --- |\n| 2022年1月26日 | Rui | 编写文档 | --- |\n\n对于一些自定义的神经网络结构，可以通过 MaixPy3 部署到所支持的开发板上。将边缘检测部署到 MaixII-Dock 为例，讲述一下部署的思路\n\n## 前置知识\n\n进行之前，需要学习神经网络的基础知识，可以通过查看前面的 【[深度神经网络基础知识](./information.html)】 快速的了解一下，想要了解更多可以自行百度神经网络的相关教程\n\n## 边缘检测原理介绍\n\n边缘就是值变化剧烈的地方, 如果对值的变化求导, 则边缘部分就是导数局部最大。但是在图像处理时没有具体的函数让我们求导, 使用卷积运算则可以很好的近似替代。\n\n如下图, 假设左上为坐标原点, 横轴为 x, 纵轴为y, 如下图左上角9个像素点, P(x, y)表示坐标(x, y)的点, 要求P(1, 1)处在x轴的变化率, 则只需将P(2, 1) - P(0, 1) 得到值为0, P(1, 0)处为1-3 = -2, 这个差值即变化率, 类比成导数, 我们就能知道横轴在哪些地方变化率更大。\n\n![](https://neucrack.com/image/1/377/conv.jpg)\n\n上面这种方法我们可以得到横轴的变化率, 这里使用**横轴卷积核**\n\n~~~ python\n[-1, 0, 1],\n[-2, 0, 2],\n[-1, 0, 1]\n~~~\n\n对图像进行卷积运算, 如图中的计算方法, 像素点左右权值取2, 角上的也参与计算,但是权值为1,没有左右边的权值高。这样我们就得到了横轴的变化率图, 即边缘检测图。\n\n注意, 这里是对横轴计算了, 比较的点左右的值变化, 所以实际看到的图像会出现明显的纵轴边缘, 如下图左边\n\n![](https://neucrack.com/image/1/377/vertical_horizontal.jpg)\n\n同理, 上图右边的图使用**纵轴卷积核**\n```\n[1, 2, 1],\n[0, 0, 0],\n[-1, -2, -1]\n```\n得到的纵轴的边缘图。\n\n注意这里用右边减左边, 如果右边的值比左边的小会是负数, 如果我们希望只检测颜色值变大(变白)则可以直接使用, 如果两个变化方向都要检测, 则可以取绝对值. 比如下图左边是没有取绝对值, 右边取了绝对值\n\n![](https://neucrack.com/image/1/377/without_with_abs.jpg)\n\n得到两个方向的图后, 对其进行合并, 对每个像素平方和开方即可\n\n![](https://neucrack.com/image/1/377/final.jpg)\n\n这张图左边是使用 GIMP 的 sobel 边缘检测(垂直+水平)的效果, 略微有点不同:\n\n![](https://neucrack.com/image/1/377/sobel_edge2.jpg)\n\n不同的原因是使用水平和垂直的图平方和开根后, 直接用 `plt.imshow` 显示, 和 GIMP 的处理方式不同\n```python\nout = np.sqrt(np.square(out_v) + np.square(out_h))\nplt.imshow(out)\n```\n简单地直接将值规范到`[0, 255]`就和 GIMP 的图相似了(但不完全一样)\n```python\nout = np.sqrt(np.square(out_v) + np.square(out_h))\nout = out * 255.0 / out.max()\nplt.imshow(out.astype(np.uint8))\n```\n![](https://neucrack.com/image/1/377/sobel_v_h.jpg)\n\n## 定义卷积核\n\n除了上面说了使用两次卷积计算, 也可以用只计算一次的卷积核, 比如:\n```bash\n[-1, -1, -1],\n[ -1, 8, -1],\n[ -1, -1, -1]\n```\n这是对于一个通道(灰度图)来说, 如果要扩充到三个通道(RGB), 卷积核参数就是如下形式\n```bash\n# 全边缘卷积核\nconv_rgb_core_sobel = [\n                        [[-1,-1,-1],[-1,8,-1], [-1,-1,-1],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,-1,-1],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],a\n\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,-1,-1],\n                        ]]\n```\n\n前面所有介绍的横轴边缘和纵轴边缘卷积核参数形式同理\n\n## 代码实现边缘检测\n\n整个边缘检测的网络模型只有一层卷积核\n\n### 定义网络模型\n\n```python\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 3, 3, padding=(0, 0), bias=False)\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\nnet = Net()\n```\n\n### 定义卷积核参数\n\n```python\nconv_rgb_core_sobel = [\n                        [[-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                        ]]\n```\n\n### 定义载入权重函数\n\n```python\ndef sobel(net, kernel):\n    sobel_kernel = np.array(kernel,    dtype='float32')\n    sobel_kernel = sobel_kernel.reshape((3,    3,    3,    3))\n    net.conv1.weight.data = torch.from_numpy(sobel_kernel)\nparams = list(net.parameters())\n```\n\n### 输入数据进行处理\n\n加载一张图片\n\n```python\npil_img = Image.open(\"./images/class1_5.jpg\")\ndisplay(pil_img)\ninput_img = np.array(pil_img)\nprint(input_img.shape)\n```\n\n对图片进行归一化处理并转换成 PyTorch 张量\n\n```python\n# 归一化处理\ninput_tensor = (input_img.astype(np.float32) - 127.5) / 128 # to [-1, 1]\nprint(input_tensor.shape)\ninput_tensor = torch.Tensor(input_tensor).permute((2, 0, 1))\ninput_tensor = input_tensor.unsqueeze(0)\nprint(\"input shape:\", input_tensor.shape)\n\n# 转换成 PyTorch 张量\ninput_tensor = (input_img.astype(np.float32) - 127.5) / 128 # to [-1, 1]\ninput_tensor = torch.Tensor(input_tensor).permute((2, 0, 1))\nprint(input_tensor.shape)\ninput_tensor = input_tensor.unsqueeze(0)\nprint(\"input shape:\", input_tensor.shape)\n```\n\n### 进行边缘检测\n\n```python\n# 载入网络权重\nsobel(net, conv_rgb_core_sobel)\n\n# 输入图片到网络中进行处理\nwith torch.no_grad():\n    out = net(input_tensor)   \n    sobel_img_t = out.numpy()[0].transpose([1,2,0])\n\n# 显示输出结果\nplt.figure()\nplt.subplot(1, 5, 1)\nplt.imshow(input_img)\nplt.subplot(1, 5, 2)\nplt.imshow(sobel_img_t)\n```\n\n经过卷积运算后, 前后图如下:\n\n![](https://neucrack.com/image/1/377/sobel_edge.jpg)\n\n注意, 输入值范围如果为`[0, 255]`, 输出值则范围会变化, 以图片形式查看时需要注意加以处理, 这里使用了`plt.imshow(out)`来显示, 这个函数会自动对图像做简单的处理, 才会看起来是黑色背景\n\n## 模型导出"}, "/soft/maixpy3/zh/question/maixpy3_faq.html": {"title": "MaixPy3 常见问题与解决方法", "content": "##无法启动IDE启动MaixPy3的时候出现如下图的错误提示，这时参照[没有正常退出IDE](#没有正常退出-ide)重新启动一下即可![error_run_maixpy3](./assets/maixpy3_startup_error.png)##无法新建或保存文件出现的提示信息：![](./assets/other_1.png)和![](./assets/other_2.png)这是IDE权限不足导致，在任务栏中IDE图标的右键菜单中勾选提权即可，在Windows下的IDE软件0.3.6之前默认不提权。##没有正常退出IDE通过任务栏中小图标的右键菜单重启IDE即可![](./assets/IDE.png)##开发板与电脑连接没有U盘设备添加信息弹出1、检查USB线和USB口是可以进行数据传输的。2、检查是否接上了开发板上的OTG接口。3、系统镜像是否在0.3.5以上。4、打开设备管理器，找到AndroidADBInterface设备，右键卸载改设备并删除其驱动，然后拔插一下开发板，等待系统安装新驱动即可##在运行上述代码的时候出现以下信息![842bf6204549825d5c0bb9c85b53edf.png](attachment:842bf6204549825d5c0bb9c85b53edf.png)![e5c650a86849719557dd5097d96fa6d.jpg](attachment:e5c650a86849719557dd5097d96fa6d.jpg)1、检查USB线和USB口是正常可用的。2、检查驱动正确安装成功。3、检查板子通电并已插入电脑。4、请更新镜像到至少0.3.5以上。5、搜索电脑是否存在其他adb.exe文件，请将其全部删除，并重装此IDE。>因为不同的版本的adb工具存在连接上的差异，导致部分adb工具不适配开发板。##新建文件时出现报错报错如图：![](./assets/IDE_1.png)解决办法：这是权限不足导致的，可以通过任务栏中图标右键菜单勾选提权，即可解决![](./assets/IDE_2.png)##接上开发的板的OTG接口之后，电脑上没有U设备弹出打开电脑上的设备管理器，查看是否存在AndroidADBInterface设备![adb](./assets/adb.jpg)右键卸载，该设备，并卸载勾选上删除设备驱动程序![](./assets/adb_1.jpg)![](./assets/adb_2.jpg)等待卸载结束，重新拔插一下开发板，即可出现U盘设备。##运行代码之后报Nomodulenamed'maix'![](./assets/no-module.png)这是使用的本机的Python内核运行代码导致的，只需要将内核服务切换成Rpyc-python即可##系统找不到指定的文件![](./assets/no-adb.png)这是由于之前系统安装过jupyter并修改过默认工作区文件路径导致的，需要将C:\\Users\\用户名\\\\.jupyter\\jupyter_notebook_config.py中的带有**c.NotebookApp.notebook_dir**的一行前面添加一个“#”注释掉>更多可以查看论坛的[常见问题](https://bbs.sipeed.com/thread/1303)"}, "/soft/maixpy3/zh/tools/0.MaixII-Dock.html": {"title": "通过 IDE 连接 MaixII-Dock", "content": "---|时间|负责人|更新内容||----------|------------|:------------------------------------------------:||2021-12-3|Rui|制定文档初稿||2021-12-7|大老鼠&Ray|微调排版及审核||2021-12-8|Rui|添加IDE基本的使用介绍,MaixPy3入门目录||2022-01-14|dalaoshu|配合IDE更新优化了用户使用体验||2022-01-17|dalaoshu|根据小徐同学的视频反馈，修订了用词和补充SD卡说明||2022-03-02|Rui|添加MaixII-Dock开箱视频，修改部分表述错误||2022-04-14|Coty|添加main.py会被kill掉的解决方法||2022-06-09|dalaoshu|重新强调了m2dock必须有U盘出现才能使用||2022-07-13|dalaoshu|更新到最新的0.5.1了|#使用MaixPy3IDE连接MaixII-Dock##产品宣传<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=298543445&bvid=BV1sF411u7xb&cid=586467021&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##开箱说明<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?bvid=BV14Z4y147Lg&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>**使用前请仔细阅读以下内容，可以减少很多疑问**##避坑要点（重要）1.检查USB线和USB口是正常可用的，确定板子通电并插入电脑后，插入电脑驱动有提示，屏幕有背光有画面出现，硬件接线电源无损坏。2.准备好可以启动系统的卡，如购买已烧录好系统的内存卡，可在购买时选择预烧录套餐（没有的需要[烧录系统方法](https://wiki.sipeed.com/hardware/zh/maixII/M2/flash.html)）。3.烧录或更新到最新MaixPy3镜像，确认`piplist`中的maixpy3包版本大于0.5.0以上。4.安装好MaixPy3IDE软件，确保大于0.5.0版本。5.安装软件时会弹出驱动安装程序，请确认驱动安装完毕，连上硬件后启动系统后就会弹出虚拟U盘。6.上一步操作未出现虚拟U盘的话，Windows用户需要手动前往设备管理器卸载ADB驱动且**勾选上删掉其驱动**，重启电脑后才能够弹出U盘。7.**没有出现U盘的话那么电脑通过OTG接口是没法操作M2Dock的，因此务必重复上述的5、6直到看到U盘才能继续阅读下面的内容**8.确认adbshell终端可用（需要安装驱动），并确认mjpg服务存在后，就可以开始使用了。##初期准备工作1.安装MaixPy3IDE软件，它会在安装时提示用户安装驱动，安装好后会弹出jupyternotebook的工作区，也就是你现在看到的文档。![driver.png](attachment:driver.png)2.选择一条可以传输数据的Tpye-C数据线（如购买时附带的数据线），不可以是充电线，或者转接头，不要延长或转接数据线，插入支持输出500ma以上电流的USB口，遇到无法解决的问题可以换线换口换电脑。![typec.jpg](attachment:typec.jpg)3.插入带有maixpy3系统的SD卡，所烧录的系统版本大于0.5.1即可。![sd.jpg](attachment:sd.jpg)4.将板子与电脑通过OTG标识的USB口连接，确认设备通电亮起（power）电源**红灯**，请看下图红圈别接错USB线，OTG标识的丝印在板子背面。![index.jpg](attachment:index.jpg)5.确认屏幕出现logo或二维码（wiki）表示系统启动并已工作，此时电脑会弹出一个U盘，这意味着板子的系统已经准备就绪。（在Windows平台初次使用需要通过步骤5卸载一下驱动即可弹出U盘）![udisk.jpg](attachment:udisk.jpg)>虚拟U盘设备在传输大文件后可能会因为没有正常关机而丢失文件，并且需要reboot重启才会出现系统创建的文件。![udisk_list.png](attachment:udisk_list.png)>该U盘目录对应的是板子上的linux系统/root/目录，这里main.py是默认的开机脚本，wpa_supplicant.conf是WIFI配置，最新系统支持/root/app/main.py文件夹优先于main.py脚本启动。6.如果没有出现U盘，意味着驱动存在异常，则需要按下图手动卸载一下AndroidADBInterface手机驱动（常见于XX手机助手）。![list_dev.png](attachment:list_dev.png)找到它，勾选卸载驱动即可，此时U盘跳出，系统准备就绪，**之后遇到的问题与底层硬件没有任何关系！！！**![remove_dev.png](attachment:remove_dev.png)**如果U盘还是没有如上述步骤出现，可以重烧系统或重启设备或考虑换台电脑操作，有可能是个别系统驱动不兼容导致的，实在是解决不了，可以在bbs.sipeed.com汇报给@管理员帮忙解决。**##如何运行代码运行前的可以检查一下运行环境是否正常。-IDE软件启动时会附带一个keep_adb.exe命令行终端的程序提供给熟悉linux终端操作开发板的同学。-IDE0.4.2后keep_adb服务会自动调用adb配置映射（forward）端口（22，18811，18812）。-与板子连接的ide服务是否工作，判断方法可以在交互终端输入`ps|grepmjpg`查看是否存在下图红框所指示的服务。-如果发现不存在ide服务（`python-c'frommaiximportmjpg;mjpg.start();'`），可以手动运行服务，并把现象汇报到社区，目前发现该现象主要出现在Windows11系统之间的差异上。![grep.png](attachment:grep.png)-确认系统防火墙是否阻止了软件底层所需要TCP1881118812的端口号，主要用于运行程序和图像传输。想知道更多，可以点此查看关于[MaixPy3IDE](https://wiki.sipeed.com/soft/maixpy3/zh/tools/MaixPy3_IDE.html)的更加详细的介绍，此处不再赘述。###选择RPyc-Python核心表示在板子上运行Python程序将IDE的内核应切换成RPyc-python![b1006db53eb3486c65986e16aece9e09_3ghn+H8RiZsU9sSYKAAAAAElFTkSuQmCC.png](attachment:b1006db53eb3486c65986e16aece9e09_3ghn+H8RiZsU9sSYKAAAAAElFTkSuQmCC.png)###如何运行Python代码点击选择代码块，点击上方工具中的运行，即可运行代码并输出结果，运行后会出现*表示程序正常运行。![ac379e3ca0627839de12394216d3d87.png](attachment:ac379e3ca0627839de12394216d3d87.png)###如何停止刚才运行的Python代码选择正在运行的代码，点击上方工具栏中的停止，即可停止运行代码，快捷键是按两下ii喔。![9665737f11ad60c602ed81796954c07.png](attachment:9665737f11ad60c602ed81796954c07.png)如果有其他程序正在运行的话，需要先点停止，等程序断开了后（代码块前的*号消失），再点运行。![MaixPy3-IDE.gif](attachment:adc-5.gif)###运行时出现问题？IDE的每个代码单元块运行是按顺序依次排队运行，当前一个代码没有运行结束的时候，后一个代码是不会运行。当出现代码运行之后没有出现结果，或者是卡住了，停下程序、重启IDE、重启开发板等操作就可以解决问题。>2022年7月13日画面撕裂问题可以重新运行一下程序即可，原因是show的图像大小发生变化导致的撕裂，等待后续的修复。##不妨来试试?点击下列代码块，再点击上方菜单栏中的运行，测试板子连接是否正常。importplatformprint(platform.platform())importtimeprint(time.asctime())[ rpyc-kernel ]( running at Wed Jul 13 15:20:20 2022 )\nLinux-4.9.118-armv7l-with-libc\nThu Jan  1 04:04:55 1970上述结果可知：一、运行这段代码的时间是\\[rpyc-kernel\\](runningatWedJul1315:20:202022)二、运行这段代码的平台是Linux-4.9.118-armv7l-with-libc三、运行这段代码的时候，板子系统时间是ThuJan104:04:551970。当代码运行的时间为当前时间，并打印出以上代码，说明开发板已经连接上并可以正常的使用![b045bfba849b23527feb9952d6d57703_AdrALr8ioAAAAASUVORK5CYII=.png](attachment:b045bfba849b23527feb9952d6d57703_AdrALr8ioAAAAASUVORK5CYII=.png)##如何连接网络开发板上的OTG接口与电脑连接之后，就会在资源管理器中得到一个U盘设备。通过记事本打开里面名为`wpa_supplicant.conf`文件![9917ccc6b0563cb0f72084dea137e38.png](attachment:9917ccc6b0563cb0f72084dea137e38.png)![net.gif](attachment:adc-1.gif)-wpa_supplicant.conf```bashctrl_interface=/var/run/wpa_supplicantupdate_config=1network={ssid=\"yourWIFIname\"psk=\"yourWIFIpassword\"}```将根据提示修改成需要连接的网络名字和密码。(只支持连接2.4G频段的WIFI，每个人的网络环境不同，也有可能连接不上，需要测试多个网络)##如何更新MaixPy3包可以手动下载最新的MaixPy3[安装包](https://pypi.org/project/maixpy3/#history)![5abb0e5b5b3abc2a60bb0fe8557d216a_ekCZMejeMjAAAAABJRU5ErkJggg==.png](attachment:5abb0e5b5b3abc2a60bb0fe8557d216a_ekCZMejeMjAAAAABJRU5ErkJggg==.png)下载带有cp8的安装包，cp9是给别的平台使用的。将这个安装的名字修改成`maixpy3-9.9.9-cp38-cp38-linux_armv7l.whl`,直接存放到开发板中，重启开发板就会自动更新和安装MaixPy3。![2387eda63480f059a12350154f64628.png](attachment:2387eda63480f059a12350154f64628.png)更新前请关闭IDE或不接OTG口，防止有其他操作影响系统的软件更新，在放入U盘后，断电开机会看到如下画面，如果超过3分钟画面没有变化，那可能就是失败了，就请重烧系统吧。（2022年1月14日至今还没出现过失败样本）##更多连接方式使用mobaxterm可以进行串口连接和ssh连接，具体教程查看【<ahref='https://wiki.sipeed.com/hardware/zh/maixII/M2/tools/mobaxterm.html'target=_blank>如何使用mobaxterm</a>】![image.png](attachment:image.png)##配置开机启动脚本对于M2Dock，我们只需要把开机想要运行的程序存成main.py文件，然后根据/root目录下的内容来选择存放位置。注意带maixhub词缀的镜像内置了app开机应用，开机时会优先运行app文件夹下的main.py脚本，因此有下面的说法：-如果root目录下存在名为app的文件夹的话，那么我们把main.py文件存放到app文件夹里就行了-如果root目录下没有名称为app的文件夹，那么我们直接把main.py文件存放在root目录下即可简单来说M2Dock会优先执行root目录app文件夹下的main.py脚本，如果app文件夹下没有main.py文件的话就会执行root目录下的main.py脚本。>MaixPy3IDE在运行时会停止M2Dock的开机脚本程序，所以我们应该在电脑托盘找到并退出MaixPy3IDE或者选择仅使用板子上的TypeC串口来进行供电以查看开机运行脚本效果---补充说明：在编辑`main.py`的时候记得确认文本编码格式为UTF-8。如果main.py开机启动异常，可能出现Python语法错误的时候可以在所执行的`main.py`的当前目录里的`main.py.log`文件查看得知。重启板子的话使用`reboot`命令，关机使用`poweroff`指令。尽量不要用reset按键或者直接断电来操作板子。下面的代码是root目录下main.py文件里的默认内容#!/usr/bin/envpythonfrommaiximportcamera,display,image,nnimage.load_freetype(\"/home/res/sans.ttf\")qrcode=image.open('/home/res/qrcode.png')canvas=image.new((display.width(),display.height()),(0xFF,0xFF,0xFF),\"RGB\")canvas.draw_image(qrcode,(canvas.width-qrcode.width)//2,(canvas.height-qrcode.height)//2)info=\"wiki.sipeed.com/maixpy3\"w,h=image.get_string_size(info,1.2)canvas.draw_string((canvas.width-w)//2+5,canvas.height-h-5,info,1.2,color=(0x00,0x00,0x00))foriinrange(120):img=camera.capture().draw_image(canvas,alpha=0.7)display.show(img)##常见问题指南###没有弹出U盘或者adb提示devicenotfound这种需要先确认安装MaixPy3IDE的时候已经安装快结束时所弹出的驱动安装。然后确认自己的设备管理器中无ADB设备显示，如果有adb设备那就需要手动卸载掉并且**勾选删除其驱动**，成功卸载后重启电脑就能够连上M2Dock了。###无法启动IDE![d4d9b5c0be29b6d790a2da08a11c3171_hAxAEQVB4xV+mOrdBHj9+nP4fgELQXAykHRQAAAAASUVORK5CYII=.png](attachment:d4d9b5c0be29b6d790a2da08a11c3171_hAxAEQVB4xV+mOrdBHj9+nP4fgELQXAykHRQAAAAASUVORK5CYII=.png)这个说明IDE已经在正常运行了在电脑的任务栏通知区域找到下图所示的图标（注意是退出两个字左边的类似于盾牌的标志）且选择重启即可，![4b33cc14940d197a7f08b68cf603c4e2_P2WGBQrwh7epAAAAAElFTkSuQmCC.png](attachment:4b33cc14940d197a7f08b68cf603c4e2_P2WGBQrwh7epAAAAAElFTkSuQmCC.png)###如何退出MaixPy3IDE因为它是jupyter网络服务，所以需要从底下托盘右键退出，否则关闭浏览器后，它的网络服务还挂在后台运行的。![exit.png](attachment:exit.png)###我的MaixPy3IDE没有跳出浏览器，没有可以操作的页面。![pan.png](attachment:pan.png)点击托盘查看MaixPy3jupyter服务是否存在，存在则手动复制以下红框的地址到支持谷歌内核(chromium)的浏览器中进入（有少数同学遇到）。![d671e1fd30f56b03e7b5ce023cfa7ecb_H0T++3I4nfQVAAAAAElFTkSuQmCC.png](attachment:d671e1fd30f56b03e7b5ce023cfa7ecb_H0T++3I4nfQVAAAAAElFTkSuQmCC.png)###无法新建或保存文件出现如下信息时，需要开启IDE提权，这可以在[无法启动IDE](#无法启动IDE)内容的图中看到应当勾选内容![d5f47298608d9e7809adf826470aeab4_AekMokGwRz9XAAAAAElFTkSuQmCC.png](attachment:d5f47298608d9e7809adf826470aeab4_AekMokGwRz9XAAAAAElFTkSuQmCC.png)###如何设置成中文界面请搜索jupyternotebook如何设置成中文，修改对应的语言环境变量即可，如设置中文时需要变量名为：LANG变量值：zh_CN.UTF8。###虚拟U盘文件复制失败，看不到文件。####从电脑复制到U盘这种情况可能是文件较大没能完整传输所导致的，可以在传输结束后在板子终端执行一下`sync`来同步一下文件，或者可以输入`reboot`来重启生效####在U盘创建文件只用板子命令行终端在`/root/`下创建文件够不会同步在U盘里显示，这时需要重启一下就能在U盘里看到通过板子终端在`/root/`所创建的文件了###终端执行python文件报错-提示`SyntaxError:Non-UTF-8`的话是需要先在文件头部声明编码格式比如再头部添加一句`#encoding:utf-8`可以看[这里](https://wiki.sipeed.com/soft/maixpy3/zh/origin/python.html)学习一下###notebook实时画面会出现撕裂，而图传正常。-停止程序后重新运行一下程序就行，常发生在运行多次显示不同尺寸的图像时候，这是一个不太好修的bug（逃）。![image.png](attachment:image.png)-不过运行代码时的图传服务http://127.0.0.1:18811不受影响，是jupyterrpyc_ikernel内核内部实现的问题。###更多请查阅[MaixPy3常见问题与解决方法](https://wiki.sipeed.com/soft/maixpy3/zh/question/maixpy3_faq.html)"}, "/soft/maixpy3/zh/usage/train_AI/v831_sobel.html": {"title": "v831 部署 Sobel 卷积边缘检测", "content": "|时间|负责人|更新内容|备注||---|---|---|:---:||2021年8月3日|neucrack|首次编写并发布教程|发布于neucrack的[博客](https://neucrack.com/p/377)||2021年10月23日|Rui|收集文章并整理到文档中|---||2022年1月22日|Rui&dianjixz|使用Jupyter编写文档|工程文件[下载](https://github.com/dianjixz/v831_sobel)，文档还需要二次整理||2022年1月18日|Rui|修改部分代码|MaixPy3更新到0.4.3，部分API发生变化|>Windows系统不支持！边缘就是值变化剧烈的地方,如果对值的变化求导,则边缘部分就是导数局部最大。但是在图像处理时没有具体的函数让我们求导,使用卷积运算则可以很好的近似替代。卷积运算是神经网络的最常用的基本运算，所以非常适合用来展示神经网络在v831上的部署过程。##边缘检测效果![](https://neucrack.com/image/1/377/test.jpg)![](https://neucrack.com/image/1/377/sobel_edge2.jpg)![](https://neucrack.com/image/1/377/final.jpg)![](https://neucrack.com/image/1/377/sobel_edge.jpg)![](https://neucrack.com/image/1/377/sobel_v831.jpg)![](./dnn/sobel.jpg)##卷积边缘检测原理如下图,假设左上为坐标原点,横轴为x,纵轴为y,如下图左上角9个像素点,P(x,y)表示坐标(x,y)的点,要求P(1,1)处在x轴的变化率,则只需将P(2,1)-P(0,1)得到值为0,P(1,0)处为1-3=-2,这个差值即变化率,类比成导数,我们就能知道横轴在哪些地方变化率更大。![](https://neucrack.com/image/1/377/conv.jpg)上面这种方法我们可以得到横轴的变化率,这里使用卷积核~~~python[-1,0,1],[-2,0,2],[-1,0,1]~~~对图像进行卷积运算,如图中的计算方法,像素点左右权值取2,角上的也参与计算,但是权值为1,没有左右边的权值高.这样我们就得到了横轴的变化率图,即边缘检测图.注意,这里是对横轴计算了,比较的点左右的值变化,所以实际看到的图像会出现明显的纵轴边缘,如下图左边![](https://neucrack.com/image/1/377/vertical_horizontal.jpg)同理,上图右边的图使用卷积核[1,2,1],[0,0,0],[-1,-2,-1]得到的纵轴的边缘图。注意这里用右边减左边,如果右边的值比左边的小会是负数,如果我们希望只检测颜色值变大(变白)则可以直接使用,如果两个变化方向都要检测,则可以取绝对值.比如下图左边是没有取绝对值,右边取了绝对值![](https://neucrack.com/image/1/377/without_with_abs.jpg)得到两个方向的图后,对其进行合并,对每个像素平方和开方即可![](https://neucrack.com/image/1/377/final.jpg)这张图左边是使用GIMP的sobel边缘检测(垂直+水平)的效果,略微有点不同:![](https://neucrack.com/image/1/377/sobel_edge2.jpg)不同的原因是使用水平和垂直的图平方和开根后,直接用`plt.imshow`显示,和GIMP的处理方式不同```pythonout=np.sqrt(np.square(out_v)+np.square(out_h))plt.imshow(out)```简单地直接将值规范到`[0,255]`就和GIMP的图相似了(但不完全一样)```pythonout=np.sqrt(np.square(out_v)+np.square(out_h))out=out*255.0/out.max()plt.imshow(out.astype(np.uint8))```![](https://neucrack.com/image/1/377/sobel_v_h.jpg)##自定义卷积核来实现边缘检测除了上面说了使用两次卷积计算,也可以用只计算一次的卷积核,比如:```bash[-1,-1,-1],[-1,8,-1],[-1,-1,-1]```这是对于一个通道(灰度图)来说,如果要扩充到三个通道(RGB),卷积核参数就是如下形式```bashconv_rgb_core_sobel=[[[-1,-1,-1],[-1,8,-1],[-1,-1,-1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[-1,-1,-1],[-1,8,-1],[-1,-1,-1],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[-1,-1,-1],[-1,8,-1],[-1,-1,-1],]]```经过卷积运算后,前后图如下:![](https://neucrack.com/image/1/377/sobel_edge.jpg)注意,输入值范围如果为`[0,255]`,输出值则范围会变化,以图片形式查看时需要注意加以处理,这里使用了`plt.imshow(out)`来显示,这个函数会自动对图像做简单的处理,才会看起来是黑色背景##导出成模型使用可以将Net导出成onnx即可在其它平台使用,就是一个简单的卷积层部署到V831后的样子(使用了卷积核`[-1,-1,-1],[-1,8,-1],[-1,-1,-1],`):![](https://neucrack.com/image/1/377/sobel_v831.jpg)V831部署[源码](https://github.com/sipeed/MaixPy3/blob/master/ext_modules/_maix_nn/example/load_forward_sobel_edge_camera.py)在github，模型在[maixhub](https://www.maixhub.com/modelInfo?modelId=24)上可以下载##边缘检测源码>这是在电脑上运行的代码，不是在开发板平台上运行的代码，需要安装Pytorch使用环境，请自行百度安装。工程文件[下载](https://github.com/dianjixz/v831_sobel)'''simplesobeledgedemovisit:https://neucrack.com/p/377@authorneucrack@licenseMIT'''#引入模块importtorchimporttorch.nnasnnimportnumpyasnpfromPILimportImageimportmatplotlib.pyplotasplt###定义一个网络模型classNet(nn.Module):def__init__(self):super(Net,self).__init__()self.conv1=nn.Conv2d(3,3,3,padding=(0,0),bias=False)defforward(self,x):x=self.conv1(x)returnxnet=Net()###定义卷积权重#sobel全边缘检测算子conv_rgb_core_sobel=[[[-1,-1,-1],[-1,8,-1],[-1,-1,-1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[-1,-1,-1],[-1,8,-1],[-1,-1,-1],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[-1,-1,-1],[-1,8,-1],[-1,-1,-1],]]#sobel垂直边缘检测算子conv_rgb_core_sobel_vertical=[[[-1,0,1],[-2,0,2],[-1,0,1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[-1,0,1],[-2,0,2],[-1,0,1],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[-1,0,1],[-2,0,2],[-1,0,1],]]#sobel水平边缘检测算子conv_rgb_core_sobel_horizontal=[[[1,2,1],[0,0,0],[-1,-2,-1],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[1,2,1],[0,0,0],[-1,-2,-1],[0,0,0],[0,0,0],[0,0,0]],[[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0],[1,2,1],[0,0,0],[-1,-2,-1],]]###网络载入权重函数defsobel(net,kernel):sobel_kernel=np.array(kernel,dtype='float32')sobel_kernel=sobel_kernel.reshape((3,3,3,3))net.conv1.weight.data=torch.from_numpy(sobel_kernel)params=list(net.parameters())###打开一张图片pil_img=Image.open(\"./images/class1_5.jpg\")display(pil_img)input_img=np.array(pil_img)print(input_img.shape)###图片归一化处理input_tensor=(input_img.astype(np.float32)-127.5)/128#to[-1,1]print(input_tensor.shape)input_tensor=torch.Tensor(input_tensor).permute((2,0,1))input_tensor=input_tensor.unsqueeze(0)print(\"inputshape:\",input_tensor.shape)(224, 224, 3)\ninput shape: torch.Size([1, 3, 224, 224])###输入图片转换成PyTorch张量input_tensor=(input_img.astype(np.float32)-127.5)/128#to[-1,1]input_tensor=torch.Tensor(input_tensor).permute((2,0,1))print(input_tensor.shape)input_tensor=input_tensor.unsqueeze(0)print(\"inputshape:\",input_tensor.shape)torch.Size([3, 224, 224])\ninput shape: torch.Size([1, 3, 224, 224])###模型推理sobel_img_t=Nonesobel_vertical_img_t=Nonesobel_horizontal_img_t=None#载入网络权重sobel(net,conv_rgb_core_sobel_vertical)#在推理模式下运行网络withtorch.no_grad():out=net(input_tensor)sobel_vertical_img_t=out.numpy()[0].transpose([1,2,0])#载入网络权重sobel(net,conv_rgb_core_sobel_horizontal)#在推理模式下运行网络withtorch.no_grad():out=net(input_tensor)sobel_horizontal_img_t=out.numpy()[0].transpose([1,2,0])#载入网络权重sobel(net,conv_rgb_core_sobel)#在推理模式下运行网络withtorch.no_grad():out=net(input_tensor)sobel_img_t=out.numpy()[0].transpose([1,2,0])plt.figure()plt.figure()plt.subplot(1,5,1)plt.imshow(input_img)plt.subplot(1,5,2)plt.imshow(sobel_img_t)plt.subplot(1,5,3)plt.imshow(sobel_vertical_img_t)plt.subplot(1,5,4)plt.imshow(sobel_horizontal_img_t)plt.subplot(1,5,5)out=np.sqrt(np.square(sobel_vertical_img_t)+np.square(sobel_horizontal_img_t))plt.imshow(out)plt.show()Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\nClipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).###导出onnx网络withtorch.no_grad():torch.onnx.export(net,input_tensor,\"./model.onnx\",export_params=True,input_names=[\"input0\"],output_names=[\"output0\"])print(\"导出网络完成！\")导出网络完成！###使用ncnn工具将onnx网络转换成ncnn网络以下代码中会调用用户环境中的ncnn工具，请确保已经安装好并加入环境变量。defonnx_to_ncnn(input_shape,onnx=\"out/model.onnx\",ncnn_param=\"out/conv0.param\",ncnn_bin=\"out/conv0.bin\"):importos#onnx2ncnntoolcompiledfromncnn/tools/onnx,andinthebulddircmd=f\"onnx2ncnn{onnx}{ncnn_param}{ncnn_bin}\"#可以更换工具目录os.system(cmd)withopen(ncnn_param)asf:content=f.read().split(\"\\n\")iflen(input_shape)==1:content[2]+=\"0={}\".format(input_shape[0])else:content[2]+=\"0={}1={}2={}\".format(input_shape[2],input_shape[1],input_shape[0])content=\"\\n\".join(content)withopen(ncnn_param,\"w\")asf:f.write(content)onnx_to_ncnn(input_shape=(3,224,224),onnx=\"./model.onnx\",ncnn_param=\"./conv0.param\",ncnn_bin=\"./conv0.bin\")print(\"netsuccess!\")net success!##使用MaixHub在线量化工具进行网络量化在线转换需要上传一个压缩包文件-该功能只能支持上传一个无密码的zip压缩包-压缩包内需要包含一个images目录，一个xxx.bin，一个xxx.param-需要将矫正图片放入images目录内；矫正图片集可考虑直接采用训练中的验证数据集，并务必保证矫正时图像的预处理方式与训练和部署时一致。>注意：确保images目录内没有混入其他文件，否则会导致模型量化错误。zip压缩包目录结构~~~bash└─sobel.zip.├──images│├──class1_0.jpg│├──class1_1.jpg│├──class1_2.jpg│├──class1_3.jpg│├──class1_4.jpg│└──class1_5.jpg├──sobel.bin└──sobel.param1directory,8files~~~制作好压缩包后，使用[MaixHub](https://maixhub.com/modelConvert)的在线转换工具进行模型转换，查看使用说明。![](https://neucrack.com/image/1/358/maixhub.jpg)登陆后,上传你的压缩包等待模型转换任务完成。##边缘检测模型部署等待模型转换完成,下载转换好的模型文件。得到的*.param和*.bin文件就是部署在v831上的文件。将模型文件上传到v831上。#v831运行边缘检测的代码frommaiximportnn,camera,display,imageimportnumpyasnpimporttimemodel={\"param\":\"./sobel_int8.param\",\"bin\":\"./sobel_int8.bin\"}input_size=(224,224,3)output_size=(222,222,3)options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":input_size},\"outputs\":{\"output0\":output_size},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],}print(\"--loadmodel:\",model)m=nn.load(model,opt=options)print(\"--loadok\")whileTrue:img=camera.capture().resize(224,224)out=m.forward(img,quantize=True,layout=\"hwc\")out,=out.astype(np.float32).reshape(output_size)out=(np.ndarray.__abs__(out)*255/out.max()).astype(np.uint8)data=out.tobytes()img2=img.load(data,(222,222),mode=\"RGB\")display.show(img2)边缘检测到此结束。"}, "/soft/maixpy3/zh/usage/hardware/back/UART_1.html": {"title": "UART", "content": "---\ntitle: UART\nkeywords: maixpy3, UART\ndesc: maixpy3 doc: UART\n---\n\n## UART 的使用\n\n串口可以用于与别的开发板或者是单片机进行数据的通信，用于链接别的开发板或者单片机。\n\n根据所接的串口号进行修改以下代码，\n\n> 以下代码由于 MaixPy3 还在优化中，可能不能运行，具体的代码到 [github](https://github.com/sipeed/MaixPy3) 上查看\n\n```python\nimport serial\n\nser = serial.Serial(\"/dev/ttyS1\",115200)    # 使用 UART1 ，波特率设置为 115200\n\nprint('serial test start ...')\nser.write(b\"Hello Wrold !!!\\n\")\ntry:\n    while True:\n        ser.setDTR(True)\n        ser.setRTS(True)\n        tmp = ser.readline()\n        print(tmp)\n        ser.write(tmp)\n        ser.setDTR(False)\n        ser.setRTS(False)\nexcept KeyboardInterrupt:\n    if ser != None:\n        ser.close()\n```\n\n## 什么是串口\n\n通用异步收发传输器（Universal Asynchronous Receiver/Transmitter，通常称作UART） 是一种串行异步收发协议，应用十分广泛。UART工作原理是将数据的二进制位一位一位的进行传输。在UART通讯协议中信号线上的状态位高电平代表’1’低电平代表’0’。当然两个设备使用UART串口通讯时，必须先约定好传输速率和一些数据位。\n\n## 硬件连接\n硬件连接比较简单，仅需要3条线，注意连接时两个设备UART电平，如电平范围不一致请做电平转换后再连接，如下图所示：\n\n- TX：发送数据端，要接对面设备的RX\n- RX：接收数据端，要接对面设备的TX\n- GND：保证两设备共地，有统一的参考平面\n\n![](./../asserts/UART.jpg)\n\n## 串口工作原理\n\n- 发送接收\n\n发送逻辑对从发送FIFO 读取的数据执行“并→串”转换。控制逻辑输出起始位在先的串行位流，并且根据控制寄存器中已编程的配置，后面紧跟着数据位（注意：最低位 LSB 先输出）、奇偶校验位和停止位。\n\n在检测到一个有效的起始脉冲后，接收逻辑对接收到的位流执行“串→并”转换。此外还会对溢出错误、奇偶校验错误、帧错误和线中止（line-break）错误进行检测，并将检测到的状态附加到被写入接收FIFO 的数据中。 [3] \n\n- 波特率产生\n\n波特率除数（baud-rate divisor）是一个22 位数，它由16 位整数和6 位小数组成。波特率发生器使用这两个值组成的数字来决定位周期。通过带有小数波特率的除法器，在足够高的系统时钟速率下，UART 可以产生所有标准的波特率，而误差很小。\n\n- 数据收发\n\n发送时，数据被写入发送FIFO。如果UART 被使能，则会按照预先设置好的参数（波特率、数据位、停止位、校验位等）开始发送数据，一直到发送FIFO 中没有数据。一旦向发送FIFO 写数据（如果FIFO 未空），UART 的忙标志位BUSY 就有效，并且在发送数据期间一直保持有效。BUSY 位仅在发送FIFO 为空，且已从移位寄存器发送最后一个字符，包括停止位时才变无效。即 UART 不再使能，它也可以指示忙状态。BUSY 位的相关库函数是UARTBusy( )\n\n在UART 接收器空闲时，如果数据输入变成“低电平”，即接收到了起始位，则接收计数器开始运行，并且数据在Baud16 的第8 个周期被采样。如果Rx 在Baud16 的第8 周期仍然为低电平，则起始位有效，否则会被认为是错误的起始位并将其忽略。\n\n如果起始位有效，则根据数据字符被编程的长度，在 Baud16 的每第 16 个周期（即一个位周期之后）对连续的数据位进行采样。如果奇偶校验模式使能，则还会检测奇偶校验位。\n最后，如果Rx 为高电平，则有效的停止位被确认，否则发生帧错误。当接收到一个完整的字符时，将数据存放在接收FIFO 中。\n\n- 中断控制\n    - 出现以下情况时，可使UART 产生中断：\n    - FIFO 溢出错误\n    - 线中止错误（line-break，即Rx 信号一直为0 的状态，包括校验位和停止位在内）\n    - 奇偶校验错误\n    - 帧错误（停止位不为1）\n    - 接收超时（接收FIFO 已有数据但未满，而后续数据长时间不来）\n    - 发送\n    - 接收\n    - 由于所有中断事件在发送到中断控制器之前会一起进行“或运算”操作，所以任意时刻 UART 只能向中断产生一个中断请求。通过查询中断状态函数UARTIntStatus( )，软件可以在同一个中断服务函数里处理多个中断事件（多个并列的if 语句）。\n\n- FIFO 操作\n\nFIFO 是“First-In First-Out”的缩写，意为“先进先出”，是一种常见的队列操作。 Stellaris 系列ARM 的UART 模块包含有2 个16 字节的FIFO：一个用于发送，另一个用于接收。可以将两个FIFO 分别配置为以不同深度触发中断。可供选择的配置包括：1/8、 1/4、1/2、3/4 和7/8 深度。例如，如果接收FIFO 选择1/4，则在UART 接收到4 个数据时产生接收中断。\n\n发送FIFO的基本工作过程： 只要有数据填充到发送FIFO 里，就会立即启动发送过程。由于发送本身是个相对缓慢的过程，因此在发送的同时其它需要发送的数据还可以继续填充到发送 FIFO 里。当发送 FIFO 被填满时就不能再继续填充了，否则会造成数据丢失，此时只能等待。这个等待并不会很久，以9600 的波特率为例，等待出现一个空位的时间在1ms 上下。发送 FIFO 会按照填入数据的先后顺序把数据一个个发送出去，直到发送 FIFO 全空时为止。已发送完毕的数据会被自动清除，在发送FIFO 里同时会多出一个空位。\n\n接收FIFO的基本工作过程： 当硬件逻辑接收到数据时，就会往接收FIFO 里填充接收到的数据。程序应当及时取走这些数据，数据被取走也是在接收FIFO 里被自动删除的过程，因此在接收 FIFO 里同时会多出一个空位。如果在接收 FIFO 里的数据未被及时取走而造成接收FIFO 已满，则以后再接收到数据时因无空位可以填充而造成数据丢失。\n\n收发FIFO 主要是为了解决UART 收发中断过于频繁而导致CPU 效率不高的问题而引入的。在进行 UART 通信时，中断方式比轮询方式要简便且效率高。但是，如果没有收发 FIFO，则每收发一个数据都要中断处理一次，效率仍然不够高。如果有了收发FIFO，则可以在连续收发若干个数据（可多至14 个）后才产生一次中断然后一并处理，这就大大提高了收发效率。\n\n完全不必要担心FIFO 机制可能带来的数据丢失或得不到及时处理的问题，因为它已经帮你想到了收发过程中存在的任何问题，只要在初始化配置UART 后，就可以放心收发了， FIFO 和中断例程会自动搞定一切。\n\n- 回环操作\nUART 可以进入一个内部回环（Loopback）模式，用于诊断或调试。在回环模式下，从Tx 上发送的数据将被Rx 输入端接收。\n\n- 串行红外协议\n\n在某些 Stellaris 系列 ARM 芯片里，UART 还包含一个 IrDA 串行红外（SIR）编码器/ 解码器模块。IrDA SIR 模块的作用是在异步UART数据流和半双工串行SIR 接口之间进行转换。片上不会执行任何模拟处理操作。SIR 模块的任务就是要给UART 提供一个数字编码输出和一个解码输入。UART 信号管脚可以和一个红外收发器连接以实现IrDA SIR物理层连接。\n\n\n> 参考：百度百科、CSDN博客"}, "/soft/maixpy3/zh/usage/hardware/back/GPIO_1.html": {"title": "GPIO", "content": "---\ntitle: GPIO\nkeywords: maixpy3, GPIO\ndesc: maixpy3 doc: GPIO\n---\n\n## 如何使用 GPIO 输出高低电平\n\nGPIO 是可以复用成别的通信接口，对于 MaixPy3 来说并不需要那么麻烦，GPIO 就用来输出高低电平，别的用法后面再说。\n\n下面以 MaixII-Dock 开发板为例子讲述如果使用 MaixPy3 输出高低电平。\n\n通过查看 MaixII-Dock 的引出管脚图可以知道，那些管脚可以直接用来当 GPIO 口使用\n\n![](./../asserts/M2Dock_pin.png)\n\n> 以下代码由于 MaixPy3 还在优化中，可能不能运行，具体的代码到 [github](https://github.com/sipeed/MaixPy3) 上查看\n\n```python\nfrom maix import GPIO\nimport time\nled =GPIO.pin(\"PH\", 14)             # 设置使用 PH 14 管脚\nwhile led:\n    led.set_value(0)                # 设置低电平\n    time.sleep(0.5)\n    print(\"0\", led.get_value())     # 获取管脚当前状态\n    led.set_value(1)                # 设置高电平\n    time.sleep(0.5)\n    print(\"1\", led.get_value())     # 获取管脚当前状态\n\n```\n\n## 什么是 GPIO \nGPIO（英语：General-purpose input/output），通用型之输入输出的简称，功能类似8051的P0—P3，其接脚可以供使用者由程控自由使用，PIN脚依现实考量可作为通用输入（GPI）或通用输出（GPO）或通用输入与输出（GPIO），如当clk generator, chip select等。\n\n既然一个引脚可以用于输入、输出或其他特殊功能，那么一定有寄存器用来选择这些功能。对于输入，一定可以通过读取某个寄存器来确定引脚电位的高低；对于输出，一定可以通过写入某个寄存器来让这个引脚输出高电位或者低电位；对于其他特殊功能，则有另外的寄存器来控制它们。\n> 源自[百度百科](https://baike.baidu.com/item/gpio/4723219?fr=aladdin)\n\n## GPIO 的用途\n\n不同系统间的GPIO的确切作用不同。通用常有下面几种：\n\n1. 输出值可写（高=1，低=0）。一些芯片也可以选择驱动这些值的方式，以便支持“线-或”或类似方案（开漏信号线）。\n2. 输入值可读（1，0）。一些芯片支持输出管脚回读，这在线或的情况下非常有用（以支持双向信号线）。GPIO控制器可能具有一个输入防故障/防反跳逻辑，有时还会有软件控制。\n3. 输入经常被用作中断信号，通常是边沿触发，但也有可能是电平触发。这些中断可以配置为系统唤醒事件，从而将系统从低功耗模式唤醒。\n4. 一个GPIO经常被配置为输入/输出双向，根据不同的产品单板需求，但也存在单向的情况。\n5. 大多是GPIO可以在获取到spinlock自旋锁时访问，但那些通过串行总线访问的通常不能如此操作（休眠的原因）。一些系统中会同时存在这两种形式的GPIO。\n6. 在一个给定单板上，每个GPIO用于一个特定的目的，如监控MMC/SD卡的插入/移除，检查卡写保护状态，驱动LED，配置发送器，串行总线位拆，触发一个硬件看门狗，触发一个开关之类的。\n\n> 原则[电子发烧友论坛](http://www.elecfans.com/emb/jiekou/20171206595752.html)\n\n\n\n只需要修改对应的管脚口即可进行高低电平输出，对于图中别的通信使用方式会在后面讲述\n\n对于 GPIO 更多的使用方式，通过查看开发板的规格书得知"}, "/soft/maixpy3/zh/usage/hardware/back/PWM_1.html": {"title": "PWM", "content": "---\ntitle: PWM\nkeywords: maixpy3, PWM\ndesc: maixpy3 doc: PWM\n---\n\n## 使用 PWM\n\n当需要使用 PWM 驱动一些外设的时候，可以通过以下的代码进行驱动，对于不同的 Linux 开发板来说，PWM 信号输出的管脚号是不一样的，只需要修改 PWM 输出口\n\n> 以下代码由于 MaixPy3 还在优化中，可能不能运行，具体的代码到 [github](https://github.com/sipeed/MaixPy3) 上查看\n\n```python\nfrom maix import pwm\nimport time\npwm_output = pwm.PWM(6, freq=1000)          # s设置输出 PWM 的管脚和频率\nwhile True:\n    pwm_output.out(pulse_width_percent = 50)    # 设置 PWM 的占空比并开始输出\n    time.sleep(1)\n    pwm_output.step()                           # 停止输出 pwm 信号\n    time.sleep(0.1)\n\n```\n\n## 什么是 PWM？\n脉冲宽度调制是一种模拟控制方式，根据相应载荷的变化来调制晶体管基极或MOS管栅极的偏置，来实现晶体管或MOS管导通时间的改变，从而实现开关稳压电源输出的改变。这种方式能使电源的输出电压在工作条件变化时保持恒定，是利用微处理器的数字信号对模拟电路进行控制的一种非常有效的技术。脉冲宽度调制是利用微处理器的数字输出来对模拟电路进行控制的一种非常有效的技术，广泛应用在从测量、通信到功率控制与变换的许多领域中。\n\n## PWM 工作原理\n脉宽调制（PWM）基本原理：控制方式就是对逆变电路开关器件的通断进行控制，使输出端得到一系列幅值相等但宽度不一致的脉冲，用这些脉冲来代替正弦波或所需要的波形。也就是在输出波形的半个周期中产生多个脉冲，使各脉冲的等值电压为正弦波形，所获得的输出平滑且低次谐波少。按一定的规则对各脉冲的宽度进行调制，既可改变逆变电路输出电压的大小，也可改变输出频率。\n\n例如，把正弦半波波形分成N等份，就可把正弦半波看成由N个彼此相连的脉冲所组成的波形。这些脉冲宽度相等，都等于 π/n ，但幅值不等，且脉冲顶部不是水平直线，而是曲线，各脉冲的幅值按正弦规律变化。如果把上述脉冲序列用同样数量的等幅而不等宽的矩形脉冲序列代替，使矩形脉冲的中点和相应正弦等分的中点重合，且使矩形脉冲和相应正弦部分面积（即冲量）相等，就得到一组脉冲序列，这就是PWM波形。可以看出，各脉冲宽度是按正弦规律变化的。根据冲量相等效果相同的原理，PWM波形和正弦半波是等效的。对于正弦的负半周，也可以用同样的方法得到PWM波形。\n\n在PWM波形中，各脉冲的幅值是相等的，要改变等效输出正弦波的幅值时，只要按同一比例系数改变各脉冲的宽度即可，因此在交－直－交变频器中，PWM逆变电路输出的脉冲电压就是直流侧电压的幅值。\n\n根据上述原理，在给出了正弦波频率，幅值和半个周期内的脉冲数后，PWM波形各脉冲的宽度和间隔就可以准确计算出来。按照计算结果控制电路中各开关器件的通断，就可以得到所需要的PWM波形。下图为变频器输出的PWM波的实时波形。\n![](./../asserts/pwm.gif)\n![](./../asserts/pwm.jpg)\n\n\n\n> 源自百度百科"}, "/soft/maixpy3/zh/usage/hardware/back/I2C_1.html": {"title": "I2C", "content": "---\ntitle: I2C\nkeywords: maixpy3, I2C\ndesc: maixpy3 doc: I2C\n---\n\n## 如何使用 I2C \n\n通过查看开发板上的管脚引出 I2C 相关引脚，管脚图中得 TWI 对应的就是 I2C 管脚\n\n![](./../asserts/M2Dock_pin.png)\n\n```python\nfrom maix import i2c\ni2c_address = i2c.I2CSecan('/dev/i2c-2')            # 获取设备在 I2C 的地址数据\nprint(i2c_address)\ni2c_im = i2c.read(i2c_address, 4)                   # 获取设备信息\ni2c_ID = i2c.read_register(i2c_address, 0x36, 4)    # 读取设备寄存器上的信息\ni2c.writes(i2c_address, 1)                          # 对设备上发送数据\ni2c.writes_register(i2c_address, 0x38, 0xab)        # 对设备上的寄存器写入数据\n```\n> 以上是 MaixII-Dock 开发板的板载三轴加速度传感器部分代码，完整代码在 [Github](https://github.com/sipeed/MaixPy3/blob/release/tests/maix_v831/usage_v831_i2c-2.py)\n\n## 什么是 I2C 总线\n\n1. I2C 总线是由 Philips 公司开发的一种简单、双向二线制同步串行总线。它只需要两根线即可在连接于总线上的设备之间传送信息。\n2. 主设备用于启动总线传送数据，并产生时钟以开放传送的设备，此时任何被寻址的设备均被认为是从设备．总线上主设备和从设备、发数据设备和收数据设备的关系不是恒定的，而取决于此时数据传送方向。\n3. 如果主设备要发送数据给从设备，则主设备首先要寻址从设备，然后主动发送数据至从设备，最后由主设备终止数据传送；如果主设备要接收从设备的数据，首先由主设备寻址从设备．然后主设备接收从设备发送的数据，最后由主设备终止接收过程。在这种情况下．主机负责产生定时时钟和终止数据传送。\n\n## I2C 通信设备原理\n\n1. 硬件结构：通信只需要两根传输线，结构上及其简单。\n    - SCL(serial clock)：时钟线，传输 CLK 信号，一般是 I2C 主设备向从设备提供时钟的通道。\n    - SDA(serial data): 数据线，通信数据都通过SDA线传输。\n\n2. 通信特征：串行、同步、非差分、低速率\n    - I2C 属于串行通信，所有的数据以位为单位在 SDA 线上串行传输。\n    - 同步通信就是通信双方工作在同一个时钟下，一般是通信的 A 方通过一根 CLK 信号线传输 A 自己的时钟给 B，B 工作在 A 传输的时钟下。所以同步通信的显著特征就是：通信线中有 CLK\n    - 非差分。因为 I2C 通信速率不高，而且通信双方距离很近，所以使用电平信号通信。\n    - 低速率。I2C 一般是用在同一个板子上的2个 IC 之间的通信，而且用来传输的数据量不大，所以本身通信速率很低（一般几百KHz，不同的 I2C 芯片的通信速率可能不同，具体在编程的时候要看自己所使用的设备允许的 I2C 通信最高速率，不能超过这个速率）\n\n3. 通信方向：主设备+从设备\n    - I2C 通信的时候，通信双方地位是不对等的，而是分主设备和从设备。通信由主设备发起，由主设备主导，从设备只是按照 I2C 协议被动的接受主设备的通信，并及时响应。\n    - 谁是主设备、谁是从设备是由通信双方来定的（ I2C 协议并无规定），一般来说一个芯片可以只能做主设备、也可以只能做从设备、也可以既能当主设备又能当从设备（软件配置）。\n\n4. 一对多通信\n    - I2C 通信可以一对一（1个主设备对1个从设备），也可以一对多（1个主设备对多个从设备）。\n    - 主设备来负责调度总线，决定某一时间和哪个从设备通信。注意：同一时间内，I2C的总线上只能传输一对设备的通信信息，所以同一时间只能有一个从设备和主设备通信，其他从设备处于“冬眠”状态，不能出来捣乱，否则通信就乱套了。\n    - 每一个 I2C 从设备在通信中都有一个 I2C 从设备地址，这个设备地址是从设备本身固有的属性，然后通信时主设备需要知道自己将要通信的那个从设备的地址，然后在通信中通过地址来甄别是不是自己要找的那个从设备。（这个地址是一个电路板上唯一的，不是全球唯一的）\n\n5. 主要用途\n    - 主要用途：SoC 和周边外设之间的通信（典型的如 EEPROM、电容触摸 IC、各种 sensor 等）\n\n\n> 原文链接：https://blog.csdn.net/weixin_46089486/article/details/108992588"}, "/soft/maixpy3/zh/usage/hardware/back/SPI_1.html": {"title": "SPI", "content": "---\ntitle: SPI\nkeywords: maixpy3, SPI\ndesc: maixpy3 doc: SPI\n---\n\n## 使用 SPI\n\n> 以下代码由于 MaixPy3 还在优化中，可能不能运行，具体的代码到 [github](https://github.com/sipeed/MaixPy3) 上查看\n\n```python\nimport spidev, time\nspi = spidev.SpiDev(mode=SPI.MODE_MASTER, baudrate=10000000, polarity=0, phase=0, bits=8, firstbit=SPI.MSB)   # SPI 初始化\nspi.open(1, 0)                      # 使用  SPI 1.0 \n\nwhile True:\n  time.sleep(0.1)\n  to_send = [0x01, 0x02, 0x01]\n  to_get = []\n  print(spi.write_readinto(to_send, to_get, cs=SPI.CS0)) \n\n```\n\n## 什么是 SPI\nSPI是串行外设接口（Serial Peripheral Interface）的缩写，是一种高速的，全双工，同步的通信总线，并且在芯片的管脚上只占用四根线，节约了芯片的管脚，同时为PCB的布局上节省空间，提供方便，正是出于这种简单易用的特性，越来越多的芯片集成了这种通信协议，比如AT91RM9200。\n\n## SPI 工作原理\nSPI总线是一种4线总线，因其硬件功能很强，所以与SPI有关的软件就相当简单，使中央处理器（Central Processing Unit，CPU）有更多的时间处理其他事务。正是因为这种简单易用的特性，越来越多的芯片集成了这种通信协议，比如AT91RM9200。SPI是一种高速、高效率的串行接口技术。通常由一个主模块和一个或多个从模块组成，主模块选择一个从模块进行同步通信，从而完成数据的交换。SPI是一个环形结构，通信时需要至少4根线（事实上在单向传输时3根线也可以）。\n\nSPI的通信原理很简单，它以主从方式工作，这种模式通常有一个主设备和一个或多个从设备，需要至少4根线，事实上3根也可以（单向传输时）。也是所有基于SPI的设备共有的，它们是MISO（主设备数据输入）、MOSI（主设备数据输出）、SCLK（时钟）、CS（片选）。\n\n>（1）MISO– Master Input Slave Output，主设备数据输入，从设备数据输出；\n\n>（2）MOSI– Master Output Slave Input，主设备数据输出，从设备数据输入；\n\n>（3）SCLK – Serial Clock，时钟信号，由主设备产生；\n\n>（4）CS – Chip Select，从设备使能信号，由主设备控制。\n\n其中，CS是从芯片是否被主芯片选中的控制信号，也就是说只有片选信号为预先规定的使能信号时（高电位或低电位），主芯片对此从芯片的操作才有效。这就使在同一条总线上连接多个SPI设备成为可能。\n\n接下来就负责通讯的3根线了。通讯是通过数据交换完成的，这里先要知道SPI是串行通讯协议，也就是说数据是一位一位的传输的。这就是SCLK时钟线存在的原因，由SCLK提供时钟脉冲，SDI，SDO则基于此脉冲完成数据传输。数据输出通过 SDO线，数据在时钟上升沿或下降沿时改变，在紧接着的下降沿或上升沿被读取。完成一位数据传输，输入也使用同样原理。因此，至少需要8次时钟信号的改变（上沿和下沿为一次），才能完成8位数据的传输。\n\n时钟信号线SCLK只能由主设备控制，从设备不能控制。同样，在一个基于SPI的设备中，至少有一个主设备。这样的传输方式有一个优点，在数据位的传输过程中可以暂停，也就是时钟的周期可以为不等宽，因为时钟线由主设备控制，当没有时钟跳变时，从设备不采集或传送数据。SPI还是一个数据交换协议：因为SPI的数据输入和输出线独立，所以允许同时完成数据的输入和输出。芯片集成的SPI串行同步时钟极性和相位可以通过寄存器配置，IO模拟的SPI串行同步时钟需要根据从设备支持的时钟极性和相位来通讯。\n\n最后，SPI接口的一个缺点：没有指定的流控制，没有应答机制确认是否接收到数据。\n\nSPI的片选可以扩充选择16个外设,这时PCS输出=NPCS,说NPCS0~3接4-16译码器,这个译码器是需要外接4-16译码器，译码器的输入为NPCS0~3，输出用于16个外设的选择。"}, "/soft/maixpy3/zh/usage/net.html": {"title": "Python3 网络应用", "content": "---\ntitle: Python3 网络应用\nkeywords: MaixPy3,net, Python3\ndesc: maixpy doc: net\n---\n\n\n\n| 更新时间 | 负责人 | 内容 | 备注 |\n| --- | --- | --- | --- |\n| 2022年1月17日 | dalaoshu | 编写文档 | 本文讲述用户常问的实际需求所写的经验文，有经验的可以忽略本节内容，至于如何联网、配网请看类似于 [MaixII-Dock 连接网络](http://wiki.sipeed.com/soft/maixpy3/zh/tools/0.MaixII-Dock.html#%E5%A6%82%E4%BD%95%E8%BF%9E%E6%8E%A5%E7%BD%91%E7%BB%9C) 的产品说明|\n\n> 有人问为什么没有 socket 的文档，看这个[Python socket 标准库](https://www.runoob.com/python3/python3-socket.html) 自行学习吧。\n\n## 关于 Python 网络应用的一些常用例子。\n\n因为是通用的 python3 环境，所以关于网络方面的库很多，最终目的都是为了上云。\n\n- [requests](https://github.com/psf/requests)\n\n- [paho.mqtt.python](https://github.com/eclipse/paho.mqtt.python)\n\n在 AIOT 领域中，常用的网络请求接口为 mqtt 和 http ，它们均基于 socket 这个模块提供的 tcp 、udp 实现帮助用户方便进行网络传输数据。\n\n与经典的 TCP 、UDP 相比，HTTP 和 MQTT 则更是符合互联网、物联网特征的传输协议。\n\n例如 HTTP 将网络行为描述成从某个服务器上获取数据的 GET ，将某个数据提交给服务器的 POST 协议描述，意味着用户不需要对此重新设计协议，根据业务逻辑使用就行。\n\n而 MQTT 让用户忽略对服务器的部署和开发，将其作为数据中转服务，将两个设备的链接行为描述成互相通信行为，而非请求服务行为，让用户只需要关注客户端的发送行为即可，而不需要考虑服务端的接收。\n\n简化的背后是需要很多逻辑维持的，同时还存在一些安全隐患问题，如早期的 HTTP 是明文传输，MQTT 是公开通信，如果要商用就需要配置相关的安全功能。\n\n## HTTP 有什么用？怎么用？\n\n你可以在终端里 `pip3 install requests` 获取这个模块。\n\n> 如果你想要获取网页的数据，或是向服务器提交数据，那 HTTP 就是你最好的朋友。\n\nRequests 唯一的一个非转基因的 Python HTTP 库，人类可以安全享用。\n\n**警告**：非专业使用其他 HTTP 库会导致危险的副作用，包括：安全缺陷症、冗余代码症、重新发明轮子症、啃文档症、抑郁、头疼、甚至死亡。\n\n看吧，这就是 Requests 的威力：\n\n```\n>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\n>>> r.status_code\n200\n>>> r.headers['content-type']\n'application/json; charset=utf8'\n>>> r.encoding\n'utf-8'\n>>> r.text\nu'{\"type\":\"User\"...'\n>>> r.json()\n{u'private_gists': 419, u'total_private_repos': 77, ...}\n```\n\n[参见 未使用 Requests 的相似代码.](https://gist.github.com/kennethreitz/973705)\n\nRequests 允许你发送纯天然，植物饲养的 HTTP/1.1 请求，无需手工劳动。你不需要手动为 URL 添加查询字串，也不需要对 POST 数据进行表单编码。Keep-alive 和 HTTP 连接池的功能是 100% 自动化的，一切动力都来自于根植在 Requests 内部的 urllib3。\n\n更多请看该文档[Requests: 让 HTTP 服务人类](https://docs.python-requests.org/zh_CN/latest/)。\n\n## MQTT 有什么用？怎么用？\n\n你可以在终端里 `pip3 install paho.mqtt` 获取这个模块。\n\n> 如果没有服务器也想要一对多采集数据，或是许多设备彼此建立通信，那 MQTT 就是你最好的朋友。\n\n使用 MQTT 服务需要一台运行 MQTT 的服务，网上有许多公共服务器供你使用，而对于任何一台机器，只要是运行 python 代码，只会发生 订阅主题 和 推送主题 两个逻辑，这个主题就是一间房，在这间房的所有连接都会收到关于这个主题的信息，它就像一个聊天室，设备加入了，可以收到所有通知。\n\n### 如何推送？\n\n向公共服务 mqtt.eclipseprojects.io 发送主题 `paho/test/single` 内容为 `boo123` 的消息。\n\n```python\n>>> import paho.mqtt.publish as publish\n>>>\n>>> publish.single(\"paho/test/single\", \"boo123\", hostname=\"mqtt.eclipseprojects.io\")\n>>>\n```\n\n### 如何订阅？\n\n那发出去的数据如何收到呢？看下图代码示例\n\n```python\n>>> import paho.mqtt.subscribe as subscribe\n>>>\n>>> topics = ['paho/test/single']\n>>>\n>>> m = subscribe.simple(topics, hostname=\"mqtt.eclipseprojects.io\", retained=False)\n>>> m.topic\n'paho/test/single'\n>>> m.payload\nb'boo123'\n>>>\n```\n\n\n这两个逻辑，在任何设备上都是一样的代码，用户不需要写服务端的代码，而服务端的任务就是负责转发数据和保护整个传输内容的安全、可靠性。\n\n> 关于 MQTT 的官网 [MQTT: The Standard for IoT Messaging](https://mqtt.org/)。\n\n## MJPG 图传 怎么用？\n\n想通过网络来接收摄像头拍摄到画面，可以使用在 MaixPy3 中内置的 mjpg 包，MJPG 编码是一种常见且简易的视频编码方案，只需要将每一帧压缩成 jpg 图片后不断发送给客户端就行。\n\n确保电脑浏览器客户端和硬件开发板服务端处于到同一个网络内（可以互相 ping 通），运行以下代码，在电脑的浏览器中输入 `http://localhost:18811` 即可获取板子摄像头的实时视频流，其中 localhost 为通过 adb forward 映射的本地 IP 地址，如果是 WIFI 则需要使板子连上 WIFI 路由器后获取可以 ping 通的 IP 地址。\n\n```python\nfrom maix import camera, mjpg, utils, display\n\nqueue = mjpg.Queue(maxsize=8)\nmjpg.MjpgServerThread(\n    \"0.0.0.0\", 18811, mjpg.BytesImageHandlerFactory(q=queue)).start()\n\nwhile True:\n    img = camera.capture()\n    jpg = utils.rgb2jpg(img.convert(\"RGB\").tobytes(), img.width, img.height)\n    queue.put(mjpg.BytesImage(jpg))\n    display.show(img)\n\n```\n\n- 如果使用 MaixII-Dock 开发板，连接 OTG 接口可以实现通过有线实时显示摄像头画面。打开 MaixPy3 IDE，在图片的右键菜单中将 maixpy3_notebook 停止，然后在 adb 终端的 进入 python 环境，运行以上代码，可以直接在下面的中显示画面、\n\n    > <img src=\"http://127.0.0.1:18811\"> 图传画面（没有成功运行代码是不会显示出来的）\n\n    或在浏览器中打开 <http://127.0.0.1:18811>\n\n想了解为什么的可以查看这份代码实现 [mjpg.py](https://github.com/sipeed/MaixPy3/blob/release/maix/mjpg.py) 。"}, "/soft/maixpy3/zh/origin/python.html": {"title": "什么是 Python ？", "content": "---\ntitle: 什么是 Python ？\nkeywords: MaixPy, Python, AIOT, 边缘计算\ndesc: maixpy doc: Python\n---\n\n## 说起 Python 语言\n\n> 只提及一些重点，更详细的就请到一些 Python 教程网站学习吧。\n\n为了让从程序员从~~秃头~~事业中解脱，在开源世界里诞生了名为大蟒蛇（Python）的编程语言。\n\n它带来了什么？\n\n- 提供了完整的软件开发标准库。\n- 适应多种编程方式。\n- 用尽量少的代码完成更多工作。\n- 适应时间短、变化快的需求。\n- 让编程工作看起来更像是在“搭积木”。\n- 开源社区长期贡献了大量的第三方库。\n- 可拓展 C 、 C++ 等其他语言编写的模块。\n- 满足了想偷懒的愿望。\n\n它是一门易用的动态编程语言，可以看作现代入门编程语言的起点，程序员平时或多或少都需要使用 Python 代码帮助完成一些日常、简单、重复的脚本操作。\n\n相比其他编程语言，它看起来会很容易理解和使用，对于非专业人士也可以轻松使用起来，以 “Hello, World” 为例。\n\n- C\n\n```c\n#include <stdio.h>\n\nint main(void) {\n  printf(\"Hello, World!\\n\");\n  return 0;\n}\n```\n\n- C++\n\n```c++\n#include <iostream>\nusing namespace std;\n\nint main() {\n  cout << \"Hello, World!\" << endl;\n  return 0;\n}\n```\n\n- Java\n\n```java\nclass HelloWorld {\n  public static void main(String[] args) {\n    System.out.println(\"Hello, World!\");\n  }\n}\n```\n\n- JavaScript\n\n```javascript\nalert(\"Hello World\")\n```\n\n```javascript\ndocument.write(\"Hello World\")\n```\n\n```javascript\nconsole.log(\"Hello World\")\n```\n\n- Python\n\n```python\nprint(\"Hello, World!\")\n```\n\n不难看出，从人类自然语言的角度来看 Python 语法简单直接，减少了不必要的讯息。\n\n## 它是怎样工作的呢？\n\n在编辑框上编写的 Python 代码，实际上是依次输入到实时运行的解释器程序当中的。\n\n例如下述代码：\n\n```python\ntmp = 1 + 1\nprint(tmp)\n```\n\n运行它后就会输出 2 ，其中 tmp （变量）等于 2 ，如果这时候再运行下述代码。\n\n```python\ntmp = tmp + 1\nprint(tmp)\n```\n\n这时候就会输出 3 ，表示 tmp （变量）等于 3 了。\n\n这是因为每一次运行的 Python 程序都并非是一个全新的开始，它是一直存在于一段程序当中的，只有当解释器程序退出后，才是真的结束程序，所以上一次运行的结果并没有清除，这也是程序不需要编译的原因。\n\n这是与 C / C++ / JAVA 一类编程语言相冲突的地方，我们基于这个差异将 Python 称为动态语言，与此关联的还有 JavaScript 和 Lua 等编程语言。\n\n实时上还有各种各样支持 Python 语言的解释器，虽然写的都是 Python 代码，但并非同一个事物。\n\n我们常用的 Python 编程环境通常指 C 语言实现的 Python 解释器，涵盖 Python2.7 ~ Python3.10 的语法。\n\n而在其他领域来说，有各式各样的 Python 语言的实现，如下：\n\n- MicroPython 使用 Python3.5 语法\n\n- Jython 使用 Java 实现的 Python 语言\n\n- PyPy 通过 JIT 优化的 Python 语言\n\n- IPython 基于 Python 语言的交互接口\n\n也就是说 Python 这个名词，不一定是说 Python 编程语言，还可能是解释器，也可能是程序，这可能会有利于你理解 Python 这个事物背后可能存在的事物。\n\n> 2020 年后不再提及 Python2.7 的任何内容，今后描述的 Python 以 Python3 的语法为准。\n\n## 这一切都这么美好？\n\n并不是。\n\n在这么多编程语言中，Python 对于初学者来说是很上手且简单的，对于一些调用各种库的代码、不在意运行效率，甚至是代码的可维护性也可以忽略的场合，随手一写就可以完成任务。\n\n但代价就是想提升你瞎写程序的性能，你要付出更多的时间去做优化。\n\n> 写出文章不难，写好文章才难。\n\n因为最初不了解 Python 所写出的代码，就像小孩子的涂鸦，到处复制粘贴，以至于写出来的代码东拼西凑整合起来的。\n\n到了这时候，你会发现，一旦你想要改进它是很难做到的，从一开始就不了解它，又谈何改进。\n\n所以这里提及一下，传统的编程语言入门要经历的过程：\n\n- 学习计算机历史\n- 学习计算机语法\n- 学习编程范式\n- 学习使用开发工具\n- 学习使用调试代码\n- 学习程序设计\n\n而使用 Python 语言速成入门培养兴趣和获得成就感的代价就是以后从事这份职业会花费额外的时间补计算机的基础。\n\n> 至少 Python 能让你点火先把火箭飞起来，而不用把火箭制造原理研究透彻了再起飞。\n\n## 学哪个语言更好？\n\n至于哪个更好，这里无法做出评价，建议顺应时代潮流，先解决问题，其他的再说。\n\n- 黑猫白猫抓住佬鼠就是好猫。\n- 实践是检验真理的唯一标准。\n\n最终怎么选择，就取决于你自己了。"}, "/soft/maixpy3/zh/origin/video.html": {"title": "Python 教程", "content": "# Python 教程\n\n## 文档相关的\n\n廖雪峰官方网站的《python教程》 https://www.liaoxuefeng.com/\n\nPython3 教程 | 菜鸟教程 https://www.runoob.com/python3/python3-tutorial.html\n\n## 视频相关的\n\n> 如果你没有任何计算机语言基础的话，看一点视频也是极好的，但没必要看完，太多太长。\n\n<iframe src=\"//player.bilibili.com/player.html?aid=14184325&bvid=BV1ex411x7Em&cid=23153678&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n<iframe src=\"//player.bilibili.com/player.html?aid=27789609&bvid=BV1Fs411A7HZ&cid=64841219&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>"}, "/soft/maixpy3/zh/origin/difference.html": {"title": "MaixPy 与 MaixPy3 的区别", "content": "---\ntitle: MaixPy 与 MaixPy3 的区别\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: MaixPy3\n---\n\n## 区别是？\n\n因为使用 MaixPy 的同学可能有两类人群，一类是从 MicroPython 一路使用过来的，另一类是从 Python3 过来的，所以针对两边的差异，分别做一下说明。\n\n可以这样理解，它们都是专门为 AIoT 提供的 Python 开发环境，提供了各种各样的模块。\n\n- MaixPy 指的是基于 MicroPython 的环境制作的。\n\n- MaixPy3 指的是基于 Linux Python3 的环境制作的。\n\n> 前者是基于 MCU 无系统的，后者是基于 Linux 系统。\n\n除了基本的 Python3 语法一致，在提供的模块方面的存在着不小的差异。\n\n### Python3 与 MicroPython 的区别\n\n大多数时候，Python 的发展以 Python3 为主，以下列出一些与 Python3 的差异化信息。\n\n- MicroPython 和 Python3 在 Python 语法上保持高度的一致性，常用的标准语法命令都已经支持。\n\n- MicroPython 虽然只实现了 Python3 的标准库和容器库的一些部分，常见容器库有同类功能，但不同名的模块，但大多算法类的 Python 逻辑代码是可以拿来即用的。\n\n- MicroPython 兼容实现的 Python3 的异常机制、没有实现元类（metaclass）机制，独立的 GC 机制。\n\n- 在许当不同的硬件微芯片（最低在 nRF51）的移植上， MicroPython 代码接口缺乏一致性，呈现碎片化。\n\n- MicroPython 编译（mpy-corss）后得到的是 mpy ，而不是 Python3 的 pyc 文件。\n\n- MicroPython 在移植 Python3 代码时，经常缺少各种方法，所以要习惯寻找同类接口，而它们的使用方法除了看文档外就只能看源码。\n\n### 总结\n\n- MaixPy 相比 MaixPy3 功能要更简单（简陋）。\n- MaixPy 和 MaixPy3 的开发工具不同。\n- MaixPy 标准库（MicroPython）相比 MaixPy3 有一定的不足。\n- MaixPy 的外设驱动模块具体函数存在差异。\n- 不同的芯片执行效率有差异，MaixPy 和 MaixPy3 的有着不同的内存与性能消耗。\n\n> 如有更多欢迎补充。"}, "/soft/maixpy3/zh/origin/loop_python.html": {"title": "", "content": "> 本文是给有一点 Python 基础但还想进一步深入的同学，有经验的开发者建议跳过。\n\n## 前言\n\n上文讲述了如何认识开源项目和一些编程方法的介绍，这节主要来说说 Python 代码怎么写的一些演化过程和可以如何写的参考，在现在的 Sipeed 开源社区/社群里，有太多的新手不知道如何写好 Python 代码，尤其是嵌入式中的 Python 代码也是有不少的技巧和观念需要注意的，至少让这篇文章从循环开始说起。\n\n> 可以把本文当作一篇经验之谈，主要是探讨代码稳定性与性能，以及一些计算机知识的拓展。\n\n## 学会循环执行代码\n\n当写下第一行代码的时候，在电脑上的 Python 解释器运行效果是这样的。\n\n```python\nprint('Hello World')\n```\n\n![](./../usage/asserts/win_python.png)\n\n而嵌入式设备上的 python 是通过串口（serial）传出来。\n\n![](./../usage/asserts/maix_python.png)\n\n当写完了第一行 `Hello World` 的 `print` 函数，总不能一直复制、粘贴代码吧。\n\n```python\n\nprint('Hello World')\nprint('Hello World')\nprint('Hello World')\nprint('Hello World')\nprint('Hello World')\n\n```\n\n也不是只运行验证功能就好了吧，所以加上了循环（`while`）执行代码。\n\n```python\n\nwhile True:\n    print('Hello World')\n\n```\n\n如果想要稳定一些，最好还要为它加入异常机制，保证它不会因为 Python 代码的运行出错而停下来。\n\n```python\n\nwhile True:\n    try:\n        print('Hello World')\n    except Exception as e:\n        pass\n\n```\n\n## 循环代码中为什么需要异常机制\n\n是不是以为 print 这样的代码就不会出错？其实不然，其实程序越接近底层硬件越容易出错。\n\n从功能上说上文两者之间并没有什么区别，都是输出，但你会发现串口输出可能会出现下面几类情况。\n\n- 串口芯片损坏或线路断路、串口到芯片的通路损坏导致的串口没有数据输出。\n- 串口线路数据不稳定、串口协议（波特率、停止位）等配置错误导致的数据乱码。\n\n这就意味着你会遇到很多来自硬件上的问题，所以要注意到这些意外。\n\n那在软件代码上会发生什么有关于硬件上的意外呢？\n\n通常有无响应、无应答、未连接等不成功的错误，它们是来自 IO 的错误。\n\n- 当网络连接失败后需要超时重连，传输数据通道闲置时需要定时检查心跳数据包。\n- 当配置文件写入后通常会读出来确认真的写入了，也是为了防止出错，可能是存储介质出错，也可能是逻辑出错。\n- 当用户向输入框填了错误数据，不用写怎么判断和处理，不合法的数据抛出异常就行。\n\n因为这些现象太多不确定的可能性，才会需要对代码进行异常捕获机制，来决定是否放过这次意外，可能会在下一次的循环就恢复了，这样就能够基本保证了 Python 代码循环的稳定性了。\n\n## 来自外部/硬件上异常机制\n\n这样就足够了吗？\n\n事实上有些错误不源于 Python 代码，可能来自于底层 C 代码，或其他程序，上文说的异常机制只能捕获 Python 异常，不能捕获来自其他语言的异常。\n\n所以实际情况比想象的要更严峻一些，当你无法解决不稳定的系统带来其他异常的时候，通常在服务器程序上设计会在外部附加一个守护程序（如调试程序）来定时检查自己的程序，例如可以检查下面的一些情况。\n\n- 检查当前的系统是否能联网\n- 检查数据库的通路是否正常\n- 检查指定的程序是否在运行\n\n总得来说，你要为你的程序做一个监控程序，可以是守护程序，也可以是看门狗。\n\n> 具体怎么实现，可以了解一些守护进程的实现。\n\n## 看门狗（watchdog）是什么？\n\n如上述的守护程序是靠一个软件去监控另一个软件的状态，而看门狗的工作行为描述如下：\n\n假设有一条需要定时吃饭（更新）的狗、如果不定时喂它（feed）就会饿着肚子叫。那么问题来了，什么时候狗会叫呢？因为人（芯片）死了，没人喂它了。（这也许是一个冷笑话）\n\n看门狗是要求芯片程序负责定时喂狗，如果没有喂狗就狗就饿死了，作为报复狗会把芯片重启。让它可以继续喂狗。\n\n任何硬件产品都有可能出现意外和错误，看门狗相当于芯片上的最后一层保障机制，通常它可能会发生在函数栈的指针参数执行出错，导致后续的喂狗操作再也执行不到了，具体怎么实现，可以查阅不同芯片提供的程序接口或寄存器。\n\n## 优化！优化！！优化！！！\n\n当你的程序已经跑起来以后，你会发现程序并没有达到令人满意的效果，在性能、内存上都没有经过任何考虑，只是实现了最起码的功能而已，那么完成了功能以后，可以如何继续呢？\n\n当然，在优化程序之前得先建立计算代码执行时间的观念，建立起最简单的性能指标，如在代码加上时间计算。\n\n```python\ndef func():\n    i = 20**20000\n\nimport time\nlast = time.time()\nfunc()\ntmp = time.time() - last\nprint(tmp)\n```\n\n在 CPU I5-7300HQ 的计算机上见到每一次的循环的时间间隔约为 0.000997781753540039 不足 1ms 即可完成。\n\n```bash\nPS C:\\Users\\dls\\Documents\\GitHub\\MaixPy3> & C:/Users/dls/anaconda3/python.exe c:/Users/dls/Documents/GitHub/MaixPy3/test.py\n0.000997781753540039\n```\n\n注意不要写到 `print(time.time() - last)` ，因为重定向后的 print 是相当耗时的，尤其是当内容输出到串口终端或网页前端的时候，如下使用 M2dock 设备来演示一下串口输出。\n\n> 重定向指改变内容要输出的地方\n\n```bash\nroot@sipeed:/# python3\nPython 3.8.5 (default, Jan 17 2021, 06:07:56)\n[GCC 6.4.1] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> def func():\n...     i = 20**20000\n...\n>>> import time\n>>> last = time.time()\n>>> func()\n>>> tmp = time.time() - last\n>>> print(tmp)\n0.09001994132995605\n>>>\n>>>\n>>> def func():\n...     i = 20**20000\n...\n>>> import time\n>>> last = time.time()\n>>> func()\n>>> print(time.time() - last)\n1.480057954788208\n>>>\n```\n\n可以看到相差可能有 1 秒，而事实上只需要 90ms 就可以完成 func 函数的运算，这就产生了误差导致不准确，若是使用 jupyter 输出就会看到 0.026456356048583984 需要 26ms 可以较为准确的推算出它的真实运算结果。\n\n![](./asserts/time_python.png)\n\n为什么会造成这种差异的原因是因为串口依次输入命令输出结果需要时间，所以依次输入语句执行自然会存在误差，而 jupyter 是通过网络 socket 连接传输显示到屏幕上，所以耗时误差只会发生在运算重定向输出结果的时候，最终结果会较为贴近真实运算结果，通过保存下述代码文件来运行即可得知真实情况下约为 26 ~ 28ms 完成。\n\n```bash\nroot@sipeed:/# cat test.py\ndef func():\n    i = 20**20000\n\nimport time\nlast = time.time()\nfunc()\ntmp = time.time() - last\nprint(tmp)\n\nroot@sipeed:/# python test.py\n0.028677940368652344\nroot@sipeed:/#\n```\n\n所以从现在建立起最基础的计算耗时，并且认知到在计算机的世界里，毫秒其实已经很慢了，然后可以类比一种感受，人眼感到流畅的画面至少是 24 fps ，而平时的视频在 15 fps 的流动是不会让你感受到卡顿的，如果低于这个阈值，则会出现卡顿造成心理上的不愉快，这个 15 fps 意味着每秒 15 张存在变化的画面，如果用程序来类比就是 1000 ms / 15 = 66 ms ，也就是每个流程操作最好是在 66ms 内完成，这样用户才不会觉得卡顿，同理，当 1000ms / 24 = 41ms 就可以确保用户体验这个软件的时候会觉得流畅。\n\n有了基本的性能指标，就有了优化的对比参考，如果是一些测试框架会帮助你自动完成每个函数的耗时统计，但在没有现成框架工具的时候就要稍微辛苦一下自己了。\n\n### 讲一些经典案例\n\n在日常中存在最多操作就是循环和判断，显然好的优化就是减少不必要的指令操作，可以通过改变代码的执行结构来进行优化，下面就来具体分析吧。\n\n如某个向网络上发送数据的操作，最初可能会按人类直觉写出以下的代码，这是一种不用思考也可以很容易写出来的同步阻塞式的结构，每一条语句都是满足了某些条件再继续执行。\n\n```python\n\ndef xxxx_func():\n    import random\n    return random.randint(0, 1)\n\nwhile True:\n    is_idle = True\n    if is_idle is True:\n        print('try start')\n        is_ready = xxxx_func()\n        if is_ready is True:\n            print('try ready')\n            is_connected = xxxx_func()\n            if is_connected is True:\n                print('try connect')\n                is_send = xxxx_func()\n                if is_send is True:\n                    print('try send')\n                    is_reply = xxxx_func()\n                    if is_reply is True:\n                        print('wait reply')\n                        is_exit = xxxx_func()\n                        if is_exit is True:\n                            print('operate successfully')\n```\n\n而优化只需要加状态变量改写成状态机结构（fsm）就可以了，所有代码都可以平行化执行，并根据执行频率的重要程度（权重）调整各项判断的顺序，尤其是移除一些不必要的判断。\n\n```python\ndef xxxx_func():\n    return 1\n\n# state value\nis_idle, is_ready, is_connected, is_send, is_reply, is_exit = 0, 1, 2, 3, 4, 5\nstate = is_idle\n\nwhile state != is_exit:\n\n    if state is is_reply:\n        print('wait reply')\n        state = is_exit if xxxx_func() else is_send\n        continue\n\n    if state is is_send:\n        print('try send')\n        state = is_reply if xxxx_func() else is_connected\n        continue\n\n    if state is is_connected:\n        print('try connect')\n        state = is_send if xxxx_func() else is_ready\n        continue\n\n    if state is is_ready:\n        print('try ready')\n        state = is_connected if xxxx_func() else is_idle\n        continue\n\n    if state is is_idle:\n        print('try start')\n        state = is_ready\n        continue\n```\n\n这样改造执行结构后，每个代码之间的上下文关系并不强烈，是否执行某个语句取决于系统对于某个状态是否满足，如果状态失败也不会倒退回最初的判断，也就不需要每次都对各个状态做检查，检查只会发生在出错的时候状态跌落（state - 1）。\n\n缺点就是需要消耗一些记录状态的变量(●'◡'●)，不过代码的拓展性和维护性就上来了。\n\n> 可以根据实际情况增加状态的判断或是减少状态的转移（调整状态转移范围），如直接设置 state = is_ready，假设某些操作是已知的就可以跳过，可以添加 continue 跳过一些不可能发生的状态。\n\n### 还有吗？\n\n进一步优化还可以干掉 if 直接将状态与函数联合索引执行，简化代码如下。\n\n```python\n\nis_a, is_b, is_c = 0, 1, 2\n\nstate = is_a\n\ndef try_b():\n    global state\n    state = is_c\n\ndef try_a():\n    global state\n    state = is_b\n\nfunc = [try_a, try_b]\n\nwhile state != is_c:\n    func[state]()\n    # print(state)\n\n```\n\n基于上述结构给出一个示例代码参考。\n\n```python\n\nclass xxxx_fsm:\n\n    is_start, is_ready, is_connected, is_send, is_reply, is_exit = 0, 1, 2, 3, 4, 5\n\n    def xxxx_func(self):\n        return 1\n\n    def __init__(self):\n        self.func = [self.try_start, self.try_ready, self.try_connect, self.try_send, self.wait_reply]\n        self.state = __class__.is_start # state value\n\n    def wait_reply(self):\n        self.state = __class__.is_exit if self.xxxx_func() else __class__.is_send\n\n    def try_send(self):\n        self.state = __class__.is_reply if self.xxxx_func() else __class__.is_connected\n\n    def try_connect(self):\n        self.state = __class__.is_send if self.xxxx_func() else __class__.is_ready\n\n    def try_ready(self):\n        self.state = __class__.is_connected if self.xxxx_func() else __class__.is_start\n\n    def try_start(self):\n        self.state = __class__.is_ready\n\n    def event(self):\n        self.func[self.state]()\n\n    def check(self):\n        return self.state != __class__.is_exit\n\ntmp = xxxx_fsm()\n\nwhile tmp.check():\n\n    tmp.event()\n\n    # print(tmp.state)\n```\n\n其实上述的有限状态机并非万能的代码结构，只是刚好很适合拆分已知的复杂业务逻辑的同步阻塞代码，那么还有什么结构可以选择吗？有的，此前说的都是同步阻塞的代码，所以还有所谓的异步执行的代码。\n\n### 说说异步的执行方式\n\n在这之前的代码都是按每个循环的步骤有序执行完成功能（同步执行），但现实生活中的操作一定是按顺序发生的吗？其实不然，其实很多操作可能会在任意时刻发生。\n\n想象一个程序，它会响应来自网络的数据，也会响应来自人类的按键输入操作，这两个操作如果按上述的结构来写，可能会是下面这样。\n\n```python\nimport time, random\n\ndef check_http():\n    time.sleep(random.randint(0, 3))\n    return random.randint(0, 1)\n\ndef http_recv():\n    while True:\n        if check_http():\n            print('http_recv')\n            break\n\ndef check_key():\n    time.sleep(random.randint(0, 2))\n    return random.randint(0, 1)\n\ndef key_press():\n    while True:\n        if check_key():\n            print('key_press')\n            break\n\nwhile True:\n    http_recv()\n    key_press()\n```\n\n可以看到 http_recv 和 key_press 两个事件的检查会各自占据一段不知何时会触发或结束的检测的时间，程序只能循环等待这些事件会不会发生（或称轮询）。\n\n这是个看起来可以工作但浪费了很多时间的程序，现实里接收到许多用户的网络连接，而服务程序不可能只服务某个用户的连接。\n\n所以改写异步的第一步就是简化代码中不必要的循环，将每个需要循环等待的部分拆分成非阻塞的函数。\n\n> 非阻塞意味着某个操作会在有限的时间内结束，期望某个函数能够在较短的时间（10ms）内退出，退出不代表功能结束，只是需要把这个时间让出去给其他函数调用。\n\n```python\nimport time, random\n\nhttp_state, key_state = 0, 0\n\ndef http_recv():\n    global http_state\n    if http_state:\n        print('http_recv')\n\ndef key_press():\n    global key_state\n    if key_state:\n        print('key_press')\n\ndef check_state():\n    global key_state, http_state\n    time.sleep(random.randint(0, 1))\n    key_state, http_state = random.randint(0, 2), random.randint(0, 2)\n\nwhile True:\n    check_state()\n    http_recv()\n    key_press()\n```\n\n从逻辑上移除了等待，再通过统一的（check_state）检查每个操作的状态再决定是否唤醒该操作，变成只有满足某个状态才执行该操作，将此前的多个循环拆分出来。\n\n但你会发现这样写还是有问题，这样岂不是意味着所有代码都要按这个接口来写了吗？那么多的代码，不可能全都可以拆分吧。\n\n所以是时候加入异步 IO （asyncio）的 async 和 await 语法了！先来点简单的。\n\n```python\nimport asyncio\n\nasync def test_task(name, tm):\n    await asyncio.sleep(tm)\n    print('%s over...' % name)\n\nasync def main(name):\n    import time\n    last = time.time()\n    await asyncio.gather(\n        test_task(name + 'A', 0.1),\n        test_task(name + 'B', 0.2),\n        test_task(name + 'C', 0.3),\n    )\n    print(name, time.time() - last)\n\nloop = asyncio.get_event_loop()\ntasks = [ main('l: '), main('r: ') ]\nloop.run_until_complete(asyncio.wait(tasks))\n\n```\n\n运行结果如下：\n\n```bash\nPS python.exe test.py\nr: A over...\nl: A over...\nr: B over...\nl: B over...\nr: C over...\nl: C over...\nr:  0.3076450824737549\nl:  0.3076450824737549\n```\n\n可以看到代码总共耗时为 0.3s 完成，但运行了两次不同所属的 main 函数以及各自调用三次不同延时的 test_task 任务，而 await asyncio.sleep(tm) 延时期间实际上是被 asyncio 拿去运行其他的 async 函数了，基于此结构可以这样改写。\n\n```python\n\nimport asyncio, random\n\nasync def key_press():\n    await asyncio.sleep(0.1)\n    key_state = random.randint(0, 1)\n    if key_state:\n        return 'have key_press'\n\nasync def http_recv():\n    await asyncio.sleep(0.2)\n    http_state = random.randint(0, 1)\n    if http_state:\n        return 'have http_recv'\n\nasync def run():\n    import time\n    while True:\n        task_list = [http_recv(), key_press()]\n        done, pending = await asyncio.wait(task_list, timeout=random.randint(0, 1) / 2)\n        print(time.time(), [done_task.result() for done_task in done])\n        await asyncio.sleep(0.2) # remove to run too fast.\n\nloop = asyncio.get_event_loop()\nloop.run_until_complete(run())\n```\n\n执行效果如下。\n\n```bash\n1615141673.93252 [None, None]\n1615141674.134 [None, 'have http_recv']\n1615141674.3350334 [None, None]\n1615141674.7361133 ['have key_press', 'have http_recv']\n1615141674.9365196 [None, None]\n1615141675.1399093 ['have http_recv', None]\n```\n\n可以看到在运行 run 函数延时 `await asyncio.sleep(0.2)` 后就会循环加载异步事件函数执行，配置 asyncio.wait 函数的参数 `timeout` 会导致 `random.randint(0, 1) / 2` 秒后就会自行超时退出，退出的时候会收集当前的 `key_press` 和 `http_recv` 函数的运行结果，如果期间异步函数成功返回值（`return 'have http_recv'`），最终结果就会输出 `1615138982.9762554 ['have http_recv']` 表示有事件触发并执行了，否则为 None ，这将在下一次循环重新提交异步函数列表 `[http_recv(), key_press()]` 执行。\n\n> 注意 Python 3.7 以前的版本使用 loop = asyncio.get_event_loop() & loop.run_forever() & loop.run_until_complete() ，而后采用 asyncio.run() 了。每个编程语言都有自己的异步框架和语法特色，请根据实际情况选用。\n\n## 考虑一下封装模块给其他人使用吧？\n\n随着代码越写越多，项目越来越大，大到可能不是你一个人写的时候，你就要开始注意工程项目的管理了，这与个人写代码时的优化略微不同，主要强调的是不同代码之间的接口分离，尽量不干涉到他人的实现和提交，所以在写代码的时候，不妨为自己准备一个独立模块，以方便与其他人写的分离或是导入其他（import）模块。\n\n若是在某个目录（`mod`）下存在一个 `__init__.py` 的话，它就会变成 Python 模块，且名为 `mod` ，其中 `__init__.py` 的内容可能如下：\n\n```python\ndef code():\n    print('this is code')\n```\n\n而且在该目录下还存在一个额外的代码文件（如 `tmp.py` ）内容如下：\n\n```python\ninfo = 'nihao'\n```\n\n对于开发者或用户来说，在 `import mod` 的时候会调用 `mod` 目录下的 `__init__.py` ，而 `from mod import tmp` 会调用 `mod` 目录下的 `tmp.py` 代码。\n\n```python\n>>> import mod\n>>> mod\n<module 'mod' from 'C:\\\\mod\\\\__init__.py'>\n>>> mod.code()\nthis is code\n>>> from mod import tmp\n>>> tmp\n<module 'mod.tmp' from 'C:\\\\mod\\\\tmp.py'>\n>>> tmp.info\n'nihao'\n>>>\n```\n\n这样你写的代码就可以作为一个模块被其他人所使用了，注意 import 只会加载并执行一次，想要再次加载请使用 reload 函数。\n\n## 如何进行内存上的分析？\n\n这里就推荐 [memory_profiler](https://github.com/pythonprofilers/memory_profiler) 开源工具，快去体验吧。\n\n使用方法：`python -m memory_profiler example.py`\n\n```python\nfrom memory_profiler import profile\n\n@profile\ndef my_func():\n    a = [1] * (10 ** 6)\n    b = [2] * (2 * 10 ** 7)\n    del b\n    return a\n```\n\n运行结果：\n\n```bash\nLine #    Mem usage    Increment  Occurences   Line Contents\n============================================================\n     3   38.816 MiB   38.816 MiB           1   @profile\n     4                                         def my_func():\n     5   46.492 MiB    7.676 MiB           1       a = [1] * (10 ** 6)\n     6  199.117 MiB  152.625 MiB           1       b = [2] * (2 * 10 ** 7)\n     7   46.629 MiB -152.488 MiB           1       del b\n     8   46.629 MiB    0.000 MiB           1       return a\n```\n\n## 总结\n\n其实所谓的优化就是在程序上不断追求无延迟、零等待、鲁棒性、艺术品、最佳实践等指标。\n\n当完成了自己的某个作品，多少都会希望自己的作品是最好的，又或是越做越好的。熬夜辛苦写下的程序，用尽自己的脑力和各种逻辑思维来不断打磨它，尽可能的把它变成一件艺术品，然后为之自豪和兴奋，恨不得向它人炫耀自己的成果。\n\n但愿你不会在往后的一堆垃圾代码中失去了最初喜欢上编程的心情。\n\n## 附录：多线程？多进程？该不该使用？\n\n事实上多线程和多进程都是建立在操作系统之上的概念，由于操作系统中存在不同优先级的中断函数，其中优先级较高的函数栈会打断优先级低的函数栈执行，并且优先级高的操作结束就会轮到优先级低的操作，优先级高的操作通常都会被设计成尽快结束退出（哪怕是失败），不然用户程序就会像老爷爷一样缓慢运行了。\n\n多线程是由拥有内存空间进程（某个程序）创造出来的，多线程函数“看上去”是彼此并行的，并且共用所属进程的内存数据，而不同进程之间申请的内存空间并不互通，所以当你想要实现守护进程的程序，是需要对其他进程进行通信的（如卸载程序时会检查并发送信号停止要卸载的程序），并非是在代码中修改一个变量那么简单。\n\n事实上我并不鼓励用户在 Python 上使用多线程，因为全局解释器锁（GIL）的存在，CPython 解释器中执行的每一个 Python 线程，都会先锁住自己，以阻止别的线程执行。而 CPython 解释器会去轮询检查线程 GIL 的锁住情况，每隔一段时间，Python 解释器就会强制当前线程去释放 GIL，这样别的线程才能有执行的机会。总得来说 CPython 的实现决定了使用多线程并不会带来太大的性能提升，反而会带来更多线程安全的问题，尤其是需要线程资源同步了。\n\n> 警告：请不要在每个线程中都写上不会退出的死循环，多线程的并不是拿来偷懒的工具。"}, "/soft/maixpy3/zh/origin/xiaoxu.html": {"title": "喏呐の 【攻城狮成长记】", "content": "# 喏呐の 【攻城狮成长记】\n\n\n### 【攻城狮成长记】毕业设计之sipeed M2dock（v831）开箱\n\n<iframe src=\"//player.bilibili.com/player.html?aid=935420437&bvid=BV1QT4y1274g&cid=479478309&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n### 【攻城狮成长记】sipeed M2dock（v831）从零开始教程之烧录系统\n\n<iframe src=\"//player.bilibili.com/player.html?aid=723105111&bvid=BV1ZS4y1Z7mY&cid=482337635&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n\n### 【攻城狮成长记】sipeed M2dock（v831）从零开始教程之连接开发板&调用摄像头\n\n<iframe src=\"//player.bilibili.com/player.html?aid=678188418&bvid=BV1Wm4y1U7Kk&cid=483642122&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>"}, "/soft/maixpy3/zh/origin/hello_world.html": {"title": "Hello World", "content": "---\ntitle: Hello World\nkeywords: Hello World, MaixPy3, Python, Python3\ndesc: maixpy doc: Hello World\n---\n\n> 本文是给有一点 Python 基础但还想进一步深入的同学，有经验的开发者建议跳过。\n\n## 前言\n\n在写这篇案例系列的时候 [junhuanchen](https://github.com/junhuanchen) 期望能够引导用户如何成为专业的开发者，不是只会调用代码就好，所以在 MaixPy3 开源项目上期望为你带来值得学习和容易上手的开源项目，所以开篇会引导用户学习一些长期有利于编程工作上好的做法和观念，就先从最简单的认知项目开始吧。\n\n第一次接触需要编程的开源硬件项目，要做的第一件事就是先有一个好的开始，例如运行 Hello World 程序，意味着你必须能够先将这个事物跑起来才能够继续后续的内容，它可能是硬件、软件、工具等可编程的载体。\n\n> 但这里先不强调立刻开始运行程序，而是强调如何熟悉一个开源项目。\n\n要先找到它提供的开发文档（例如本文），先纵览全文，站在专业的角度来看，你需要先关注它提供了哪些资源，可以在哪里反馈你的问题，这样就有利于你后续开发过程中出现问题后，该如何迅速得到解决，避免自己之后在学习和开发过程中耽误时间。\n\n![](https://opensource.com/sites/default/files/styles/image-full-size/public/lead-images/OSDC_Resource_Main_Page.png)\n\n有哪些资源是值得关注的？\n\n- 学会搜索！！！！！\n- 找到它的开源项目（如：[github.com/sipeed](https://github.com/sipeed)），获取它所提供的一系列源码。\n- 找到它提供的用户手册、应用案例、数据手册等等一系列开发所需要的文档。\n- 找到它的开发、编译、烧录、量产等一系列配套工具链，为后续软件开发活动中做准备。\n- 找到它的公开交流的环境，如 bbs、github、twitter、facebook、qq、wechat 等社交平台。\n\n现在你可以放心的编程了，但你还需要遵守一些在开源软件上的规则，认知到开源协议的存在，不要随意地做出侵犯他人软件的行为，哪怕没有法律责任的问题。\n\n在开源软件的世界里，鼓励人们自由参与和贡献代码，而不是鼓励如何免费白嫖，自由不等于免费，免费不等于服务，将软件源码公开是为了让用户更好更具有针对性的提交和反馈项目中存在的问题，不是为了更好服务你，请不要以服务自己的产品为中心。\n\n请尊重所有在开源环境里工作的朋友们，尊重他们（或是未来的你）的劳动成果。\n\n最后在开源的世界里，学会技术，学会成长，学会参与项目，学会分享成果！\n\n## Hello World\n\n> 关于本机怎样安装运行 Python 的基础知识，建议从其他网站教程得知。\n\n说了这么多，不如先来运行一段 Python3 代码吧。\n\n```python\nprint(\"hello world\")\n```\n\n> 点击下方的 run 按钮即可运行，如果有条件就在本机运行测试。\n\n<div align=\"center\" >\n    <iframe src=\"https://tool.lu/coderunner/embed/aEj.html\" style=\"width:90%; height:320px;\" frameborder=\"0\" mozallowfullscreen webkitallowfullscreen allowfullscreen></iframe>\n</div>\n\n> 在线 Python 编程 [runoob-python](https://www.runoob.com/try/runcode.php?filename=HelloWorld&type=python3) [google-colab](https://colab.research.google.com) 备用地址。\n\n但这样的代码是不够的，稍微认真一点写。\n\n```python\n# encoding: utf-8\n\ndef unit_test():\n    '''\n    this is unit_test\n    '''\n    print(\"hello world\")\n    raise Exception('unit_test')\n\nif __name__ == \"__main__\":\n    try:\n        unit_test()\n    except Exception as e:\n        import sys, traceback\n        exc_type, exc_value, exc_obj = sys.exc_info()\n        traceback.print_tb(exc_obj)\n        print('have a error:', e)\n\n```\n\n运行结果：\n\n```bash\nPS C:\\Users\\dls\\Documents\\GitHub\\MaixPy3> & C:/Users/dls/anaconda3/python.exe c:/Users/dls/Documents/GitHub/MaixPy3/test.py\nhello world\n  File \"c:/Users/dls/Documents/GitHub/MaixPy3/test.py\", line 12, in <module>\n    unit_test()\n  File \"c:/Users/dls/Documents/GitHub/MaixPy3/test.py\", line 8, in unit_test\n    raise Exception('unit_test')\nhave a error: unit_test\n```\n\n代码瞬间就变得复杂了起来？其实不然，这么写必然有它的用意，那这么写都考虑到了哪些情况呢？\n\n### 注意字符编码和代码缩进格式\n\n初学者经常会出现缩进不对齐的语法问题，代码的语法出现问题过于基础就不详谈，检查代码的小技巧就是 `CTAL + A` 全选代码，按 TAB 键右缩进，再配合 SHIFT + TAB 左缩进来发现哪段代码存在问题。\n\n首行的 `# encoding: utf-8` 是为了避免在代码中存在中文或其他语言的字符编码导致的运行出错的问题。\n\n> 在 python3 的字符串类型中 str 与 bytes 是一对欢喜冤家，例如 print(b'123') 打印出来的是 b'123' ，而实际上就是 '123' 的 bytes 字符串，前缀 b 只是为了和 str 区分，因为用途不同，在不同的接口对数据类型的需求不对，例如传递 str 字符串时候是不允许输入 '\\xFF' (0xFF) 字符的（会在转换过程中丢失），但 bytes 可以存储和表达。\n\n### 给代码加入单元测试和异常捕获\n\n想要写出一套稳定可用的代码，需要围绕接口可重入可测试的设计来编写封装，任何人写的代码都可能存在缺陷，在不能确定是哪里产生的问题之前，要能够恢复现场也要能够定位具体位置，以求问题能够最快得到反馈。\n\n所以在代码功能还没写之前，先把测试和异常的模板写好，再开始写功能，边写边测，确保最终交付的软件代码就算出问题也可以随时被测试（定位）出来。\n\n```python\n\ndef unit_test():\n    '''\n    this is unit_test\n    '''\n    print(\"hello world\")\n\nif __name__ == \"__main__\":\n    unit_test()\n```\n\n这样的代码可以保证任何人在任何时候运行该代码的时候都可以复现当时写下的场合所做的内容，然后 `if __name__ == \"__main__\":` 意味着该代码被其他模块包含的时候，不会在 import 该 Python 模块（可取名成 `hello` ）模块时调用，而是根据自己的代码需要执行相应的单元测试进行测试。\n\n```python\nimport hello\nhello.unit_test() # print(\"hello world\")\n```\n\n接着加入异常机制（try: except Exception as e:）保护代码段，表示该段代码出错的时候，能够不停下代码继续运行，像硬件资源访问的代码常常会发生超时、找不到、无响应的错误状态，这种情况下，一个跑起来的系统程序通常不需要停下来，出错了也可以继续运行下一件事，然后把当时的错误记录下来，通过 print 或 logging 日志模块记录下来，拿着错误结果（日志）反馈给开发者，这样开发者就可以分析、定位和解决问题，这其中也包括你自己。\n\n```python\ntry:\n    raise Exception('unit_test')\nexcept Exception as e:\n    import sys, traceback\n    exc_type, exc_value, exc_obj = sys.exc_info()\n    traceback.print_tb(exc_obj)\n    print('have a error:', e)\n```\n\n单元测试是每个程序都尽可能保持的基本原则，虽然人会偷懒，但最起码的代码格式还是要有的。\n\n> 注：traceback 可以抓取最后一次运行出现的错误而不停止运行，但该模块不存在 MicroPython(MaixPy) 中，它有类似的替代方法。\n\n## 封装代码接口成通用模块的方法\n\n世上本没有路，走的人多了，也便成了路。\n\n这里说的路实际上就是一种封装和参考，它意味着你写的代码成为一种事实上的通用操作。\n\n在 Python 上有很多封装参考，主要是为了形成抽象的函数模块。\n\n所以出现了一些经典的编程思想，如面向过程、面向对象、面向切面、面向函数等编程方法，哪一种更好就不比较和讨论了。\n\n这里就简单叙述一下这些编程方法的逐渐发展与变化的过程，可以如何做出选择。\n\n### 面向过程\n\n用面向过程的思维写代码，强调的是这份代码做的这件事需要分几步完成，例如最开始写代码都是这样的。\n\n```python\none = 1\ntwo = 2\nthree = one + two\nprint(three)\n```\n\n这是用人类直觉的过程来写代码，后来意识到可以这样写成通用功能，这是最初的代码封装成某个函数。\n\n```python\ndef sum(num1, num2):\n    return num1 + num2\none, two = 1, 2\nprint(sum(one, two)) # 1 + 2 = 3\n```\n\n于是你多写了个类似的乘法操作。\n\n```python\ndef mul(num1, num2):\n    return num1 * num2\none, two = 1, 2\nprint(mul(one, two)) # 1 * 2 = 2\n```\n\n这时的代码是按照每一个代码操作流程来描述功能的。\n\n### 面向对象\n\n面向对象是相对于面向过程来讲的，把相关的数据和方法组织为一个整体来看待，从更高的层次来进行系统建模，更贴近事物的自然运行模式，一切事物皆对象，通过面向对象的方式，将现实世界的事物抽象成对象，现实世界中的关系抽象成类、[继承](https://baike.baidu.com/item/继承)，帮助人们实现对现实世界的[抽象](https://baike.baidu.com/item/抽象)与数字建模。\n\n在看了一些面向对象的描述后，你会意识到上节面向过程的函数操作可能很通用，应该不只适用于一种变量类型，所以可以通过面向对象（class）的方法来封装它，于是可以试着这样写。\n\n```python\nclass object:\n    def sum(self, a, b):\n        return a + b\n    def mul(self, a, b):\n        return a * b\nobj = object()\nprint(obj.sum(1, 2)) # 1 + 2 = 3\nprint(obj.mul(1, 2)) # 1 * 2 = 2\n```\n\n这样会意识到似乎还不只是数字能用，感觉字符串也能用。\n\n```python\nclass object:\n    def sum(self, a, b):\n        return a + b\n    def mul(self, a, b):\n        return a * b\nobj = object()\nprint(obj.sum('1', '2')) # 1 + 2 = 3\nprint(obj.mul('1', '2')) # 1 * 2 = 2\n```\n\n但这么写会出问题的，字符串相加的时候可以，但相乘的时候会报错误，因为是字符串这个类型的变量是不能相乘的。\n\n```bash\n12\nTraceback (most recent call last):\n  File \"c:/Users/dls/Documents/GitHub/MaixPy3/test.py\", line 8, in <module>\n    print(obj.mul('1', '2')) # 1 * 2 = 2\n  File \"c:/Users/dls/Documents/GitHub/MaixPy3/test.py\", line 5, in mul\n    return a * b\nTypeError: can't multiply sequence by non-int of type 'str'\n```\n\n显然这样写代码就不合理了，但这时运用的面向对象的思想是可行的，只是实现的方式不够好而已，所以重新设计类结构，例如可以写成下面的类结构。\n\n```python\nclass obj:\n    def __init__(self, value):\n        self.value = value\n    def __add__(self, obj):\n        return self.value + obj\n    def __mul__(self, obj):\n        return self.value * obj\n\nprint(obj(1) + 2) # 3\nprint(obj(1) * 2) # 2\n```\n\n其中 `__add__` 和 `__mul__` 是可重载运算符函数，意味着这个类实例化的对象在做 + 和 * 运算操作的时候，会调用类（class）重载函数，接着可以提升可以运算的对象类型，进一步继承对象拓展功能（`class number(obj):`）和访问超类的函数（`super().__add__(obj)`），其中 `if type(obj) is __class__:` 用于判断传入的参数对象是否可以进一步处理。\n\n```python\n\nclass number(obj):\n    def __add__(self, obj):\n        if type(obj) is __class__:\n            return self.value + obj.value\n        return super().__add__(obj)\n    def __mul__(self, obj):\n        if type(obj) is __class__:\n            return self.value * obj.value\n        return super().__mul__(obj)\n\nprint(number(1) + 2)\nprint(number(1) * 2)\nprint(number(1) + number(2))\nprint(number(1) * number(2))\n\n```\n\n这时候会发现可以进一步改写成字符串数值运算。\n\n```python\n\nclass str_number(obj):\n    def __init__(self, value):\n        self.value = int(value)\n    def __add__(self, obj):\n        if type(obj) is __class__:\n            return str(self.value + int(obj.value))\n        return str(super().__add__(int(obj)))\n    def __mul__(self, obj):\n        if type(obj) is __class__:\n            return str(self.value * int(obj.value))\n        return str(super().__mul__(int(obj)))\n\nprint(str_number('1') + '2')\nprint(str_number('1') * '2')\nprint(str_number('1') + str_number('2'))\nprint(str_number('1') * str_number('2'))\n```\n\n现在就可以解决了最初的同类操作适用不同的数据类型，把最初的一段操作通用到数值和字符串了，可以受此启发，它不仅仅只是加法或乘法，还有可能是其他操作，关于面向对象的内容就说到这里，感兴趣的可以查阅相关资料深入学习，本节只讲述可以怎样使用面向对象的思维写代码，而不是单纯把 Class 当 Struct 来使用。\n\n> 像最初写的代码，如果不通过对象继承分解函数，最终将会形成一个巨大的 Struct 结构。\n\n### 面向切面\n\n现在到了选择更多编程思维方式了，关于面向切面编程方法的场景是这样提出的，有一些函数，它在产品调试的时候会需要，但在产品上线的时候是不需要的，那这样的函数应该如何实现比较好？接下来不妨直接看代码，以日志输出的代码为例来说说面向切面，介绍一下如何使用装饰器进行编程的方法。\n\n```python\n\ndef log(param):\n    # simple\n    if callable(param):\n        def wrapper(*args, **kw):\n            print('%s function()' % (param.__name__,))\n            param(*args, **kw)\n        return wrapper\n    # complex\n    def decorator(func):\n        import functools\n        @functools.wraps(func)\n        def wrapper(*args, **kw):\n            print('%s %s():' % (param, func.__name__))\n            return func(*args, **kw)\n        return wrapper\n    return decorator\n\ndef now():\n    print(\"2019\")\n\n@log\ndef now1():\n    print(\"2020\")\n\n@log(\"Is this year?\")\ndef now2():\n    print(\"2021\")\n\nnow()\nnow1()\nnow2()\n\n```\n\n运行结果：\n\n```bash\nPS C:\\Users\\dls\\Documents\\GitHub\\MaixPy3> & C:/Users/dls/anaconda3/python.exe c:/Users/dls/Documents/GitHub/MaixPy3/test.py\n2019\nnow1 function()\n2020\nIs this year? now2():\n2021\nPS C:\\Users\\dls\\Documents\\GitHub\\MaixPy3>\n```\n\n对于产品上线时不需要的函数，注释掉就可以了，更进一步还可以重新设计某些函数满足于某些条件后再运行。\n\n- 在执行某段操作前，先打印当前的系统状态记录下来，确保出错时可以追溯到出错的地方。\n- 在发送网络数据前，要先检查网络通路是否存在，网卡是否还在工作。\n- 在运行操作前，先检查内存够不够，是否需要释放内存再继续操作。\n\n可以看到，当想要不改变某些现成库代码的条件下拓展系统的功能，就不免需要面向切面的设计方法。\n\n>  注意！面向切面提出的是编程思想，实现的方法不一定是装饰函数，可以是回调函数，也可以是重载函数。\n\n### 面向函数\n\n关于面向函数的场景是由于有些问题是被数学公式提出的，所以对于一些数学问题，并不一定要按过程化的思维来写，如实现阶乘函数（factorial），它的功能就是返回一个数的阶乘，即`1*2*3*...*`该数。\n\n```python\ndef fact(n):\n    if n == 3:\n        return 3*2*1\n    if n == 2:\n        return 2*1\n    if n == 1:\n        return 1\nprint(fact(3))\nprint(fact(2))\nprint(fact(1))\n```\n\n不难看出用最初的面向过程来写是写不下去的，不可能去定义所有的可能性，所以要找出规律，可以通过递归的方式实现。\n\n```python\ndef fact(n):\n    return 1 if n == 1 else n * fact(n - 1)\nprint(fact(1))\nprint(fact(5))\nprint(fact(100))\n```\n\n这样功能就完整了，简单来说函数式编程是让编程思维追求程序中存在的公式。\n\n## 试试快速迭代的敏捷开发？\n\n现代开源软件在经历了产测、内测、公测等环节后，直至更新到用户的手里，从前到后的过程通常在一周内就可以完成，所以在设计程序接口的时候，可以接受当下接口设计的不完美，等到未来有一个更好的替代功能接口的时候，就可以将其迭代替换下来，这意味着可以不用设计好整体的软件系统再开始工作，而是边做边改进，这套理论适用于初期需要频繁更新业务逻辑的开源软件。\n\n这里简单引用一段小故事来说明这个现象。\n\n快速迭代，不是说一定要产品做好了，才能上线，半成品也能上线。\n\n在没有上线之前，你怎么知道哪好那不好。所以半成品也是可以出门的，一定不要吝惜在家，丑媳妇才需要尽早见公婆。尽早的让用户去评判你的想法，你的设计是否可以赢得用户的喜爱。快速发出，紧盯用户反馈。百度完成了第一版的搜索引擎，也是让用户去做的选择。用百度 CEO 李彦宏（Robin）的话来说“你怎么知道如何把这个产品设计成最好的呢？只有让用户尽快去用它。既然大家对这版产品有信心，在基本的产品功能上我们有竞争优势，就应该抓住时机尽快将产品推向市场，真正完善它的人将是用户。他们会告诉你喜欢哪里不喜欢哪里，知道了他们的想法，我们就迅速改，改了一百次之后，肯定就是一个非常好的产品了。”\n\n## 准备一个好的开始\n\n看到这里的你，可能会困惑，可能会看不懂，会觉得很复杂，这是认知上的偏差，实际上本文所讲述的都是编程思想上的基础，如果想专业起来，不认真是不行的。\n\n不妨自己动手试试看吧。"}, "/soft/maixpy3/zh/origin/helper.html": {"title": "", "content": "## 如何正确反馈问题并得到解决的方法\n\n正如标题所述，新来开源社区的同学们不知道如何正确提问和获取问题的解，所以这里特别强调一下，使用开源硬件产品如何得到问题的解决方案。\n\n要学会站在开发者角度来解决问题，把自己当作开发者来汇报问题。\n\n开发者在面对问题的时候，需要的信息主要为如下:\n\n- 什么条件下，什么场景下，触发了什么问题\n\n- 你做了什么，遇到了什么，你想要做什么？\n\n- 要有图，有日志，有前因后果，整理成一个帖子发到社区里，然后@管理员解决。\n\n嫌字少看这个 https://bbs.sipeed.com/thread/183\n\n开源社区是大家一起构建的，而不是由少数几个人完成的任务，所以一定要怀有感恩之心，尤其是国内这种氛围下，一个好的可以技术交流和提问的社区少之又少，珍惜这些愿意浪费自己生命，而不去搞业绩赚钱的开发者吧。"}, "/soft/maixpy3/zh/update_history.html": {"title": "MaixPy3更新历史", "content": "# MaixPy3更新历史\n\n\n## 2021/8/5\n增加了 MaixII Dock 在 Windows 下的dd烧录方法。\n\n## 2021/8/2\n更新了 MaixII Dock 的烧录方式，不再使用 PhoenixSuit 进行烧录。"}, "/soft/maixpy3/zh/guide/project.html": {"title": "项目使用说明", "content": "---\ntitle: 项目使用说明\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 项目使用说明\n---\n\n## 认识 MaixPy3 项目\n\n请点此进入 [MaixPy3](https://github.com/sipeed/maixpy3) 的开源项目仓库。\n\n## 如何安装使用？\n\nMaixPy3 是通过 pip 安装得到的，适用于 Python3 环境，请查阅左侧目录的 [如何安装](../install/index.html) 。\n\n> MaixPy3 在安装的时候可能会依赖一些需要编译才能安装的 Python 包，这需要通过镜像内置依赖包来解决这个问题。\n\n## 支持哪些开发工具？\n\n推荐使用 jupyter 或 VSCode 进行开发，使用串口终端也可以直接进行粘贴运行。\n\n请查阅左侧目录的 [常用的开发工具](../tools/mobaxterm.html) 进行安装与配置。\n\n## 在哪获取示例代码呢？\n\n将会使用 jupyter 文档提供所见即所得的结果。\n\n请查阅左侧目录的 [一些使用案例](../usage/00_hello_world.html) 完成基本的使用。\n\n## 关于内置模块的说明？\n\n对于一些底层实现的模块 API 说明，会统一放到【内置模块说明】当中。\n\n## 其他？\n\n更多的就自己探索吧。"}, "/soft/maixpy3/zh/guide/participate.html": {"title": "如何参与项目", "content": "---\ntitle: 如何参与项目\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 如何参与项目\n---\n\n如果你对开源社区文化很感兴趣，也想一同加入其中学习和分享，那了解 MaixPy3 开源项目可能是一个好的开始。\n\n## 什么是开源文化\n\n> 下文内容选自 [什么是开源文化？](https://www.cnblogs.com/hdwl/archive/2013/04/23/3037549.html)\n\n所谓“开源”，就是开放资源（Open Source）的意思。不过在程序界更多人习惯把它理解为“开放源代码”的意思。\n\n在计算机发展的早期阶段，软件几乎都是开放的，任何人使用软件的同时都可以查看软件的源代码，或者根据自己的需要去修改它。在程序员的社团中大家互相分享软件，共同提高知识水平。这种自由的风气给大家带来了欢乐，也带来了进步。\n\n“自由软件”的英文是“Free Software”，这不免为许多人所误解，人们经常把它和“免费软件”连系在一起，免费的恰恰都是些低级的，这使得自由软件蒙上了一层阴影。于是大家决定给自由软件一个更易理解的别名“开源软件”。开源一词则由此得来。 \n\n所以，开源即是自由的化身。它讲述了一种公开的、自由的精神。软件开源的发展历程 ，为我们软件行业以及非软件行业的人都带来的巨大的参考价值。\n## 可以如何参与开源项目？\n\n以 MaixPy3 为例，如果你对此项目感兴趣，想参与到其中，除了一般的开发活动，您的关注（给个 Star ）已经是最好的开源项目参与方式了，此外参与项目的方法还可以是通过提意见、提 bug 、反馈问题、修改文档说明、修改源代码并提交等等方式，值得注意的是，你要尊重这些开源项目所选取的开源协议。\n\nMaixPy3 开源项目采用 MIT 许可证。\n\n- 您可以使用复制和修改软件。\n- 您可以免费使用软件或出售。\n- 唯一的限制是，它是必须附有MIT授权协议。\n\n您可以在 [MaixPy3](https://github.com/sipeed/MaixPy3) 上公开获取它的项目源码，可以任意修改或提交，也可以学习它是如何实现的或是软件设计。\n\n您也可以在其他开源社区里公开的讨论和交流这个事物，开源总是期望用户能够提出自己的想法和宝贵的建议。\n\n若是有经验的开发人员，还可以阅读【MaixPy3开发文档】尝试提交或适配不同平台的软件。\n\n## 附录：GNU/Linux 和开源文化的背后\n\n> 下文内容选自 [GNU/Linux与开源文化的那些人和事](https://linux.cn/article-6270-1.html)\n\nGNU/Linux 来了，虽然没成为大多数人电脑里的系统，但每个人都离不开它。诸多互联网公司的服务器里都跑着 GNU/Linux ，名单不完全确认，Google、Facebook、淘宝、百度、腾讯、小说阅读网等等。\n\n大多人使用的安卓手机的系统也是基于 Linux 内核。\n\nGNU/Linux 的繁荣有宏伟的规划，有个人的努力，也有很多无形的力量在起着作用。\n\n没有 Unix 就没有 GNU/Linux ，是 Unix 给予了伟大的启示。\n\n没有 C 语言就有没有 GNU/Linux ， C 语言简单，优雅，介于高级语言和低级语言之间，开发系统软件的首选编程语言。\n\n没有一系列 Unix 标准的制定就没有 GNU/Linux 的繁荣。标准就是“车同轨、书同文、统一度量衡”，秦始皇可算最早的标准制订者。遵循 Posix 标准为 GNU/Linux 发展铺平了道路。\n\n没有互联网就没有 GNU/Linux ， GNU/Linux 不是一个人在开发，是全球无数人协作的结果。如果没有互联网实在不可想象。当然， GNU/Linux 也反哺了互联网，无数互联网公司采用GNU/Linux搭建服务器， GNU/Linux 也促进了互联网的繁荣。\n\n还有 GNU/Linux 诞生之前就开发开源项目的人和组织。Larry Wall因为懒，整天被报表搞得焦头烂额，发布了 Perl 语言；高德纳教授因为对排版工人不满意，十年时间停止重要工作，发明了伟大的排版软件 TeX ； Guido 为了打发圣诞节的无聊，编写了新的脚本语言 Python ……\n\n版本管理软件对 GNU/Linux 的发展和开源文化运动也功不可没。 Linux 内核开发起初使用的是商用版本管理软件，某天 Linus Torvalds 先生不开心了，就自己动手搞了一个。他好取自嘲调侃的名字，命名为 Git，意为没什么用的东西。\n\nGit 可不是没什么用，太好用了，Linux 内核源码从此都用 Git 管理。Git 免费、开源， Git 成就了一家伟大的网站 github.com ，伟大的源码项目托管网站。很多开源项目纷纷把源码托管到了 Github 上。"}, "/soft/maixpy3/zh/guide/feedback.html": {"title": "如何反馈问题", "content": "---\ntitle: 如何反馈问题\nkeywords: MaixPy, MaixPy3, Python, Python3, MicroPython\ndesc: maixpy doc: 如何反馈问题\n---\n\n在没开始使用 MaixPy3 开源项目之前，建议先看一下如何反馈问题，免得之后使用过程中出了问题，不知道在哪反馈，也不知道在哪获取解决方法。在开源项目上反馈的问题通常需要一些时间才能得到解决。\n\n## 找到反馈的地方\n\n按问题的严重性和客户的重要性依次排列反馈的地方，注意商业支持邮件不对个人用户开放，勿扰。\n\n- MaixPy 开源群组\n  - QQ 交流群 256336487 / 1129095405 / 756313869 等\n  - [telgram-sipeed](https://t.me/sipeed)\n- 开源社区\\论坛\\官博等\n  - [bbs.sipeed.com](https://bbs.sipeed.com)\n  - [twitter-sipeedio](https://twitter.com/sipeedio)\n- MaixPy3 开源项目的 issue 区\n  - [github.com/sipeed/MaixPy3/issues](https://github.com/sipeed/MaixPy3/issues)\n- 商业支持邮件\n  - 从 [sipeed.com](https://sipeed.com) 底部获取。\n\n如果是交流群反馈可能会被聊天消息刷下去，建议发到社区或 issue 保证问题可以被记录和得到解决，因为在聊天群组反馈的问题可能不会被解决。\n\n## 正确的反馈问题\n\n在遇到问题之前，请浏览一下（项目、社区）是否有相关的问题反馈记录，再把你遇到的问题，操作，环境，现象都写下来。\n\n- 遇到了什么问题？\n- 你做了什么操作？要有截图或代码！\n- 你在什么系统环境下进行的操作？\n- 期望的现象与实际的现象有什么不符？（有图有真相！）\n\n## 尊重开源开发者\n\n请反馈问题的小白用户们尊重每一位参与开源的开发者，参与其中并没有为此获取利益，仅仅是出于对技术的热爱和分享，以及展现自己技术实力。\n\n> *不要任性，不要耍脾气，更不要玻璃心，要认真的虚心学习！！！！*"}, "/soft/maixpy3/zh/api/maix/v831_wifi.html": {"title": "MaixII M2dock wifi 调试", "content": "---\ntitle: MaixII M2dock wifi 调试\nkeywords: MaixII, MaixPy3, Python, Python3, M2dock\ndesc: maixpy doc: MaixII M2dock wifi 调试\n---\n\n\n## V831 WIFI 调试\n\n在 /etc/wpa_supplicant.conf 中新增用户 WIFI \n\n```text\nctrl_interface=/tmp/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"Sipeed\"\n    psk=\"123456789\"\n}\n# 自己可以配置多个 wifi\nnetwork={\n    ssid=\"Geek-mi\"\n    psk=\"Geek.99110099\"\n}\n```\n\n重启系统（重新上电）之后板子就能自动连接 WIFI\n\n***\n\n## 调试使用\n\n开启 WIFI 网络相关工具包的编译\n\n开启 WIFI， 连接网络过程\n\n1. 挂载网卡\n\n```text\ninsmod /lib/modules/4.9.118/8189fs.ko\n```\n\n2. 开启网口 wlan0\n\n```text\nifconfig wlan0 up\n```\n\n3. 添加/修改网络配置文件\n\n```text\nvi /etc/wpa_supplicant.conf\n```\n\n\n\n在 /etc/wpa_supplicant.conf 中新增内容(该步骤可省略)\n\n```text\nctrl_interface=/tmp/wpa_supplicant\nupdate_config=1\n\nnetwork={\n    ssid=\"Sipeed\"\n    psk=\"1234567890\"\n}\n# 自己可以配置多个 wifi\nnetwork={\n    ssid=\"Geek-mi\"\n    psk=\"Geek.99110099\"\n}\n```\n\n\n\n4. 启用配置文件，连接网络\n\n```text\nwpa_supplicant -B -i wlan0 -c /etc/wpa_supplicant.conf\n```\n\n\n\n5. 启用 DHCP 分配 IP\n\n> 注意：需要先安装好天线\n\n```text\nudhcpc -i wlan0\n```\n\n6. 测试 ping\n\n```text\nping www.baidu.com\n```\n\n### 配置 WIFI 自动连接\n\n\n\n在用户自定义路径下新建文件内容如下：\n\n> 文件路径: ` /root/develop/wifi_connect.sh`\n\n```text\nmkdir -p /root/develop/ # 创建路径\nvim /root/develop/wifi_connect.sh # 创建 sh 文件\nchmod +x /root/develop/wifi_connect.sh # 修改脚本权限\n\n```\n\n\n\n```text\ninsmod /lib/modules/4.9.118/8189fs.ko\nsleep 1s\n\nifconfig wlan0 up\nsleep 1s\n\nwpa_supplicant -B -i wlan0 -c /etc/wpa_supplicant.conf\nsleep 3s\n\nkillall udhcpc\nsleep 1s\n\nudhcpc -i wlan0\n```\n\n\n\n```text\necho -e  \"sh /root/develop/wifi_connect.sh\" >> /etc/init.d/rcS\n```\n\n```text\n# 1. 挂载网卡驱动\ninsmod /lib/modules/4.9.118/8189fs.ko\nsleep 1s\n# 2. 开启网口 wlan0\nifconfig wlan0 up\nsleep 1s\n# 3. 启用配置文件，连接网络\nwpa_supplicant -B -i wlan0 -c /etc/wpa_supplicant.conf\nsleep 3s\n# 4. 杀死以前的dhcp进程\nkillall udhcpc\nsleep 1s\n# 5. 启用 DHCP 分配 IP\nudhcpc -i wlan0\n```\n\n\n\n```text\n# 关闭有线连接\nifconfig eth0 down\n# 打开无线连接\nifconfig wlan0 up\n# 杀死以前配置进程\nkillall wpa_supplicant\n# 启动wifi配置，使文件生效\nwpa_supplicant -B -Dwext -iwlan0 -c/etc/wpa_supplicant.conf\n# 启动有点慢，等一下启动完毕\nsleep 3s\n# 杀死以前的dhcp进程\nkillall udhcpc\n# 启动dhcp获取ip\nudhcpc -b -i wlan0\n# static ip\n# ifconfig wlan0 192.168.134.250 netmask 255.255.255.0\n# route add default gw 192.168.134.1\n\n```\n\n\n\n## WIFI 带宽/延迟测试\n\n使用 iperf3 测试网络带宽\n\niperf3,默认端口: 5210\n\n\n\n服务端（这里使用 PC）：\n\n```text\niperf3 -s\n```\n\n客户端（这里使用 V831）：\n\n```text\niperf3 -c [serve ip] -p [port]\n```\n\n测试项目：\n\n- WIFI 吞吐量（带宽测试）\n\n- WIFI 丢包/时延测试"}, "/soft/maixpy3/zh/api/maix/display.html": {"title": "MaixPy3 display 模块", "content": "---\ntitle: MaixPy3 display 模块\nkeywords: MaixPy3，屏幕, display API\ndesc: MaixPy3 display 模块 API 文档, 以及使用说明\n---\n\n>! API 仍处于非完全稳定状态, 可能在未来会有小幅改动。\n\n## 导入模块\n\n```python\nfrom maix import display\n```\n\n### display.width & display.height\n\n返回当前屏幕配置的图像的宽和高。\n\n```python\n>>> print(display.width, display.height)\n<function width at 0x7f54a80f9160> <function height at 0x7f54a80f91f0>\n>>> print(display.width(), display.height())\n240 240\n>>>\n```\n\n### display.config\n\n配置屏幕，如大小、类型，在桌面环境上  。\n\n```python\n>>> display.config((416, 416))\n>>> print(display.width(), display.height())\n416 416\n>>>\n```\n\n> 官方的板子会自动从设备树或 fb 里取大小配置，若是桌面系统可以通过该参数配置成更大的窗口。\n\n### display.as_image\n\n可以把 display 作为 _maix_image.image 使用。\n\n```python\n>>> print(display.as_image())\n<_maix_image.Image 0x2033160 \" width\":240, \"height\":240, \"type\"=RGB, \"size\":172800>\n```\n\n### display.show()\n\n根据传入的 image 对象来自适应显示图像内容。\n\n```python\nfrom maix import display\ndisplay.show(display.as_image())\n```\n\n如果处于 IDE 模式下会自动进行图传到 jupyter rpyc 核心上。"}, "/soft/maixpy3/zh/api/maix/pwm.html": {"title": "MaixII M2dock PWM 调试", "content": "---\ntitle: MaixII M2dock PWM 调试\nkeywords: MaixII, MaixPy3, Python, Python3, M2dock\ndesc: maixpy doc: MaixII M2dock PWM 调试\n---\n\n## PWM介绍\n\nPWM（Pulse Width Modulation）控制——脉冲宽度调制技术，通过对一系列脉冲的宽度进行调制，来等效地获得所需要波形（含形状和幅值）.PWM控制技术在逆变电路中应用最广，应用的逆变电路绝大部分是PWM型，PWM控制技术正是有赖于在逆变电路中的应用，才确定了它在电力电子技术中的重要地位。V831的PWM功能是由硬件产生的,所以我们只用配置好硬件寄存器即可.芯片寄存器请参考[V833／V831 Datasheet V1.0.pdf](https://linux-sunxi.org/images/b/b9/V833%EF%BC%8FV831_Datasheet_V1.0.pdf).\n\n\n\n## V831 Dock PWM 引脚\n\n如下图所示,V831有8个引脚可以输出PWM波,但是由于复用关系,我们并不能随心所欲的使用这8个引脚.所以我们要注意他们的复用关系,以PH0为例,但是这个引脚被SPI功能所占用,所以我们使用时,需要改变PWM功能所在引脚的复用功能才能正常使用PWM功能.能够被我们正常使用的功能引脚是`PH6`、`PH7` 和 `PH8`引脚,其他引脚使用时需要注意引脚复用关系.如需一定要使用,请参考 [改变引脚复用关系](/soft/maixpy3/zh/module/linux/pwm.html#V831-Dock-改变引脚复用关系).\n\n![](./../../assets/linux/PWM/2021-09-22_10-27.png)\n\n| PIN Number  | PIN      | function                                   | 设备树配置     | 功能        | 备注  |\n| ----------- | -------- | ------------------------------------------ | --------- | --------- | --- |\n| 238(224+14) | PH14     | SPI1_CS0TWI3_SDAPH_EINT14                  |           | State_LED |     |\n| ---         | ---      |                                            |           | ---       | --- |\n| 166(160+6)  | PF6      | PF_EINT6                                   |           |           |     |\n|             | RST      |                                            |           |           |     |\n| 199(192+7)  | PG7      | UART1_RXPG_EINT7                           |           |           |     |\n| 198(192+6)  | PG6      | UART1_TXPG_EINT6                           |           |           |     |\n| 236(224+12) | PH12     | JTAG_CKRMII_TXENSPI1_MOSITWI2_SDAPH_EINT12 | TWI2_SDA  |           |     |\n| 235(224+11) | PH11     | JTAG_MSRMII_TXCKSPI1_CLKTWI2_SCKPH_EINT11  | TWI2_SCK  |           |     |\n| 238(224+14) | PH14     | JTAG_DIMDIOSPI1_CS0TWI3_SDAPH_EINT14       |           |           |     |\n| 237(224+13) | PH13     | JTAG_DOMDCSPI1_MISOTWI3_SCKPH_EINT13       |           |           |     |\n| 234(224+10) | PH10     | RMII_TXD0TWI3_SDAUART0_RXPH_EINT10         |           |           |     |\n|             | CPUX-RX  |                                            |           |           |     |\n|             | UART0-TX | PWM_9RMII_TXD1TWI3_SCKUART0_TXPH_EINT9     |           |           |     |\n| ---         | ---      |                                            |           | ---       | --- |\n|             | GND      |                                            |           |           |     |\n|             | 5V       |                                            |           |           |     |\n| 230(224+6)  | PH6      | PWM_6RMII_RXD0TWI2_SDAUART2_RXPH_EINT6     |           |           |     |\n| 231(224+7)  | PH7      | PWM_7RMII_CRS_DVUART0_TXUART2_RTSPH_EINT7  |           |           |     |\n| 232(224+8)  | PH8      | PWM_8RMII_RXERUART0_RXUART2_CTSPH_EINT8    |           |           |     |\n|             | GPADC0   |                                            |           |           |     |\n| 224(224+0)  | PH0      | PWM_0I2S0_MCLKSPI1_CLKUART3_TXPH_EINT0     | SPI1_CLK  |           |     |\n| 225(224+1)  | PH1      | PWM_1I2S0_BCLKSPI1_MOSIUART3_RXPH_EINT1    | SPI1_MOSI |           |     |\n| 226(224+2)  | PH2      | PWM_2I2S0_LRCKSPI1_MISOUART3_CTSPH_EINT2   | SPI1_MISO |           |     |\n| 227(224+3)  | PH3      | PWM_3I2S0_DOUTSPI1_CS0UART3_RTSPH_EINT3    | SPI1_CS0  |           |     |\n\n\n\n\n\n## PWM LED 测试\n\n使用 sysfs 操作 PWM 的例子：\n     \n<!-- 678 -->\n\n```shell\n#首先打开PWM6通道\ncd /sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0\n\nroot@sipeed:/sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0# echo 6 > export \n\nroot@sipeed:/sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0# ls\ndevice     export     npwm       pwm6       subsystem  uevent     unexport\n\n#设置PWM周期\ncd /sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0/pwm6\nroot@sipeed:/sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0/pwm6# echo 1000000 > period \n\n#设置PWM占空比\nroot@sipeed:/sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0/pwm6# echo 10000 > duty_cycle\n\n#开启PWM功能\nroot@sipeed:/sys/devices/platform/soc/300a000.pwm/pwm/pwmchip0/pwm6# echo 1 > enable\n\n#用示波器或者LED灯观察PH0端口是否有PWM波输出\n```\n\n## Python-PWM\n\n```python\nimport time\nfrom maix import pwm\nimport signal\n\n\ndef handle_signal_z(signum,frame):\n    print(\"APP OVER\")\n    exit(0)\n\n\nsignal.signal(signal.SIGINT,handle_signal_z)\n\n\nwith pwm.PWM(6) as pwm6:\n        pwm6.period = 1000000\n        pwm6.duty_cycle = 10000\n        pwm6.enable = True\n        duty_cycle = 10000\n        while True:\n            for i in range(1,10):\n                pwm6.duty_cycle = 10000 * i\n                time.sleep(1)\n```\n\n\n## V831 Dock 改变引脚复用关系\n\n以PH0为例,查询v831的datasheet手册我们能得到:\n![](./../../assets/linux/PWM/2021-09-22_10-37.png)\n![](./../../assets/linux/PWM/2021-09-22_10-35.png)\n`0x0300B0FC`寄存器的最低三位是控制PH0的引脚复用关系的,我们通过linux指令进行查看该寄存器中的值.\n\n~~~ shell\nroot@sipeed:/sys/class/sunxi_dump# ls\ncompare  dump     rw_byte  write\nroot@sipeed:/sys/class/sunxi_dump# echo 0x0300B0FC > dump \nroot@sipeed:/sys/class/sunxi_dump# cat dump \n0x77114444\n~~~\n由寄存器中的值我们知道最低两位是00,为此我们改变最低位的值,然后再写回去.\n\n~~~ shell\nroot@sipeed:/sys/class/sunxi_dump# echo 0x0300B0FC 0x77114442 > write \nroot@sipeed:/sys/class/sunxi_dump# cat dump \n0x77114442\n~~~\n修改成功后,我们就可以正常使用PWM0通道的PWM波输出了.\n\n\n~~~ python\n#使用PWM0的python3模块,使用其他复用引脚可以参考该写法\n#import PWM就可以使用\n\"\"\"Linux PWM driver sysfs interface\"\"\"\n\nimport os\nimport struct\n__author__ = 'Scott Ellis'\n__version__ = '1.0'\n__license__ = 'New BSD'\n__copyright__ = 'Copyright (c) 2016 Scott Ellis'\nfrom types import (\n    TracebackType,\n)\nfrom typing import (\n    Optional,\n    Type,\n)\nclass PWM(object):\n    \"\"\"\n    A class to work with the Linux PWM driver sysfs interface\n    \"\"\"\n\n    def __init__(self, channel: int = 0, chip: int = 0) -> None:\n        \"\"\" Specify channel and chip when creating an instance\n        The Linux kernel driver exports a sysfs interface like this\n            /sys/class/pwm/pwmchip<chip>/pwm<channel>\n        A <chip> can have multiple <channels>.\n        The channel and chip are determined by the kernel driver.\n        For example the two PWM timers from the RPi kernel driver\n        show up like this\n            /sys/class/pwm/pwmchip0/pwm0\n            /sys/class/pwm/pwmchip0/pwm1\n        To use the RPi timers create instances this way\n            pwm0 = PWM(0) or PWM(0,0)\n            pwm1 = PWM(1) or PWM(1,0)\n        \"\"\"\n        self._channel = channel\n        self._chip = chip\n        self.base = '/sys/class/pwm/pwmchip{:d}'.format(self._chip)\n        self.path = self.base + '/pwm{:d}'.format(self._channel)\n        #调整引脚复用功能\n        with open(\"/sys/class/sunxi_dump/dump\",\"wb\") as f:\n            f.write(b'0x0300B0FC')\n        with open(\"/sys/class/sunxi_dump/dump\",\"rb\") as f:\n            self.gpio = f.read()\n            self.gpio = self.gpio[:-1]\n        with open(\"/sys/class/sunxi_dump/write\",\"wb\") as f:\n            gpio_H0 = int(self.gpio,16)\n            gpio_H0 &= ~0x00000007\n            gpio_H0 |= 0x00000002\n            gpio_io = b'0x0300B0FC ' + bytes(hex(gpio_H0), 'ascii')\n            f.write(gpio_io)\n        if not os.path.isdir(self.base):\n            raise FileNotFoundError('Directory not found: ' + self.base)\n\n    # enable class as a context manager\n    def __enter__(self) -> 'PWM':\n        self.export()\n        return self\n\n    def __exit__(self,\n            exc_type: Optional[Type[BaseException]],\n            exc_value: Optional[BaseException],\n            traceback: Optional[TracebackType]) -> None:\n        self.enable = False\n        self.inversed = False\n        self.unexport()\n        #还原引脚复用功能\n        with open(\"/sys/class/sunxi_dump/dump\",\"wb\") as f:\n            f.write(b'0x0300B0FC')\n        with open(\"/sys/class/sunxi_dump/dump\",\"rb\") as f:\n            self.gpio_o = f.read()\n            self.gpio_o = self.gpio[:-1]\n        with open(\"/sys/class/sunxi_dump/write\",\"wb\") as f:\n            gpio_H0 = int(self.gpio,16)\n            gpio_H0 &= 0x00000007\n            gpio_H0_o = int(self.gpio_o,16)\n            gpio_H0_o &= ~0x00000007\n            gpio_H0_o |= gpio_H0\n            gpio_io = b'0x0300B0FC ' + bytes(hex(gpio_H0_o), 'ascii')\n            f.write(gpio_io)\n        return\n\n    def export(self) -> None:\n        \"\"\"Export the channel for use through the sysfs interface.\n        Required before first use.\n        \"\"\"\n        if not os.path.isdir(self.path):\n            with open(self.base + '/export', 'w') as f:\n                f.write('{:d}'.format(self._channel))\n\n    def unexport(self) -> None:\n        \"\"\"Unexport the channel.\n        The sysfs interface is no longer usable until it is exported again.\n        \"\"\"\n        if os.path.isdir(self.path):\n            with open(self.base + '/unexport', 'w') as f:\n                f.write('{:d}'.format(self._channel))\n\n    @property\n    def channel(self) -> int:\n        \"\"\"The channel used by this instance.\n        Read-only, set in the constructor.\n        \"\"\"\n        return self._channel\n\n    @property\n    def chip(self) -> int:\n        \"\"\"The chip used by this instance.\n        Read-only, set in the constructor.\n        \"\"\"\n        return self._chip\n\n    @property\n    def period(self) -> int:\n        \"\"\"The period of the pwm timer in nanoseconds.\"\"\"\n        with open(self.path + '/period', 'r') as f:\n            value = f.readline().strip()\n\n        return int(value)\n\n    @period.setter\n    def period(self, value: int) -> None:\n        with open(self.path + '/period', 'w') as f:\n            f.write('{:d}'.format(value))\n\n    @property\n    def duty_cycle(self) -> int:\n        \"\"\"The duty_cycle (the ON pulse) of the timer in nanoseconds.\"\"\"\n        with open(self.path + '/duty_cycle', 'r') as f:\n            value = f.readline().strip()\n\n        return int(value)\n\n    @duty_cycle.setter\n    def duty_cycle(self, value: int) -> None:\n        with open(self.path + '/duty_cycle', 'w') as f:\n            f.write('{:d}'.format(value))\n\n    @property\n    def enable(self) -> bool:\n        \"\"\"Enable or disable the timer, boolean\"\"\"\n        with open(self.path + '/enable', 'r') as f:\n            value = f.readline().strip()\n\n        return True if value == '1' else False\n\n    @enable.setter\n    def enable(self, value: bool) -> None:\n        with open(self.path + '/enable', 'w') as f:\n            if value:\n                f.write('1')\n            else:\n                f.write('0')\n\n    @property\n    def inversed(self) -> bool:\n        \"\"\"normal polarity or inversed, boolean\"\"\"\n        with open(self.path + '/polarity', 'r') as f:\n            value = f.readline().strip()\n\n        return True if value == 'inversed' else False\n\n    @inversed.setter\n    def inversed(self, value: bool) -> None:\n        with open(self.path + '/polarity', 'w') as f:\n            if value:\n                f.write('inversed')\n            else:\n                f.write('normal')\n~~~"}, "/soft/maixpy3/zh/api/maix/camera.html": {"title": "MaixPy3 camera 模块", "content": "---\ntitle: MaixPy3 camera 模块\nkeywords: MaixPy3，摄像头, camera API\ndesc: MaixPy3 camera 模块 API文档, 以及使用说明\n---\n\n>! API 仍处于非完全稳定状态, 可能在未来会有小幅改动。\n\n## 导入模块\n\n```python\nfrom maix import camera\n```\n\n### camera.width & camera.height\n\n返回当前摄像头配置的图像的宽和高，默认值为（0，0）。\n\n```\n>>> print(camera.width(), camera.height())\n0 0\n```\n\n### camera.config\n\n> 这个 API 涉及硬件容易使软件崩溃所以详细请查阅 [maix/camera.py](https://github.com/sipeed/MaixPy3/blob/release/maix/camera.py)。\n\n主要用于配置摄像头，如获取的图像大小、旋转（部分芯片可选）、翻转（部分芯片可选）。\n\n```python\n>>> camera.camera.config(size=(240, 240))\n[v4l2] Current data format information:\n\twidth:320\n\theight:240\n\tpixelformat:56595559\n[camera] config input size(240, 240, 0)\n>>> print(camera.width(), camera.height())\n240 240\n>>>\n```\n\n现在作为开发调试的保留功能，像缩放、裁剪、翻转、旋转请使用 image 的一系列函数。\n\n#### 关于增益和曝光的摄像头控制接口\n\n> 0.5.2 以后给 v83x m2dock 系列的加入了 [exp_gain](https://github.com/sipeed/MaixPy3/commit/d7e5cb04ed31a2ffe135407a0379a701bd3a5522) 函数。\n\n```python\nfrom maix import camera, display, image\ncamera.config(size=(224, 224))\nexp, gain = 16, 16 # 初值，exp 曝光[0, 65536]，gain 增益[16 - 1024]，随意设置得值会受到驱动限制。\nfor i in range(120):\n    exp, gain = exp + 32, gain + 16\n    camera.config(exp_gain=(exp, gain))\n    img = camera.capture()\n    display.show(img)\ncamera.config(exp_gain=(0, 0)) # 设置为 0, 0 表示放弃控制恢复成自动曝光。\n```\n\n做这些控制需要了解摄像头控制增益、曝光会发生什么，传统视觉有时候需要拉低曝光固定亮度去寻色寻线，这时候就需要设置特定的增益和曝光，比如拍白灯的时候需要拉低曝光才能看到灯罩的轮廓。\n\n### camera.capture\n\n捕获一张图像并返回 _maix_image.image 。\n\n（可选功能一）某些设备支持选择获取多个不同尺寸的图像，如 V831 设备。\n（可选功能二）可以返回其他模块实现的 image ，如 pillow 模块。\n\n```python\n>>> print(camera.capture())\n<_maix_image.Image 0x1eb3a10 \" width\":240, \"height\":240, \"type\"=RGB, \"size\":172800>\n```\n\n### camera.close\n\n关闭、释放当前设备。\n\n```python3\ncamera.close()\n```"}, "/soft/maixpy3/zh/api/maix/nn.html": {"title": "MaixPy3 nn模块(maix.nn)", "content": "---\ntitle: MaixPy3 nn模块(maix.nn)\nkeywords: MaixPy3, maix.nn, MaixPy3运行模型, maix.nn API\ndesc: MaixPy3 nn模块 API文档, 以及使用说明\n---\n\n>! API 仍处于非完全稳定状态, 可能在未来会有小幅改动, 如果你遇到了语法错误， 记得回来看更新哦~\n\n## maix.nn 基本使用介绍\n\n* 准备模型\n\n比如从 maixhub 下载, 这里以边缘检测模型为例, 先[下载模型](https://maixhub.com/modelInfo?modelId=24)(需要先注册登录)\n\n* 准备一张 `224 x 224` 分辨率的图像, 比如这里放到了开发板文件系统的`/root/test.png`位置\n\n* 运行代码, 将下面的代码保存到开发板的`test_model.py`中, 然后运行`python test_model.py`\n\n其中最重要的就是`m = nn.load`和`m.forward()`两个函数, 即加载模型, 和进行模型前向推理\n\n```python\nfrom maix import nn, display\nfrom PIL import Image\nimport numpy as np\n\ntest_jpg = \"/root/test.png\"\n\nmodel = {\n    \"param\": \"/root/models/sobel_int8.param\",\n    \"bin\": \"/root/models/sobel_int8.bin\"\n}\n\ninput_size = (224, 224, 3)\noutput_size = (222, 222, 3)\n\noptions = {\n    \"model_type\":  \"awnn\",\n    \"inputs\": {\n        \"input0\": input_size\n    },\n    \"outputs\": {\n        \"output0\": output_size\n    },\n    \"mean\": [127.5, 127.5, 127.5],\n    \"norm\": [0.0078125, 0.0078125, 0.0078125],\n}\nprint(\"-- load model:\", model)\nm = nn.load(model, opt=options)\nprint(\"-- load ok\")\n\nprint(\"-- read image\")\nimg = Image.open(test_jpg).resize(input_size[:2])\nprint(\"-- read image ok\")\nprint(\"-- forward model with image as input\")\nout = m.forward(img, quantize=True)\nprint(\"-- forward ok\")\nout = out.astype(np.float32).reshape(output_size)\nout = (np.abs(out) * 255 / out.max()).astype(np.uint8)\nimg2 = Image.fromarray(out, mode=\"RGB\")\n\ndisplay.show(img2)\n```\n\n## 方法 maix.nn.load()\n\n加载模型, 返回 `maix.nn.Model` 对象\n\n### 参数\n\n* `model_path`: 模型路径, 包含了字符串和字典两种形式\n\n字典形式：\n```python\n{\n    \"param\": \"/root/models/sobel_int8.param\",\n    \"bin\": \"/root/models/sobel_int8.bin\"\n}\n```\n\n字符串形式:\n```python\nmodel_path = \"/root/mud/sobel_int8.mud\"\n```\n\n>! 特别说明，MUD文件是模型统一描述文件，后文有[详细描述](https://wiki.sipeed.com/soft/maixpy3/zh/api/maix/nn.html#MUD%E6%96%87%E4%BB%B6)\n\n* `opt`: 设置项, 字典形式, 包括了:\n  * `model_type`: 模型类别, 目前仅支持 `awnn`\n  * `inputs`: 输入层, 字典形式, 关键字是层名称, 为字符串, 如果是加密模型, 需要使用整型; 值是层形状, 为一个`tuple`类型:`(h, w, c)`. 目前只支持单层输入层(未来会支持多层输入, 欢迎提交 `PR`)\n  比如:\n```python\n    # 未加密模型\n    \"inputs\": {\n        \"input0\": (224, 224, 3)\n    }\n    # 加密模型\n    \"inputs\": {\n        0: (224, 224, 3)\n    }\n```\n  * `outputs`: 输出层, 同理输入层. 支持多层输出\n  * `mean`: 如果在`forward`使参数`standardize=True`（默认为True）, 则会使用这个参数对输入数据进行归一化, 计算方法为`(x - mean) * norm`; 格式为`list` 或者 `float`(未支持, 欢迎提交 PR)\n  * `norm`: 定义参见上述`mean`描述\n>! 当`model_path`选择字符串格式时， 此项`opt`不用进行赋值，默认为None\n\n### 返回值\n\n返回 `maix.nn.Model` 对象\n\n\n## 类 maix.nn.Model\n\n包含了一系列神经网络操作,  `maix.nn.load()` 会返回其对象\n\n### maix.nn.Model.forward()\n\n只能由具体的对象调用, 不能类直接调用\n\n#### 参数\n\n* `inputs`: 输入, 可以是`_maix_image`的`Image`对象, 也可以是`HWC`排列的`bytes`对象, 也可以是`HWC`排列的`numpy.ndarray`对象(还未支持), 多层输入使用`list`(还未支持)\n>! 这个参数未来可能会进行优化\n* `standardize`: `int` 类型,默认为`1`, 当`load()`加载字典类型时， `opt` 的`mean,norm`参数对数据进行标准化；当`load()`加载字符串类型时，会根据mud文件自动进行标准化；置为`0`时不会对输入数据进行处理，则输入前需要将数据进行手动处理，处理方式跟模型训练时的数据处理一致 \n\n* `layout`: `\"hwc\"` 或者 `\"chw\"`， 默认设置为 `\"hwc\"`\n* `debug`: `int` 类型，默认为`0`,该值不为`0`时会打印`debug`信息和`forward`的推理耗时 \n\n#### 返回值\n\n特征图, 如果是单层输出, 是一个浮点类型的 `numpy.ndarray` 对象, 如果是多层输出, 会是一个`list`对象, 包含了多个`numpy.ndarray`对象.\n\n\n### maix.nn.Model.__del__()\n\n删除对象, 内存回收时会自动调用这个函数, 会释放模型占用的资源\n```python\ndel m\n```\n\n## 模块 maix.nn.decoder\n\n`nn` 后处理模块, 集成了常见的模型的后处理, 使用 `forward` 进行模型推理后得到特征图输出, 使用这个模块下的方法对输出的特征图进行后处理\n\n### 类 maix.nn.decoder.Yolo2\n\n`YOLO V2` 的后处理模块, 使用时需要创建一个对象,调用`run`方法对模型推理输出进行解码得到物体的坐标和类别.\n\n等价于如下`python`伪代码:\n\n```python\nclass Yolo2:\n    def __init__(self, class_num, anchors, net_in_size=(224, 244), net_out_size=(7, 7)):\n        pass\n\n    def run(self, fmap, nms = 0.3, threshold = 0.5, img_size = None):\n        boxes = []\n        probs = []\n        for x, y, w, h, _probs in valid_results:\n            if img:\n                x *= img_size[0]\n                y *= img_size[1]\n                w *= img_size[0]\n                h *= img_size[1]\n                x, y, w, h = int(x), int(y), int(w), int(h)\n            boxes.append([x, y, w, h])  # item type is float if img_size == 0, else int type\n            probs.append([max_probs_index, _probs]) # probs is list type, item type is float\n        return [boxes, probs]\n\n```\n\n使用时:\n\n```python\nfrom maix.nn import decoder\n\nlabels = [\"A\", \"B\", \"C\"]\nanchors = [1.19, 1.98, 2.79, 4.59, 4.53, 8.92, 8.06, 5.29, 10.32, 10.65]\n\nyolo2_decoder = decoder.Yolo2(len(labels), anchors)\nyolo2_decoder.run(bytes([0]*10))\n\n...\n\nout = m.forwar(img, layout=\"hwc\")\nboxes, probs = yolo2_decoder.run(out, thres=0.5, nms=0.3, img_size=(img.width, img.height))\nfor i, box in enumerate(boxes):\n    class_id = probs[i][0]\n    prob = probs[i][1][class_id]\n    disp_str = \"{}:{:.2f}%\".format(labels[class_id], prob*100)\n    print(\"final box: {}, {}\".format(box, disp_str))\n\n```\n\n#### maix.nn.decoder.Yolo2.__init__()\n\n构造对象时会自动调用\n\n##### 参数\n\n\n* `class_num`: 类别数量\n* `anchors`: 预选框, `list` 类型, 数量为偶数, 必须要和训练时使用的`anchors` 相同, 也就是说跟模型绑定的参数, 如果你不知道, 请找提供模型的人提供\n* `net_in_size`: 网络输入层分辨率\n* `net_out_size`: 网络输出层分辨率\n\n\n\n#### maix.nn.decoder.Yolo2.run()\n\n执行解码(后处理), 只能对象进行调用, 不能类直接调用\n\n##### 参数\n\n* `fmap`: 网路输出的特征图, 一般是`forward` 函数的结果\n* `nms`: 非极大值抑制(Non-Maximum Suppression), 用来去掉重复的框, `IOU`(两个框的重叠区域)大于这个值就只取概率大的一个, 取值范围:`[0, 1]`, 默认值为 `0.3`\n* `threshold`: 置信度阈值, 大于这个值才认为物体有效, 取值范围:`[0, 1]`, 默认 `0.5`\n* `img_size`: 源图像大小, `tuple`类型, 比如`(320, 240)`, 这会使返回值的`box` 坐标自动计算为相对于源图的坐标, 并且类型为整型, 默认`None` 则不会计算, `box` 返回相对值(百分比), 浮点类型, 范围:`[0, 1]`\n\n\n##### 返回值\n\n`[boxes, probs]`, `list` 类型, 可以参考上面的使用例子, 其中:\n\n* `boxes`: `list` 类型, 每个元素也是一个`list`, 包含`[x, y, w, h]`, 分别代表了框左上角坐标, 以及框的宽高\n* `probs`: `list` 类型, 每个元素也是一个`list`, 包含`[max_prob_idx, all_classes_probs]`\n  * `all_classes_probs`: `list` 类型, 包含了该框所有类别的置信度\n  * `max_prob_idx`: 整型, 代表了`all_classes_probs`中最大值的下标, 取值范围: `[0, classes_num - 1]`\n\n\n\n\n\n## 模块 maix.nn.app\n\n应用模块， 包含了一些有意思的应用模块\n\n### 模块 maix.nn.app.classifier\n\n自学习分类器（视觉）， 无需训练模型， 只使用特征提取模型， 在运行时学习多个物体特征，然后即可对物体进行分类识别。 适用于简单的分类场景。\n\n`maix.nn.app.classifier`的`python`伪代码:\n\n```python\nclass Classifier:\n    def __init__(self, model, class_num, sample_num, feature_len, input_w, input_h):\n        pass\n\n    def add_class_img(self, img):\n        return idx\n\n    def add_sample_img(self, img):\n        return idx\n\n    def train(self):\n        pass\n\n    def predict(self, img):\n        return idx, min_dist\n\n    def save(self, path):\n        pass\n\ndef load(model, path):\n    return Classifier()\n\n```\n\n#### 类 Classifier\n\n使用时需要指定类别数量，通过`add_class_img`函数传入物体图像来获得物体的特征值， 然后通过`add_sample_img`获取这几个类别的图像，用以对开始采集的图像特征值进行优化， `sample`的图像和开始采集的类别图像可以有一定的差异， 但是不要相差太大， 采集的顺序无所谓；\n然后调用`train`方法进行训练(其实就是`kmeans` 聚类)， 就可以得到使用`sample`图像特征值优化过后的几个分类的特征值；\n最后使用`predict`就可以对输入图像的类别进行识别\n\n##### 构造方法： __init__(self, model, class_num, sample_num, feature_len, input_w, input_h)\n\n* 参数：\n  * `model`: `maix.nn.Model`对象， 用于获得图片的特征值\n  * `class_num`: 要学习的物体类别数量， 比如 `3`\n  * `sample_num`: 用以学习特征的物体数量， 比如`3*5 => 15`\n  * `feature_len`: 特征值的长度， 取决于特征提取模型的输出形状， 比如例程使用`resnet18 1000 分类`模型， 倒数第二层输出长度是`512`\n  * `input_w`: 输入的图像的宽度\n  * `input_h`: 输入的图像的高度\n\n##### 方法: add_class_img(self, img)\n\n添加分类图片， 会自动调用模型推理获取图片的特征值\n\n* 参数：\n  * `img`: 输入图像， 可以是`Pillow`的`Image`对象, 也可以是`HWC`排列的`bytes`对象\n\n* 返回值： `int` 类型, 代表返回成功添加第几个类别的特征值， 取值∈`[0, class_num)`\n\n* 抛出错误： 如果出现错误， 比如添加图片超过类别数量等， 会抛出错误信息\n\n\n##### 方法: add_sample_img(self, img)\n\n添加样本图片， 会自动调用模型推理获取图片的特征值\n\n* 参数：\n  * `img`: 输入图像， 可以是`Pillow`的`Image`对象, 也可以是`HWC`排列的`bytes`对象\n\n* 返回值： `int` 类型, 代表返回成功添加第几个样本图片的特征值， 取值∈`[0, sample_num)`\n\n* 抛出错误： 如果出现错误， 比如添加图片超过设置的样本数量等， 会抛出错误信息\n\n\n##### 方法: train(self)\n\n训练样本（本质上是聚类分类）， 需要在`add_class_img`和`add_sample_img`完成后才能调用，否则会出现误差\n\n\n* 抛出错误： 如果出现错误， 比如类别或者样本采集未完成， 会抛出错误信息\n\n\n##### 方法： predict(self, img)\n\n预测给定的图片所属的类别\n\n* 参数：\n  * `img`: 输入图像， 可以是`Pillow`的`Image`对象, 也可以是`HWC`排列的`bytes`对象\n\n* 返回值：\n  * `idx`: `int` 类型, 代表给定的图片的特征和这个分类最接近， 取值∈`[0, sample_num)`\n  * `min_dist`: 图片的特征和`idx`类别的特征的距离， 距离越小则代表和该类越相似\n\n* 抛出错误： 如果出现错误， 比如参数错误等， 会抛出错误\n\n\n##### 方法： save(self, path)\n\n保存当前的特征值参数到文件， 方便断电保存并下次加载使用\n\n* 参数：\n  * `path`: 保存的路径， 字符串\n\n* 抛出错误： 保存出错， 会抛出错误信息\n\n\n#### 方法 load(model, path)\n\n加载保存的特征值参数文件， 获得[类 Classifier](#类-Classifier)的对象， 加载完成后可直接使用`predict`函数\n\n* 参数：\n  * `model`: `maix.nn.Model`对象， 用于获得图片的特征值， 需要和保存的时候使用的模型相同\n  * `path`: 保存参数的路径\n\n* 返回值： 获得[类 Classifier](#类-Classifier)的对象\n\n\n### 模块 maix.nn.app.face\n\n人脸识别模块， [这里](https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/face_recognize.py)有一个`Face_Recognizer`类提供了人脸识别的简单封装， 推荐使用\n\n#### 类 FaceRecognize\n\n伪代码：\n\n```python\nclass FaceRecognize:\n  def __init__(self, model_detect, model_fea, fea_len, input_shape, threshold, nms, max_face_num)\n    pass\n\n  def get_faces(self, img, std_img = False):\n    return [ prob, [x,y,w,h], [[x,y], [x,y], [x,y], [x,y], [x,y]], feature ]\n\n  def compare(self, feature_a, feature_b):\n    return score\n```\n\n[这里](https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/face_recognize.py)有一个`Face_Recognizer`类提供了人脸识别的简单封装， 推荐使用\n\n使用的模型可以到[这里下载](https://maixhub.com/modelInfo?modelId=29)\n\n##### 构造方法: __init__(self, model_detect, model_fea, fea_len, input_shape, threshold, nms, max_face_num)\n\n\n* 参数\n  * `model_detect`: 检测模型， [maix.nn.Model](#类-maix.nn.Model) 对象\n  * `model_fea`: 特征提取模型， [maix.nn.Model](#类-maix.nn.Model) 对象\n  * `fea_len`: 人脸特征的长度，比如 `256`\n  * `input_shape`: 输入图像的形状，`(w, h, c)`格式， 比如`(224, 224, 3)`\n  * `threshold`: 人脸检测阈值， 默认`0.5`\n  * `nms`: 人脸检测非极大值抑制值，即用来防止重复框一个人脸， 默认`0.3`\n  * `max_face_num`: 支持的同时框的人脸数量，取`1`或者更多\n\n##### 获取人脸信息: get_faces(self, img, std_img = False)\n\n获取人脸信息， 包括位置和特征值等\n\n* 参数\n  * `img`: 输入图像， 分辨率需要和检测模型的输入相同，  比如`224 x 224`， 可以是`PIL.Image.Image`对象， 或者`bytes`对象\n  * `std_img`: 取值`True`或者`False`, 选择是否返回纠正过的标准人脸图片\n\n* 返回值: 返回一个 `list` 对象，`[ prob, box, landmarks, feature, std_img ]`，其中`std_img`根据构造函数的参数`std_img`决定是否存在\n  * `prob`: 检测到人脸的概率， 比设置的`threshold`大\n  * `box`: 人脸框， 值为`[x,y,w,h]` ， 分别代表左上角坐标和框的宽高\n  * `landmarks`: 人脸关键点， 共`5`个点, 格式`[[x,y], [x,y], [x,y], [x,y], [x,y]]`，分别代表了左眼、右眼、鼻子、左嘴角、右嘴角的坐标\n  * `feature`: 人物脸的特征值， 一个`list`，`list`中的项目值类型为`float`（未来有可能会有`feature`为`bytes`的可选项）\n  * `std_img`: 人脸图像，`PIL.Image.Image`对象， 只有当构造函数的参数`std_img`为`True`时才会有这个返回值\n\n##### 对比人脸特征: compare(self, feature_a, feature_b)\n\n对比两个人脸特征值相似度，并返回相似度百分比\n\n* 参数\n  * `feature_a`: `get_faces`函数的返回值, 一个`list`对象或者`bytes`对象\n  * `feature_b`: `get_faces`函数的返回值, 一个`list`对象或者`bytes`对象\n\n* 返回值： 返回两个人脸特征值的对比相似度分数（百分比），取值范围 `∈` `[0.0, 100.0]`\n\n## MUD文件\n\nMUD 文件是模型统一描述文件，全称 model universal description file ；mud 文件可以简化模型运行 Python 代码，还可以使得不同平台的模型文件使用同一份代码运行\n\n### 文件形式\n\n文件以 .mud 为后缀，严格按照 INI 的格式进行解析\n\n### 文件内容\n\nMUD 文件是按照既定顺序和内容进行解读。不同的 MUD 文件之间结构都非常类似，由若干段落（section）组成，在每个带括号的标题下面，是若干个以单个单词开头的关键词（keyword）和一个等号，等号右边的就是关键字对应的值（value); section部分通常用`[]`进行声明，例如：\n```bash\n[basic]\ntype = awnn\nparam =/root/models/awnn_retinaface.param\nbin =/root/models/awnn_retinaface.bin\n\n[inputs]\ninput0 = 224,224,3,127.5, 127.5, 127.5,0.0078125, 0.0078125, 0.0078125\n\n[outputs]\noutput0 = 1,4,2058\noutput1 = 1,2,2058\noutput2 = 1,10,2058\n\n[extra]\noutputs_scale =\ninputs_scale =\n```\n\n#### section & keywords\n* [basic]: 基础参数段，包含了模型模型类型、模型参数、模型结构三种文件\n    * type：不同目标平台的标识，如R329称为aipu，V831代称为awnn\n    * bin：V831，R329 平台模型的参数二进制文件\n    * param：V831的模型结构文件，R329融合了模型参数和模型结构故此项置为空\n\n* [inputs] :输入信息段\n\n  input ：输入节点名称，输入个数随模型而定，其前三个参数定义为 H,W,C ，后两个的参数以此是mean，norm \n    >! H != 1 && W != 1 && C == 3 的时候，输入为三通道图像，依次按照mean_R , mean_G ，mean_B ，norm_R ，norm_G ，norm_B 的顺序往后排列  \n    >  H != 1 && W != 1 && C == 1 的时候，输入为灰度图， mean_gray , norm_gray ，按顺序两个值，进行排列  \n    >  H ==  1 && W == 1 && C != 1 ，输入为三维向量， mean_v , norm_v ，按顺序两个值，进行排列  \n\n* [outputs] :输出信息段\n\n  output ：输出节点名称，输出节点个数根据模型而定，其参数定义为 H,W,C\n\n* [extra]:额外的参数段，一般是不同平台额外的参数。\n    *  inputs_scale：输入层在量化后的scale值，按照输入顺序排列 ，如inputs_scale = input0_scale , input1_scale, input2_scale ……\n    *  outputs_scale：输出层在量化后的scale值，按照输出顺序排列，如outputs_scale = ouput0_scale , output1_scale , output2_scale …… （目前仅R329需要）"}, "/soft/maixpy3/zh/api/maix/image.html": {"title": "MaixPy3 image 模块", "content": "---\ntitle: MaixPy3 image 模块\nkeywords: MaixPy3，图像处理，图像算法，传统视觉, image API\ndesc: MaixPy3 image 模块 API 文档, 以及使用说明\n---\n\n>! API 仍处于非完全稳定状态, 可能在未来会有小幅改动。\n\n## 导入模块\n\nImage 对象是机器视觉操作的基本对象。\n\n```python\nfrom maix import image\n```\n\nimage(_maix_image) 模块采用 pybind11 c++ 语言进行开发。\n\n```bash\n            any_image\n         |              |\n         V              V\n    maix_vision      other\n                | |\n                 T\n                 V\n            maix_image\n```\n\n## 图像模块全局接口\n\n> 0.4.6 到 0.4.8 后才引入全局模块接口。\n\n- image.new\n\n  创建 Image 对象，对应下述的 Image.open 实现。\n\n- image.load\n\n  加载图像数据，并创建 Image 对象，对应下述的 Image.load 实现。\n\n- image.open\n\n  打开图像文件，并创建 Image 对象，对应下述的 Image.open 实现。\n\n- image.load_freetype(path=\"xxx.ttf\")\n\n  加载 freetype 字体,加载后执行 draw_string() 函数将会用该函数加载的字体.\n\n- image.get_string_size(str=\"hello\", [scale=1.0])\n\n  计算某个字符串的高度和长度，方便字符串确定显示的位置。\n  str 字符串， scale 对应 draw_string 的 scale 参数。\n\n- image.free_freetype()\n\n  释放当前加载 freetype 字体。\n\n## Image 对象属性变量\n\n- Image.width\n\n  返回以像素计的图像的宽度。\n\n- Image.height\n\n  返回以像素计的图像的高度。\n\n- Image.mode\n\n  返回图像的格式,目前支持\"RGB\", \"RGBA\", \"L\", \"RGB16\"(RGB565)\n\n- Image.size\n\n  返回以像素计的图像的大小，如相同 width height 但不同 mode 的 RGB 和 RGBA 不同缓冲区长度。\n\n## Image 对象基础接口\n\n打开、关闭、加载、保存、转换、复制、清除、序列化。\n\n- Image.new([size = (240, 240), [color = (0, 0, 0) , [mode = \"RGB\"]]])\n\n  创建一张新的图片,size为图像尺寸,color为图像填充颜色,mode为图像格式.目前支持\"RGB\", \"RGBA\", \"L\", \"RGB16\"(RGB565)\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.open(path)\n\n  从二进制文件中打开一张图片,path为路径名,会统一转换成RGB模式.Image.open(\"./tmp.jpg\")\n\n- Image.save(path)\n\n  将Image 对象保存成二进制文件,Image.save(\"./tmp.jpg\")\n\n- Image.load(data, [size = (240, 240) , [mode = \"RGB\"]])\n\n  在 python 对象中加载出一张图像,会将 python 对象的数据 copy 到 Image 对象内部，如将 tobytes 的二进制数据重新恢复成 Image对象。\n  date可以是PIL对象, image.Image() 对象,bytes对象,numpy 对象.\n  当data为bytes,numpy对象时,需要提供size和mode参数.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.copy([img = \"maix_image\"])\n\n  返回一张img类的Image 对象,img可以是\"maix_image\", \"PIL\", \"numpy\"\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.clear()\n\n  清除 image.Image 对象内部的图像数据，但不删除对象。\n\n- Image.delete()\n\n  释放 image.Image 对象。\n\n- Image.tobytes()\n\n  返回图像的 bytes 数据，用于序列化数据。\n\n- Image.resize(w, h, func = 1, padding = 1, size=(0, 0))\n\n  将图像调整至(w, h)大小,func 可选,size 可选（和 w h 互斥），padding 默认会按比例缩放填充，而不是 CV 的拉伸图像变形。\n    0     (INTER_NEAREST 最近邻插值)\n    1     (INTER_LINEAR 双线性插值)（默认设置）\n    2     (INTER_CUBIC 4x4像素邻域的双三次插值)\n    3     (INTER_AREA 使用像素区域关系进行重采样)\n    4     (INTER_LANCZOS4 8x8像素邻域的Lanczos插值)\n\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.draw_line(x1, y1, x2, y2, [color = (127, 127, 127), [thickness = 1]])\n\n  在图像上绘制一条从(x0，y0)到(x1，y1)的线。\n  color 是RGB888元组。默认为灰色。\n  thickness 控制线的粗细像素。\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.draw_cross(x1, y1, c, size, [color = (127, 127, 127), [thickness = 1]])\n\n  还不确定实现接口，待测试和设计。\n\n- Image.draw_rectangle(x1, y1, x2, y2, [color = (127, 127, 127), [thickness = 1]])\n\n  在图像上绘制一个矩形。\n  color 是RGB888元组。默认为灰色。\n  thickness 控制线的粗细像素。当thickness=-1时,将会用color颜色填充整个区域.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.draw_circle(x, y, radius, [color = (127, 127, 127), [thickness = 1]])\n\n  在图像上绘制一个圆形。x, y为圆的中心点坐标, radius为圆的半径\n  color 是RGB888元组。默认为灰色。\n  thickness 控制线的粗细像素。当thickness=-1时,将会用color颜色填充整个区域.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.draw_ellipse(cx, xy, rx, ry, angle, startAngle, endAngle, [color = (127, 127, 127), [thickness = 1]])\n\n  在图像上绘制椭圆。\n  cx, xy, rx, ry ,椭圆的中心坐标和长轴短轴.\n  angle 椭圆旋转角度(0~180)\n  startAngle椭圆的开始绘图角度(0~360),\n  endAngle椭圆的结束绘图角度(0~360),\n  color 是RGB888元组。默认为灰色。\n  thickness 控制线的粗细像素。当thickness=-1时,将会用color颜色填充整个区域.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.draw_string(x, y, str, [scale = 1.0, [color = (127, 127, 127), [thickness = 1]]])\n\n  从图像中的(x, y)位置开始绘制文本\n  scale 可以放大/缩小图像上文本的大小。您可以传递大于0的整数或浮点值。\n  color 是RGB888元组。默认为灰色。\n  thickness 控制线的粗细像素。您可以传递大于0的整数.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.rotate(rotate = 1.0)\n\n  旋转图像到固定的角度,保持图像的尺寸不变\n  rotate旋转角度.您可以传递大于0的浮点值.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.flip(flip = 1)\n\n  沿着 x 或 y 轴进行翻转图像，保持图像的尺寸不变，参数 -1 horizontal & vertical, 1 horizontal, 0 vertical 。\n\n- Image.convert(mode = \"RGB\")\n\n  转换图像的格式.\n  mode为图像格式.目前支持\"RGB\", \"RGBA\", \"L\", \"RGB16\"(RGB565)\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.crop(x, y, w, h)\n\n  别名为 cut 函数接口。\n  裁剪图片返回一张全新的图片\n  x, y, w, h裁剪图像的位置和大小\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.draw_image(img, x, y)\n\n  将传递的img图像画在 image.Image() 对象内部的图像上\n  img只能传递 image.Image() 对象的图像.\n  返回 Image 对象，以便您可以使用 . 表示法调用另一个方法。\n\n- Image.get_pixel(x, y)\n\n  得到图像x,y位置的像素值,返回是一个四元组对象,(r, g, b, a),如果为灰度,那只有r有效\n\n## Image 对象视觉操作\n\nmaix_vision 类是对于图像的一系列操作方法的集合\n\n- maix_vision.get_blob_color(roi = (0, 0, 0, 0), critical = 0, co = 0)\n\n  统计函数,得到图像感兴趣区域的最大颜色值.\n  roi 是感兴趣区域的矩形元组(x，y，w，h)。如果未指定，ROI即整个图像的图像矩形。 操作范围仅限于 roi 区域内的像素。尽量选取较小的区域,区域较大时统计效果比较差.\n  critical为返回值区域范围,简单将统计到的最大颜色值和critical相加或相减,当为0时,返回原来的色值.\n  co为返回的颜色空间模型.可选为\n  0       rgb\n  1       lab\n  2       hsv\n  返回值:\n  当co为0时返回[r, g, b]\n  当co为1时返回[L - critical, A - critical, B - critical, L + critical, A + critical, B + critical]\n  当co为2时同1\n\n- maix_vision.find_blobs(thresholds, roi = (0,0,0,0), x_stride = 2, y_stride = 2, invert = 0, area_threshold = 10, pixels_threshold = 10, merge = 0, margin = 0, tilt = 0, co = 1)\n\n  查找图像中所有色块，并返回一个包括每个色块的色块对象的列表。\n  thresholds 必须是元组列表. [(minL, minA, minB, maxL, maxA, maxB)]\n  roi 是感兴趣区域的矩形元组(x，y，w，h)。如果未指定，ROI即整个图像的图像矩形。 操作范围仅限于 roi 区域内的像素。\n  x_stride 是查找某色块时需要跳过的x像素的数量。找到色块后，直线填充算法将精确像素。 若已知色块较大，可增加 x_stride 来提高查找色块的速度。\n  y_stride 是查找某色块时需要跳过的y像素的数量。找到色块后，直线填充算法将精确像素。 若已知色块较大，可增加 y_stride 来提高查找色块的速度。\n  invert 反转阈值操作，像素在已知颜色范围之外进行匹配，而非在已知颜色范围内。\n  若一个色块的边界框区域小于 area_threshold ，则会被过滤掉。\n  若一个色块的像素数小于 pixels_threshold ，则会被过滤掉。\n  merge 若为True，则合并所有没有被过滤掉的色块，这些色块的边界矩形互相交错重叠。\n  margin 可在相交测试中用来增大或减小色块边界矩形的大小。例如：边缘为1、相互间边界矩形为1的色块将被合并。\n  tilt设置是否查找最小斜矩形框,为0则不查找.\n  co为返回的颜色空间模型.可选为,如果不是特殊需要,请保持默认\n  0       rgb\n  1       lab\n  2       hsv\n  3       灰度\n  返回值:[{'x': 140, 'y': 88, 'w': 15, 'h': 7, 'pixels': 43, 'cx': 147, 'cy': 91}]\n  (x, y, w, h)色块的外框,pixels,色块的像素大小,(cx, cy)色块的中心点.\n\n- maix_vision.find_ball_color(thresholds, co = 1)\n\n  该函数是在maix_vision.find_blobs的基础上通过基尔霍夫圆拟合,并返回拟合的圆.\n  thresholds 必须是元组列表. [(minL, minA, minB, maxL, maxA, maxB)]\n  co为返回的颜色空间模型.可选为,如果不是特殊需要,请保持默认\n  0       rgb\n  1       lab\n  2       hsv\n  返回值:\n\n- maix_vision.find_line()\n\n  该函数是内置的寻线函数.通过自适应的图像操作,将图像中的黑线选出来,然后返回黑线的矩形区域,可以作为小车的寻线函数.\n  返回值:\n  {'rect': [9, 229, 9, 9, 145, 9, 145, 229], 'pixels': 12959, 'cx': 77, 'cy': 119, 'rotation': -1.570796251296997}\n  rect为线的框,\n  pixels为线的像素大小\n  (cx, cy)框的中心点\n  rotation为框的偏转角度(弧度制)\n\n> 下述函数可用，但暂时没写说明。\n\n- get_statistics\n\n- rotation_corr\n\n- gamma_corr\n\n- lens_corr\n\n- mean\n\n- find_rects\n\n- find_lines\n\n- find_circles\n\n- find_line_segments\n\n- find_apriltags\n\n- find_qrcodes\n\n- find_barcodes"}, "/soft/maixpy3/zh/api/maix/i2c.html": {"title": "MaixII M2dock I2C 调试", "content": "---\ntitle: MaixII M2dock I2C 调试\nkeywords: MaixII, MaixPy3, Python, Python3, M2dock\ndesc: maixpy doc: MaixII M2dock I2C 调试\n---\n\nV831 镜像中默认包含 **i2c-tools**, i2c-tools 包含如下四条命令\n\n## 1. i2cdetect\n\n**查询 I2C 用法**\n\n```shell\nUsage: i2cdetect [-y] [-a] [-q|-r] I2CBUS [FIRST LAST]\n       i2cdetect -F I2CBUS\n       i2cdetect -l\n  I2CBUS is an integer or an I2C bus name\n```\n\n**查询 I2C 总线**\n\n```shell\ni2cdetect -l\n```\n\n\n\n**查询 I2C 总线上挂载的设备**\n\n| -y   | 取消交互过程，直接执行指令 |\n| ---- | ------------- |\n| twi2 | I2C 总线编号      |\n\n```shell\ni2cdetect -y 1\n```\n\n\n\n## 2. i2cdump\n\n扫描寄存器内容：\n\n```shell\ni2cdump -y 1 0x68\n```\n\n\n\n## 3. i2cget\n\n```shell\ni2cget -y 1 0x68 0x00\n```\n\n| -y   | 取消交互过程，直接执行指令                 |\n| ---- | ----------------------------- |\n| 1    | I2C 总线编号                      |\n| 0x68 | I2C 设备地址，此处表示 DS3231 RTC 时钟芯片 |\n| 0x00 | 代表存储器地址                       |\n\n\n\n## 4. i2cset\n\n**寄存器内容写入：**\n\n```shell\ni2cset -y 1 0x68 0x00 0x13\n```\n\n| -y   | 取消交互过程，直接执行指令                 |\n| ---- | ----------------------------- |\n| 1    | I2C 总线编号                      |\n| 0x68 | I2C 设备地址，此处表示 DS3231 RTC 时钟芯片 |\n| 0x00 | 寄存器地址                         |\n| 0x13 | 需要写入的寄存器值                     |\n\n## python\n\n```python\nfrom maix import i2c\ni2c = i2c.I2CDevice('/dev/i2c-2', 0x26)\nprint(i2c)\nprint(i2c.read(0x1, 1))\n```"}, "/soft/maixpy3/zh/api/maix/gpio.html": {"title": "MaixII M2dock I2C gpio 调试", "content": "---\ntitle: MaixII M2dock I2C gpio 调试\nkeywords: MaixII, MaixPy3, Python, Python3, M2dock\ndesc: maixpy doc: MaixII M2dock gpio 调试\n---\n\n## PIN_CTL\n\n- lichee/linux-4.9/drivers/pinctrl/sunxi/pinctrl-sun8iw19p1-r.c\n\n- lichee/linux-4.9/drivers/pinctrl/sunxi/pinctrl-sun8iw19p1.c\n\n![](./asserts/v831_pin_maps.png)\n\n### V831 Dock PIN Maps\n\n- PINCTRL_PIN(64 + (0), \"P\" \"C\" \"0\")\n- PINCTRL_PIN(96 + (0), \"P\" \"D\" \"0\")\n- PINCTRL_PIN(128 + (0), \"P\" \"E\" \"0\")\n- PINCTRL_PIN(160 + (0), \"P\" \"F\" \"0\")\n- PINCTRL_PIN(192 + (0), \"P\" \"G\" \"0\")\n- PINCTRL_PIN(224 + (0), \"P\" \"H\" \"0\")\n- PINCTRL_PIN(256 + (0), \"P\" \"I\" \"0\")\n\n| PIN Number  | PIN      | function                                   | 设备树配置     | 功能        | 备注  |\n| ----------- | -------- | ------------------------------------------ | --------- | --------- | --- |\n| 238(224+14) | PH14     | SPI1_CS0TWI3_SDAPH_EINT14                  |           | State_LED |     |\n| ---         | ---      |                                            |           | ---       | --- |\n| 166(160+6)  | PF6      | PF_EINT6                                   |           |           |     |\n|             | RST      |                                            |           |           |     |\n| 199(192+7)  | PG7      | UART1_RXPG_EINT7                           |           |           |     |\n| 198(192+6)  | PG6      | UART1_TXPG_EINT6                           |           |           |     |\n| 236(224+12) | PH12     | JTAG_CKRMII_TXENSPI1_MOSITWI2_SDAPH_EINT12 | TWI2_SDA  |           |     |\n| 235(224+11) | PH11     | JTAG_MSRMII_TXCKSPI1_CLKTWI2_SCKPH_EINT11  | TWI2_SCK  |           |     |\n| 238(224+14) | PH14     | JTAG_DIMDIOSPI1_CS0TWI3_SDAPH_EINT14       |           |           |     |\n| 237(224+13) | PH13     | JTAG_DOMDCSPI1_MISOTWI3_SCKPH_EINT13       |           |           |     |\n| 234(224+10) | PH10     | RMII_TXD0TWI3_SDAUART0_RXPH_EINT10         |           |           |     |\n|             | CPUX-RX  |                                            |           |           |     |\n|             | UART0-TX | PWM_9RMII_TXD1TWI3_SCKUART0_TXPH_EINT9     |           |           |     |\n| ---         | ---      |                                            |           | ---       | --- |\n|             | GND      |                                            |           |           |     |\n|             | 5V       |                                            |           |           |     |\n| 230(224+6)  | PH6      | PWM_6RMII_RXD0TWI2_SDAUART2_RXPH_EINT6     |           |           |     |\n| 231(224+7)  | PH7      | PWM_7RMII_CRS_DVUART0_TXUART2_RTSPH_EINT7  |           |           |     |\n| 232(224+8)  | PH8      | PWM_8RMII_RXERUART0_RXUART2_CTSPH_EINT8    |           |           |     |\n|             | GPADC0   |                                            |           |           |     |\n| 224(224+0)  | PH0      | PWM_0I2S0_MCLKSPI1_CLKUART3_TXPH_EINT0     | SPI1_CLK  |           |     |\n| 225(224+1)  | PH1      | PWM_1I2S0_BCLKSPI1_MOSIUART3_RXPH_EINT1    | SPI1_MOSI |           |     |\n| 226(224+2)  | PH2      | PWM_2I2S0_LRCKSPI1_MISOUART3_CTSPH_EINT2   | SPI1_MISO |           |     |\n| 227(224+3)  | PH3      | PWM_3I2S0_DOUTSPI1_CS0UART3_RTSPH_EINT3    | SPI1_CS0  |           |     |\n\n## sysfs 操作 GPIO\n\n```shell\nroot@sipeed:/# ls -l /sys/class/gpio\n--w-------    1 root     root          4096 Dec  9 08:54 export\nlrwxrwxrwx    1 root     root             0 Dec  9 08:54 gpiochip0 -> ../../devices/platform/soc/pio/gpio/gpiochip0\nlrwxrwxrwx    1 root     root             0 Dec  9 08:54 gpiochip352 -> ../../devices/platform/soc/r_pio/gpio/gpiochip352\n--w-------    1 root     root          4096 Dec  9 08:54 unexport\nroot@sipeed:/#\n```\n\n/sys/class/gpio 目录下的三种文件：\n\n- export/unexport 文件:  `/sys/class/gpio/export`，只写，写入 GPIO 编号来向内核申请 GPIO 控制权（前提是没有内核代码申请这个 GPIO 端口）, 成功后会在目录下生成 gpioN 目录, `/sys/class/gpio/unexport` 和导出的效果相反。\n\n- gpioN 指代具体的 gpio 引脚:  指代某个具体的 gpio 端口, 内有以下属性文件：\n\n| Attribution | Read/Write | Value                          | Function     |\n| ----------- | ---------- | ------------------------------ | ------------ |\n| direction   | RW         | in,out;low,high                | 设置输入输出       |\n| value       | RW         | 0,非零                           | 读取或者写入 IO 电平 |\n| edge        | RW         | none , rising , falling , both | 配置中断触发方式     |\n| active_low  | RW         | 0,非零                           | 设置低电平有效      |\n\n- gpiochipN 指代 gpio 控制器:  gpiochipN 表示的就是一个 gpio_chip, 用来管理和控制一组 gpio 端口的控制器，该目录下存在以下属性文件：\n\n| Attribution | Function                      |\n| ----------- | ----------------------------- |\n| base        | 和N相同，表示控制器管理的最小的端口编号。         |\n| lable       | 诊断使用的标志，寄存器地址，1c20800.pinctrl |\n| ngpio       | 表示控制器管理的 gpio 端口数量，A~G，224    |\n\n### LED 测试\n\n使用 sysfs 操作 GPIO 的例子：\n\n```shell\nls -l /sys/class/gpio/ # show gpio\necho 238 > /sys/class/gpio/export  #export PH14(238), State_LED\nls -l /sys/class/gpio/ # show gpio\n# output test\necho \"out\" > /sys/class/gpio/gpio238/direction # set gpio mode: direction\necho 0 > /sys/class/gpio/gpio238/value # set gpio output level: low\necho 1 > /sys/class/gpio/gpio238/value # set gpio output level: height\n# input test\necho \"in\" > /sys/class/gpio/gpio238/direction #设置为输入\ncat /sys/class/gpio/gpio192/value #读取电平\n\n```\n\n```bash\nll /sys/devices/platform/soc/r_pio/\n```\n\n## Python-gpiod\n\n![](./asserts/v831_gpio.png)\n\n```python\nimport gpiod\nc = gpiod.chip(\"gpiochip1\")\n# pylint: disable=missing-docstring\nimport sys\nimport time\nimport pytest\nfrom gpiod import chip, line, line_request\n\ntry:\n    if len(sys.argv) > 2:\n        LED_CHIP = sys.argv[1]\n        LED_LINE_OFFSET = int(sys.argv[2])\n    else:\n        raise Exception()\n# pylint: disable=broad-except\nexcept Exception:\n    print(\n        \"\"\"Usage:\n    python3 -m gpiod.test.blink <chip> <line offset>\"\"\"\n    )\n    sys.exit()\n\nc = chip(LED_CHIP)\n\nprint(\"chip name: \", c.name)\nprint(\"chip label: \", c.label)\nprint(\"number of lines: \", c.num_lines)\n\nprint()\n\nled = c.get_line(LED_LINE_OFFSET)\n\nprint(\"line offset: \", led.offset)\nprint(\"line name: \", led.name)\nprint(\"line consumer: \", led.consumer)\nprint(\n    \"line direction: \",\n    \"input\" if led.direction == line.DIRECTION_INPUT else \"output\",\n)\nprint(\n    \"line active state: \",\n    \"active low\" if led.active_state == line.ACTIVE_LOW else \"active high\",\n)\nprint(\"is line used: \", led.is_used)\nprint(\"is line open drain: \", led.is_open_drain)\nprint(\"is_open_source: \", led.is_open_source)\nprint(\"is line requested: \", led.is_requested)\n\nprint(\"\\nrequest line\\n\")\n\nconfig = line_request()\nconfig.consumer = \"Blink\"\nconfig.request_type = line_request.DIRECTION_OUTPUT\n\nled.request(config)\n\nprint(\"line consumer: \", led.consumer)\nprint(\n    \"line direction: \",\n    \"input\" if led.direction == line.DIRECTION_INPUT else \"output\",\n)\nprint(\n    \"line active state: \",\n    \"active low\" if led.active_state == line.ACTIVE_LOW else \"active high\",\n)\nprint(\"is line used: \", led.is_used)\nprint(\"is line open drain: \", led.is_open_drain)\nprint(\"is_open_source: \", led.is_open_source)\nprint(\"is line requested: \", led.is_requested)\n\nwhile True:\n    led.set_value(0)\n    time.sleep(0.1)\n    led.set_value(1)\n    time.sleep(0.1)\n```\n\n```python\npython test_blink.py gpiochip0 238\n```"}, "/soft/maixpy3/zh/recommend_articles.html": {"title": "", "content": "# Maixpy3 精选文章"}, "/soft/maixpy3/zh/develop/face_reco.html": {"title": "", "content": "## R329 人脸识别\n\n## 目前进度\n\n- 已完成，等待release。\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=295857402&bvid=BV18F411p7mD&cid=488991789&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>"}, "/soft/maixpy3/zh/develop/resnet.html": {"title": "V831上部署resnet18分类网络", "content": "# V831上部署resnet18分类网络\n\n> 2022年01月11日 以下代码由于 MaixPy3 还在施工中，此处代码仅供参考和示范，功能已在 github 和 社区供其他同学使用和参考。\n\n## 前期准备\n在V831上使用resnet18分类网络，我们需要在linux环境下进行。windows系统可以使用虚拟机，或者是使用WSL，具体的安装教程请自行百度，这里就不过多的进行描述\n\n### 安装pytorch环境\n\n我们需要在系统中安装pytorch，通过在pytorch官网上可以知道安装pytorch需要执行\n\n    pip3 install torch==1.9.0+cpu torchvision==0.10.0+cpu torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n\n或者是通过conda环境进行安装\n\n    conda install pytorch torchvision torchaudio cpuonly -c pytorch\n\n我们还需要安装一个`torchsummary`库来进行神经网络的可视化\n\n    pip3 install torchsummary\n\n### 编译ncnn转换工具\n\n通过 `git clone https://github.com/Tencent/ncnn.git` 将ncnn的仓库拉取到本地，进行编译\n\n安装编译环境的依赖\n\n```bash\nsudo apt update\nsudo apt install build-essential git cmake libprotobuf-dev protobuf-compiler libvulkan-dev vulkan-utils libopencv-dev\n```\n编译ncnn需要使用到 Vulkan 后端\n要使用 Vulkan 后端，请安装 Vulkan 头文件、一个 vulkan 驱动程序加载器、GLSL 到 SPIR-V 编译器和 vulkaninfo 工具。或者从<https://vulkan.lunarg.com/sdk/home>下载并安装完整的 Vulkan SDK（大约 200MB；它包含所有头文件、文档和预构建的加载程序，以及一些额外的工具和所有源代码）\n\n```bash\nwget https://sdk.lunarg.com/sdk/download/1.2.182.0/linux/vulkansdk-linux-x86_64-1.2.182.0.tar.gz\ntar xvf vulkansdk-linux-x86_64-1.2.182.0.tar.gz\nexport VULKAN_SDK=$(pwd)/1.2.182.0/x86_64\n```\n\n拉取ncnn的子仓库\n\n```bash\ncd ncnn\ngit submodule update --init\n```\n\n开始编译ncnn\n```bash\nmkdir -p build\ncd build\ncmake -DCMAKE_BUILD_TYPE=Release -DNCNN_VULKAN=ON -DNCNN_SYSTEM_GLSLANG=ON -DNCNN_BUILD_EXAMPLES=ON ..\nmake -j$(nproc)\n```\n\n编译结束之后会在build/tools/onnx/下的到onnx2ncnn可执行文件，这个是就用ncnn的转换工具\n\n> 将编译出来的 onnx2ncnn 添加到系统的环境变量中\n\n## 获取模型并进行推理\n\n> 以下代码建议在jupyter中运行\n\n通过pytorch hub来获取resnet18的预训练模型，这里并不细说训练的过程和模型定义\n\nlabel[下载](https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt)\n使用以下代码进行模型的下载和推理\n```python\nimport os\nimport torch\nfrom torchsummary import summary\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\n## model\nmodel = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\nmodel.eval()\ninput_shape = (3, 224, 224)\nsummary(model, input_shape, device=\"cpu\")\n## test image\nfilename = \"out/dog.jpg\"\nif not os.path.exists(filename):\n    if not os.path.exists(\"out\"):\n        os.makedirs(\"out\")\n    import urllib\n    url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", filename)\n    try: urllib.URLopener().retrieve(url, filename)\n    except: urllib.request.urlretrieve(url, filename)\nprint(\"test image:\", filename)\n## preparing input data\nfrom PIL import Image\nimport numpy as np\nfrom torchvision import transforms\ninput_image = Image.open(filename)\n# input_image.show()\npreprocess = transforms.Compose([\n    transforms.Resize(max(input_shape[1:3])),\n    transforms.CenterCrop(input_shape[1:3]),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ninput_tensor = preprocess(input_image)\nprint(\"input data max value: {}, min value: {}\".format(torch.max(input_tensor), torch.min(input_tensor)))\ninput_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n## forward model\n# move the input and model to GPU for speed if available\nif torch.cuda.is_available():\n    input_batch = input_batch.to('cuda')\n    model.to('cuda')\nwith torch.no_grad():\n    output = model(input_batch)\n## result    \n# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n# print(output[0])\n# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\nmax_1000 = torch.nn.functional.softmax(output[0], dim=0)\nmax_idx = int(torch.argmax(max_1000))\nwith open(\"imagenet_classes.txt\") as f:\n    labels = f.read().split(\"\\n\")\nprint(\"result: idx:{}, name:{}\".format(max_idx, labels[max_idx]))\n```\n\n运行后得到结果:\n```python\nUsing cache found in /home/neucrack/.cache/torch/hub/pytorch_vision_v0.6.0\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1         [-1, 64, 112, 112]           9,408\n       BatchNorm2d-2         [-1, 64, 112, 112]             128\n              ReLU-3         [-1, 64, 112, 112]               0\n         MaxPool2d-4           [-1, 64, 56, 56]               0\n            Conv2d-5           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-6           [-1, 64, 56, 56]             128\n              ReLU-7           [-1, 64, 56, 56]               0\n            Conv2d-8           [-1, 64, 56, 56]          36,864\n       BatchNorm2d-9           [-1, 64, 56, 56]             128\n             ReLU-10           [-1, 64, 56, 56]               0\n       BasicBlock-11           [-1, 64, 56, 56]               0\n           Conv2d-12           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-13           [-1, 64, 56, 56]             128\n             ReLU-14           [-1, 64, 56, 56]               0\n           Conv2d-15           [-1, 64, 56, 56]          36,864\n      BatchNorm2d-16           [-1, 64, 56, 56]             128\n             ReLU-17           [-1, 64, 56, 56]               0\n       BasicBlock-18           [-1, 64, 56, 56]               0\n           Conv2d-19          [-1, 128, 28, 28]          73,728\n      BatchNorm2d-20          [-1, 128, 28, 28]             256\n             ReLU-21          [-1, 128, 28, 28]               0\n           Conv2d-22          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-23          [-1, 128, 28, 28]             256\n           Conv2d-24          [-1, 128, 28, 28]           8,192\n      BatchNorm2d-25          [-1, 128, 28, 28]             256\n             ReLU-26          [-1, 128, 28, 28]               0\n       BasicBlock-27          [-1, 128, 28, 28]               0\n           Conv2d-28          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-29          [-1, 128, 28, 28]             256\n             ReLU-30          [-1, 128, 28, 28]               0\n           Conv2d-31          [-1, 128, 28, 28]         147,456\n      BatchNorm2d-32          [-1, 128, 28, 28]             256\n             ReLU-33          [-1, 128, 28, 28]               0\n       BasicBlock-34          [-1, 128, 28, 28]               0\n           Conv2d-35          [-1, 256, 14, 14]         294,912\n      BatchNorm2d-36          [-1, 256, 14, 14]             512\n             ReLU-37          [-1, 256, 14, 14]               0\n           Conv2d-38          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-39          [-1, 256, 14, 14]             512\n           Conv2d-40          [-1, 256, 14, 14]          32,768\n      BatchNorm2d-41          [-1, 256, 14, 14]             512\n             ReLU-42          [-1, 256, 14, 14]               0\n       BasicBlock-43          [-1, 256, 14, 14]               0\n           Conv2d-44          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-45          [-1, 256, 14, 14]             512\n             ReLU-46          [-1, 256, 14, 14]               0\n           Conv2d-47          [-1, 256, 14, 14]         589,824\n      BatchNorm2d-48          [-1, 256, 14, 14]             512\n             ReLU-49          [-1, 256, 14, 14]               0\n       BasicBlock-50          [-1, 256, 14, 14]               0\n           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n             ReLU-53            [-1, 512, 7, 7]               0\n           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n           Conv2d-56            [-1, 512, 7, 7]         131,072\n      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n             ReLU-58            [-1, 512, 7, 7]               0\n       BasicBlock-59            [-1, 512, 7, 7]               0\n           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n             ReLU-62            [-1, 512, 7, 7]               0\n           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n             ReLU-65            [-1, 512, 7, 7]               0\n       BasicBlock-66            [-1, 512, 7, 7]               0\nAdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n           Linear-68                 [-1, 1000]         513,000\n================================================================\nTotal params: 11,689,512\nTrainable params: 11,689,512\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.57\nForward/backward pass size (MB): 62.79\nParams size (MB): 44.59\nEstimated Total Size (MB): 107.96\n----------------------------------------------------------------\nout/dog.jpg\ntensor(2.6400) tensor(-2.1008)\nidx:258, name:Samoyed, Samoyede\n\n```\n可以看到模型有 11,689,512的参数， 即 11MiB左右， 这个大小也就几乎是实际在 831 上运行的模型的大小了\n\n## 模型转换\n\n### pth转onnx\n通过pytorch hub获取到的resnet18 模型是pth格式的，需要转换成onnx格式的模型\n\n转换代码\n```python\ndef torch_to_onnx(net, input_shape, out_name=\"out/model.onnx\", input_names=[\"input0\"], output_names=[\"output0\"], device=\"cpu\"):\n    batch_size = 1\n    if len(input_shape) == 3:\n        x = torch.randn(batch_size, input_shape[0], input_shape[1], input_shape[2], dtype=torch.float32, requires_grad=True).to(device)\n    elif len(input_shape) == 1:\n        x = torch.randn(batch_size, input_shape[0], dtype=torch.float32, requires_grad=False).to(device)\n    else:\n        raise Exception(\"not support input shape\")\n    print(\"input shape:\", x.shape)\n    # torch.onnx._export(net, x, \"out/conv0.onnx\", export_params=True)\n    torch.onnx.export(net, x, out_name, export_params=True, input_names = input_names, output_names=output_names)\nonnx_out=\"out/resnet_1000.onnx\"\nncnn_out_param = \"out/resnet_1000.param\"\nncnn_out_bin = \"out/resnet_1000.bin\"\ninput_img = filename\ntorch_to_onnx(model, input_shape, onnx_out, device=\"cuda:0\")\n\n```\n在out文件夹中得到onnx格式模型文件\n\n### onnx转ncnn\n\n然后再利用前面编译出来的onnx2ncnn转换工具进行ncnn格式的转换\n\n```python\ndef onnx_to_ncnn(input_shape, onnx=\"out/model.onnx\", ncnn_param=\"out/conv0.param\", ncnn_bin = \"out/conv0.bin\"):\n    import os\n    # onnx2ncnn tool compiled from ncnn/tools/onnx, and in the buld dir\n    cmd = f\"onnx2ncnn {onnx} {ncnn_param} {ncnn_bin}\"\n    os.system(cmd)\n    with open(ncnn_param) as f:\n        content = f.read().split(\"\\n\")\n        if len(input_shape) == 1:\n            content[2] += \" 0={}\".format(input_shape[0])\n        else:\n            content[2] += \" 0={} 1={} 2={}\".format(input_shape[2], input_shape[1], input_shape[0])\n        content = \"\\n\".join(content)\n    with open(ncnn_param, \"w\") as f:\n        f.write(content)\nonnx_to_ncnn(input_shape, onnx=onnx_out, ncnn_param=ncnn_out_param, ncnn_bin=ncnn_out_bin)\n```\n\n> 这里需要确定 onnx2ncnn 是可以使用的命令，否则会无法使用这个函数进行模型转换\n\n\n\n### ncnn量化到int8模型\n\n通过maixhub将ncnn模型进行量化到int8模型\n\n在 maixhub 模型转换 将 ncnn 模型转换为 awnn 支持的 int8 模型 （网页在线转换很方便人为操作，另一个方面因为全志要求不开放 awnn 所以暂时只能这样做）\n\n阅读转换说明，可以获得更多详细的转换说明\n![](./../asserts/maixhub.jpg)\n\n这里有几组参数：\n\n- 均值 和 归一化因子： 在 pytorch 中一般是 (输入值 - mean ) / std, awnn对输入的处理是 (输入值 - mean ) \\* norm, 总之，让你训练的时候的输入到第一层网络的值范围和给awnn量化工具经过(输入值 - mean ) \\* norm 计算后的值范围一致既可。 比如 这里打印了实际数据的输入范围是[-2.1008, 2.6400]， 是代码中preprocess 对象处理后得到的，即x = (x - mean) / std ==> (0-0.485)/0.229 = -2.1179, 到awnn就是x = (x - mean_2\\*255) \\* (1 / std \\* 255) 即 mean2 = mean \\* 255, norm = 1/(std \\* 255), 更多可以看这里。\n\n- 所以我们这里可以设置 均值为 0.485 \\* 255 = 123.675， 设置 归一化因子为1/ (0.229 \\* 255) = 0.017125， 另外两个通道同理，但是目前 awnn 只能支持三个通道值一样。。。所以填123.675, 123.675, 123.675，0.017125, 0.017125, 0.017125 即可，因为这里用了pytorch hub的预训练的参数，就这样吧， 如果自己训练，可以好好设置一下图片输入层尺寸（问不是图片怎么办？貌似 awnn 暂时之考虑到了图片。。）\n\n- RGB 格式： 如果训练输入的图片是 RGB 就选 \n- RGB量化图片， 选择一些和输入尺寸相同的图片，可以从测试集中拿一些，不一定要图片非常多，但尽量覆盖全场景（摊手\n\n> 自己写的其它模型转换如果失败，多半是啥算子不支持，需要在 使用说明里面看支持的 算子，比如现在的版本view、 flatten、reshape 都不支持所以写模型要相当小心， 后面的版本会支持 flatten reshape 等 CPU 算子\n\n如果不出意外， 终于得到了量化好的 awnn 能使用的模型， \\*.param 和 \\*.bin\n\n## 使用模型，在v831上推理\n可以使用 python 或者 C 写代码，以下两种方式\n\npython的是需要在终端下运行的，不要使用jupyter，建议使用ssh，这样放文件什么都比较方便\n\n### MaixPy3\npython 请看MaixPy3\n\n不想看文档的话，就是在系统开机使用的基础上， 更新 MaixPy3 就可以了：\n\n    export TMPDIR=/root && pip install --upgrade maixpy3\n\n然后在终端使用 python 运行脚本（可能需要根据你的文件名参数什么的改一下代码）：\n\nhttps://github.com/sipeed/MaixPy3_scripts/blob/main/basic/v1.0/resnet.py\n\nlabel 在这里： https://github.com/sipeed/MaixPy3/blob/main/ext_modules/_maix_nn/example/classes_label.py\n\nbaars.ttf 在这里：https://github.com/sipeed/MaixPy3_scripts/blob/main/application/base/res/baars.ttf\n```python\nfrom maix import camera, nn, display\nfrom home.res.classes_label import labels\nclass Resnset:\n    m = {\n        \"param\": \"/home/model/resnet18_1000_awnn.param\",\n        \"bin\": \"/home/model/resnet18_1000_awnn.bin\"\n    }\n    options = {\n        \"model_type\":  \"awnn\",\n        \"inputs\": {\n            \"input0\": (224, 224, 3)\n        },\n        \"outputs\": {\n            \"output0\": (1, 1, 1000)\n        },\n        \"first_layer_conv_no_pad\": False,\n        \"mean\": [127.5, 127.5, 127.5],\n        \"norm\": [0.00784313725490196, 0.00784313725490196, 0.00784313725490196],\n    }\n    def __init__(self):\n        from maix import nn\n        self.model = nn.load(self.m, opt=self.options)\n       \n    def __del__(self):\n        del self.model\n\n\n\n\nwhile True:\n    img = camera.capture().resize(224, 224)\n    tmp = img.tobytes()\n    out = resnset.model.forward(tmp, quantize=True)\n    out2 = nn.F.softmax(out)\n    msg = \"{:.2f}: {}\".format(out2.max(), labels[out.argmax()])\n    img.draw_string(0, 0, str(msg), 0.5, (255, 0, 0), 1)\n    display.show(img)\n```\n> 如果运行报错了，请更新maixpy3再运行\n\n\n\n### C语言 SDK， libmaix\n访问这里，按照 https://github.com/sipeed/libmaix 的说明克隆仓库，并编译 https://github.com/sipeed/libmaix/tree/master/examples/nn_resnet\n\n上传编译成功后dist目录下的所有内容到 v831, 然后执行./start_app.sh即可\n\n> 以上内容出至：<https://neucrack.com/p/358>"}, "/soft/maixpy3/zh/develop/v83x_isp.html": {"title": "", "content": "## M2DOCK 摄像头 ISP 调试（待公开）\n\n- 摄像头 ISP 调试的经验之谈（以全志 AW hawkview 为例） https://www.cnblogs.com/juwan/p/14865188.html\n\n- 开源 SDK 地址 https://github.com/Tina-V833/tina-V833 与 文档 [Sipeed 内部培训] V831/V833 的 SDK 的 kernel & package 的开发方法 https://www.cnblogs.com/juwan/p/15226245.html\n\n- 摄像头调试工具 TigerISP-20211225.7z https://api.dl.sipeed.com/shareURL/MaixII/MaixII-Dock/tools\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=978258498&bvid=BV1p44y1L7t3&cid=487661962&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>"}, "/soft/maixpy3/zh/develop/apriltag.html": {"title": "", "content": "## apriltag\n\n开发原型时间： maixpy3 0.4.0 以上\n\n预计开发时间： 2022年 Q1 结束。\n\n开发目标：官方 apriltag 而非 openmv 版本\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=850839110&bvid=BV1wL4y1t7hc&cid=487534380&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n<iframe src=\"//player.bilibili.com/player.html?aid=677488628&bvid=BV1wm4y197Jf&cid=464634360&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n## 进度\n\n- libmaix 实现分支 https://github.com/sipeed/libmaix/blob/apriltag/examples/camera/main/src/main.c\n\n## 开发情况\n\n参考 https://book.openmv.cc/image/apriltag.html\n\n官网 https://april.eecs.umich.edu/software/apriltag.html\n\n## 性能指标记录\n\nV831 libmaix AprilTag tag36h11 SP2305 QVGA 30fps ~ OV7251 QQGVA 60fps\n\n采用 opencv 绘图，直出 yuv 取 y 做灰度识别，效果理想。"}, "/soft/maixpy3/zh/develop/self_yolo.html": {"title": "", "content": "## 自学习检测\n\n开发原型时间： maixpy3 0.4.5 以上\n\n预计开发时间： 开发结束，待合并公开。\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=465843172&bvid=BV1y5411f7Yy&cid=486868043&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n<iframe src=\"//player.bilibili.com/player.html?aid=297876630&bvid=BV1aF41137Ff&cid=568622764&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n## 进度\n\n- 未公开"}, "/soft/maixpy3/zh/develop/index.html": {"title": "近期更新内容", "content": "---\ntitle: 近期更新内容\nkeywords: maixpy3, 开发进程\ndesc: maixpy3 doc: 开发进程\nclass: heading_no_counter\n---\n\n> 2022年07月22日 设置一个板块来让大家知道最近增加了哪些内容，以及开发者们所关心的如何二次开发的问题。\n\n## MaixPy3 更新日志\n\n---\n\n### V0.5.1\n\nRelease 页面： https://github.com/sipeed/MaixPy3/releases/tag/MaixPy-0.5.1\n\n#### V831\n\n- 分离出名称带 maixhub 和 maixpy3 的镜像；前者内置了开机应用，后者无开机应用。\n\n- image 模块修复了 find_blobs 的 lab 阈值不准确的问题。\n\n- image 添加了 image.cv_orb() 的 orb 特征提取 和 find_template ncc 模板匹配函数，效率基本满足 10 ~ 15 fps。\n\n#### R329\n\n- 无新更新内容\n\n---\n\n### V0.5.0\n\nRelease 页面： https://github.com/sipeed/MaixPy3/releases/tag/MaixPy-0.5.0\n\n#### V831\n\n- 内置 maixhub 应用，可以直接在线训练并且直接部署。在线训练网址：maix.sipeed.com\n- 启动顺序为先执行 /root/app/main.py 然后是 /root/main.py。不需要 maixhub 一键部署后可以直接 `rm -rf /root/app`\n\n相关问题：\n\n- 使用时候被提醒 `Camera is busy` 时记得手动退出一下开机应用\n![camera is busy](./../assets/develop/v0.5.0_camera_busy.png)\n\n#### R329\n\n- 无新更新内容\n\n---\n\n\n## MaixPy3 开发进度跟踪\n\n- 点击相关项目名称可以跳转到对应的展示页面\n\n| 项目                                         | 进度       |\n| -------------------------------------------- | ---------- |\n| [自学习检测](./self_yolo.html)                 | 待公开     |\n| [Maixsmart 串口框架](./maixcam.html)           | 待公开     |\n| [中文语音识别模型](./maix_speech.html)         | 待公开     |\n| [MaixSense YOLO 训练方法](./r329_yolo.html)    | 待公开     |\n| [MaixSense 人脸识别](./face_reco.html)         | 待公开     |\n| [MFCC关键词检测](./maixpy3_mfcc.html)          | 已归档 |\n| [MaixII-Dock 摄像头 ISP 调试](./v83x_isp.html) | 已公开     |\n| [AprilTag 标签定位](./apriltag.html)           | 已公开     |\n| [传统视觉算法](./opmv_cv.html)                 | 已公开     |\n\n通常步骤说明如下：（点不开可能是没有写说明，但有在做。）\n\n1. 在计划 （收到社区反馈）\n2. 开发中 （在做了在做了） 或 待移植 （需要复制粘贴修代码）\n3. 待测试 （在测了在测了）\n4. 待整理 （在写了在写了）\n5. 待公开 （此时已经完成了基本功能原型、性能与效果测试、相关应用与开发文档，但没有公开）\n6. 待合并 （该功能已经公开，但未能合并到 MaixPy3 项目，通常是某些硬件专属功能、工具）\n7. 已公开 （该功能已经完成基础功能并公开）\n\n当完成上述步骤后，就表示该功能已进到 maixpy3 项目中。"}, "/soft/maixpy3/zh/develop/maixcam.html": {"title": "", "content": "## maix 二次开发串口固件\n\n开发原型时间： maixpy3 0.4.0 以上\n\n预计开发时间： 2022年 Q1 开始 Q2 结束。\n\n开发目标：将开发的成果打包转换成用户直接可用的串口固件。\n\n项目地址：https://github.com/sipeed/MaixSmart\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=465758292&bvid=BV1UL411c77c&cid=487544102&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n## 进度\n\n- 下位机原型 https://github.com/Neutree/maix-mm\n\n- 上位机原型 https://github.com/Neutree/COMTool\n\n- 云服务支持 https://maixhub.com/\n\n## 开发情况\n\n初版已经提交，待公开。"}, "/soft/maixpy3/zh/develop/maixpy3_mfcc.html": {"title": "", "content": "## maix 关键词 MFCC 检测\n\n开发原型时间： maixpy3 0.4.0 以上\n\n预计开发时间： 已完成，等待 release 合并。\n\n开发目标：可以学习的语音识别，比较适合实际的固定场景。\n\n## 进度\n\nhttps://github.com/junhuanchen/speech-recognition\n\nlinux 通用的简易 VAD + MFCC 关键词识别，使用方法如下：\n1. 输入 n 等待人说话，输入序号保存录音。\n2. 输入 l 查看 waves words 目录下保存的语音段。\n3. 输入 d 后说话，给出识别的保存的语音段可能的结果。\n4. 输入 e 退出。\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=585184775&bvid=BV1oz4y1C7yE&cid=251878910&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n<iframe src=\"//player.bilibili.com/player.html?aid=500528923&bvid=BV1xK41137Rv&cid=263534446&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>"}, "/soft/maixpy3/zh/develop/opmv_cv.html": {"title": "MaixPy3 视觉模块开发", "content": "# MaixPy3 视觉模块开发\n\n> 2022年07月22日 最新进度可以通过查看共享文档获得：<https://www.kdocs.cn/l/cfdsBPVRhXxo>\n\n## 项目简介：\n\n由于 MaixPy3 的环境传统视觉功能薄弱，所有需要结合 openmv 文档实现视觉模块的基础 API 功能以及应用，相关功能也会体现在产品介绍当中，其中一些相关模块可能配合应用需要完成，应用示例文档参考如下：https://book.openmv.cc/image/\n\n- Feature-Detection 特征检测\n- Image-Filters 图像滤波\n- Color-Tracking 颜色追踪\n- Codes-Tracking 标记追踪\n\n依赖如下仓库，可在 X86 ubuntu20 本机仿真。\n- [github.com/sipeed/maixpy3](https://github.com/sipeed/maixpy3)\n- [github.com/sipeed/libmaix](https://github.com/sipeed/libmaix)\n\n## 实现目标：（请查看底部最新进度）\n能够满足如下应用：获取图像统计信息 / 寻找色块 / AprilTag标记跟踪 / 模板匹配 / 多模板匹配 / 特征点检测 / 测距 / 扫码识别 / 颜色形状同时识别 / 颜色模板匹配同时识别 / 高级特征\n\n![](./../assets/opencv/develop_1.jpg)\n\n上图来自 maixpy3 文档 https://wiki.sipeed.com/soft/maixpy3/zh/usage/vision/maixpy3-example.html\n## 代码原型：\nMaixPy3 image 模块环境（用户直接调用的 API 层接口）\nhttps://github.com/sipeed/MaixPy3/blob/develop/ext_modules/_maix_image/include/maix_image.h#L67\nLibmaix imlib 模块环境（Openmv 底层核心算法）\nhttps://github.com/sipeed/libmaix/blob/develop/examples/imlib_test/main/src/main.c#L190\n## 开发指导：\n- 在 maixpy3 的 develop 里准备好了所有需要的环境\n- 接口采用 image.xxx 通过参数选择所用的函数。\n- 每个模块都要包装到可用的示例代码程度，先设计示例代码再封装功能函数，最后才去实现。\n- 在 ubuntu20 上测试过后再到实机中开发，可以节约大量生命。\n本模块邀请开源社区的同学一起开发，参与到开发或测试的同学可以直接领取 V831 和 R329 硬件，参与度（提交）足够的情况下是可以不用归还的，所以要来就进群吧！预计开发周期为一、二个月，我会在本周确认参与的开发人员，请自带 ubuntu20 实机测试环境喔，摄像头和屏幕缺一不可，参与者在外部参与人员中登记。\n\n\n## 官方负责人员：\n- 陈俊欢 (大佬鼠)\n- 万启超\n\n## 外部参与人员：\n- 戚晨夕\n- 王晓\n- 刘子健\n- Steven\n- 刘闫兴\n- Alex\n- 曾广仕\n- 叶炯凯\n- 针针扎\n- 斌\n- 张罗东\n- 爱笑的莫妮卡\n- 奈奎斯特不稳定\n- 黄涛\n- 东东\n- 吴冰晶\n- 李伟\n\n## 分工情况：\n主要划分为 开发 和 测试 两块\n测试 主要负责内容为准备测试案例、测试效果、文档接口说明，整理好需要开发做的功能和流程，协助开发实现。\n开发 主要负责具体代码的编写和实现，从 C / C++ 到 Python 平台的适配，根据文档完成接口的输入输出，配合测试文档完成功能。\n请在微信群中备注自己的名称，然后想做的部分登记在这里，最后再进行分工，工作内容主要分开发和测试，无论是开发还是测试都需要在本机准备好环境，先在 ubuntu20 完成功能测试后再上到实机测试，可以提高效率，节约生命。\n- 陈俊欢 开发\n- 万启超 开发\n- Steven 开发\n- Alex     测试\n- 张罗东  测试\n- 爱笑的莫妮卡     测试\n- 叶炯凯 测试\n    - 图像拼接、融合：画中画、多张局部图像合成一张全局图像、\n    - 物体检测：帧差分法(检测有无物体运动，实现简单快速)，光流检测(进一步获取物体运动轨迹）\n    - 分割： 漫水填充(魔术棒、需要找到一个种子点）\n    - 滤波：（时域、快）高斯、均值、中值、双边、NLM、导向滤波 (频域、慢）同态、带阻/带通滤波、小波\n    - 增强：gamma、自适应直方图均衡、自适应对比度、白平衡相关（灰度世界···）\n- 针针扎     测试\n- 斌 测试\n- 刘子健（兔子） 测试\n- 奈奎斯特不稳定 测试\n    - 透视矫正：将原本的透视图转换为俯视图或者任意一个平面（可以准确得获得图像信息）\n    - 距离测试：获取与指定物体的距离\n    - 标记跟踪定位：包括测距，定位物体与镜头的相对位置，在以定位物体为原点的坐标系中获取摄像头的位置，或者以摄像头为中心原点获取定位物的坐标（以俯视图为平面的二维坐标或者直接在三维上返回六个自由度）\n    - 快速线性回归：对连续的曲线进行分析，得出曲线的运动趋势。\n- 曾广仕 开发\n- 黄涛 测试\n- 东东 开发\n- pepsi 测试\n- 王晓 开发\n- 胡宗向 测试\n- 刘闫兴 开发\n- 芃芃爹  测试\n- 吴冰晶 测试\n- taorye 开发\n- 李伟 测试\n- SimonLiu 测试\n\n## 问与答\n\nQ：如何参与？\nA：进群加入我们的讨论吧！\n\nQ：如何提交？\nA：直接在 MaixPy3 和 Libmaix 的仓库上提交或 PR 就行，最终也是看提交的。\n\nQ：如何开发？选择了 测试就不可以开发了吗？\nA：测试是起点，代表内测用户的体验，然后开发是基于测试的内容进行的，这代表测试和开发都可以一起做，互不冲突，只是侧重点不同。\ne\nQ：开发需要做什么？测试需要做什么？\nA：开发的需要了解底层实现和代码在哪，能够跑代码修代码测代码改代码。测试需要了解用户是如何使用的，能够说明这个功能是怎么设计怎么使用的，需要什么接口，使用流程是怎样的？文档说明提供出来。开发需要实现对应功能的移植优化，做好鲁棒性，做好接口的统一封装，让使用者不需要了解内部数据流转的细节，调用相应接口就可以完成输出需要的数据\n\nQ：报错 InportError: No module named pybind11 ？\nA：安装 sudo apt install python3-pybind11 或 pip3 intall pybind11\n\n## 项目进度：\n\n## 第一次会议 2022-03-29 （~~准备中~~，计划 9 点左右）\n### 会议主题：\n\n主要目的是分析现有应用所需要的模块，确保分工方式和合并方式，依赖模块为 特征检测 图像滤波 颜色追踪 标记追踪。\n\n目前为大佬鼠想象的，可能不准确，需要结合文档示例代码来分析所需接口，还有待探讨。\n\n> 底层实现肯定都是存在的，但不代表完美可用，所以才需要封装。\n\n假设两个人一组，分测试和开发，测试先确定文档和示例代码效果，开发先分析要做的模块都有哪些，最后我们统一汇总各个讯息。\n\n确定接口内容我们再开始，由我@Juwan 负责统筹确保项目运作。\n\n### 功能表（如果发现有更多有意义有需要的功能可以分析后继续添加）\n- 获取指定区域图像统计信息 https://book.openmv.cc/image/statistics.html\n    - 颜色空间处理，如 LAB RGB HSV\n    - 平均数、中位数、众数、标准差，最大最小值\n- （过时）特征检测模块（准备用自学习检测取代）\n    - 模板匹配、多模板匹配、特征点检测 https://book.openmv.cc/image/keypoints.html\n- 颜色追踪模块\n    - 颜色、线条、形状同时识别 https://book.openmv.cc/image/blob_circle.html\n- 标记追踪模块\n    - 条形码、二维码扫码与定位 https://book.openmv.cc/image/code.html\n    - AprilTag标记跟踪 测距 https://book.openmv.cc/image/ranging.html\n- 图像滤波模块 https://book.openmv.cc/example/04-Image-Filters/adaptive-histogram-equalization.html\n    - 各种图像滤波、边缘滤波、腐蚀膨胀\n    - 锐化、取反、二值化、直方图均衡、旋转校正\n- （过时）高级特征 HAAR HOG LBP （方法）\n    - 人脸、眼球、梯度方向和强度的统\n\n### 准备工作、如何协作、如何提交？\n\n环境、代码、功能、接口\n\n首先在开始硬件开发之前，我们需要准备好 ubuntu20 系统，要确保能够访问摄像头方便测试。\n- [github.com/sipeed/maixpy3](https://github.com/sipeed/maixpy3)\n- [github.com/sipeed/libmaix](https://github.com/sipeed/libmaix)\n\n请准备上述两个模块，测试和开发的同学都需要，请通过 git clone 获取上述仓库。\n拉取子模块 'git submodule update --init --recursive'\n\n### 这里有自动化编译的命令给你们参考如何编译a。\n#### 准备工作需要做什么？\n\nMaixPy3 文档 https://github.com/sipeed/MaixPy3/blob/develop/docs/develop.md\nMaixPy3 编译流程 https://github.com/sipeed/MaixPy3/blob/release/.github/workflows/maixpy3_build.yml#L19-L34\nlibmaix 文档 https://github.com/sipeed/libmaix/blob/develop/README_ZH.md\nlibmaix 编译流程 https://github.com/sipeed/libmaix/blob/develop/.github/workflows/test_build.yml#L26-L36\n\n### MaixPy3 编译安装的结果（不要在 jupyter notebook 里写死循环！）\n了解本文 https://wiki.sipeed.com/soft/maixpy3/zh/install/install.html 完成 maixpy3 的安装与测试。\n\n测试人员需要会写 markdown 并能够安装 MaixPy3 并运行测试，会用 jupyter 记录运行结果，能够一步步测试给出反馈和结果，如下图。\n![](./../assets/opencv/develop_2.jpg)\n### libmaix 编译安装的结果\n\n开发人员需要能够编译 libmaix 和 MaixPy3 的运行、调试、打包，学会编译运行最基础的 camera 项目即可，知道如何使用 libmaix 的项目就行，测试人员不需要了解 libmaix 的编译与测试。\n```shell\ncd ~/MaixPy3/ext_modules/libmaix/examples/imlib_test\npython3 project.py --toolchain /usr/bin --toolchain-prefix x86_64-linux-gnu- config\npython3 project.py menuconfig\n```\n![](./../assets/opencv/develop_3.jpg)\n![](./../assets/opencv/develop_4.jpg)\n![](./../assets/opencv/develop_5.jpg)\n确认启用了模块。\n![](./../assets/opencv/develop_6.jpg)\n![](./../assets/opencv/develop_7.jpg)\n编译 python3 project.py build 出来\n![](./../assets/opencv/develop_8.jpg)\n然后直接 ./dist/start_app.sh 就行，我的示例代码演示了 find_line & rotation_corr 。\n![](./../assets/opencv/develop_9.jpg)\n这表示你具备了本机的开发环境，最起码的开发和调试环境，它会调用默认的 /dev/video0 设备显示并使用，如上图所示。\n#### 提交内容都有什么？如何提交你的成果？\n验收方式怎么做？测试怎么设计？用户怎么使用？\n最终测试人员要提供的内容有一份 jupyter notebook 文档（含测试用例、示例代码，测试报告说明），最终会以作者署名的方式提交到 wiki.sipeed.com 文档中，请参考我们其他文档是怎么写的 。\n最终开发人员要提供属于自己的提交和署名，如何 PR 提交等如何使用开发、测试、发布分支，包含独立的测试代码和自己的开发工程，从 imlib_test 拷贝出来即可。\n会议纪要：\n- 需要确认人员和准备工作（已经标黄，其他同学没有参加会议对请确认准备工作对内容后，自行标黄表示已得知，之后我会在这些同学里进行了解和分配）\n- 需要确认项目内容与全员同步（请看功能表，请继续补充，直到我们所有人准备好的时候，进行第一次合作开发，提前准备好的同学直接和我说就行）\n- 需要确认各自的定位和分工（开发和测试对内容应该都清楚了，准备好各自对要点就行，我们挨个挨个对，先收集所有必备对功能点）\n- 需要确认项目的提交与合并方式（使用 github 进行切到开发 develop 分支提交代码完成功能后最后统一 pr 进入 future 分支，最终合并回 release 分支完成最终的提交和代码审计）\n\n## 第二次会议 2022-04-02 （准备中，计划 9 点左右）\n\n在准备阶段，确保开发准备好开发环境（可电脑运行），测试准备好测试环境（可导出文档），最后准备完毕后继续。\n\n可以在节假日的时候进行人员的确认和分工，以及参与人员的统计，确定本次开发的功能列表以及进度表，预计完成时间，由我牵头开发的同学一起完成，如果出现人员特殊意外均由官方人员兜底，不会影响整体进度，如果其他同学因为事情搁置则提前移交任务出来给其他人继续完成，比如@Juwan 可以保证所有功能没有人处理的时候亲自处理。\n\n在下一次会议来之前，需要统计好需要增加的功能，进而准备 API 设计和测试文档，测试即文档，可见下文。\n\n### MaixPy3 image 现有功能表\n```shell\n>>> t = camera.capture()\n>>> t.\nt.c_img_private(          t.draw_string(            t.new(\nt.clear(                  t.find_ball_color(        t.open(\nt.convert(                t.find_blob_lab(          t.resize(\nt.copy(                   t.find_blobs(             t.rotate(\nt.crop(                   t.find_circles_blob(      t.save(\nt.custom_find_ball_blob(  t.find_line(              t.set_pixel(\nt.delete(                 t.get_blob_color(         t.show(\nt.draw_circle(            t.get_blob_lab(           t.size\nt.draw_ellipse(           t.get_pixel(              t.tobytes(\nt.draw_image(             t.height                  t.width\nt.draw_line(              t.load(\nt.draw_rectangle(         t.mode\n>>>\n```\nimage 模块：\n- 属性查询 API ：width 、height 、mode、size\n- 文件读写 API ：new 、load、open 、 save、delete\n- 图像处理 API ：tobytes、copy、crop、get_pixel、set_pixel、draw_xxxx、get_blob_color、rotate、clear、convert、show\n- 视觉算法 API ：find_xxxx、\n\n预期增强的功能表（源自于第一次会议的草稿，可随时添加更多功能，取决于有没有必要）\n- 获取指定区域图像统计信息 https://book.openmv.cc/image/statistics.html\n    - 颜色空间处理，如 LAB RGB HSV\n    - 平均数、中位数、众数、标准差，最大值、最小值\n- 颜色追踪模块\n    - 颜色、线条、线段、方块、形状同时识别 https://book.openmv.cc/image/blob_circle.html\n    - find_rects\n    - find_lines\n    - find_circles\n    - find_line_segments\n- 标记追踪模块\n    - 条形码、二维码扫码与定位 https://book.openmv.cc/image/code.html\n    - AprilTag标记跟踪 测距 https://book.openmv.cc/image/ranging.html\n    - find_barcodes\n    - find_datamatrices\n    - find_apriltags_3d_pose 3D定位\n    - find_apriltags AprilTag识别\n- 图像滤波模块 https://book.openmv.cc/example/04-Image-Filters/adaptive-histogram-equalization.html\n    - 各种图像滤波、边缘滤波、腐蚀膨胀\n    - 锐化、取反、二值化、直方图均衡、旋转校正\n\n### 沟通记录\n- @Juwan 封装接口的开发样例 https://github.com/sipeed/MaixPy3/commit/3ac83ef6a479ceafce288d0b2a2378a92f15a286 建议每次都重新编译安装 python3 setup.py clean --all build && python3 install . 确保代码有更新，最终效果如下：\n- 分别测试了一下沿 X 轴 和 Z 轴旋转了一下。\n\n![](./../assets/opencv/develop_10.jpg)\n![](./../assets/opencv/develop_11.jpg)\n\n```python\nfrom maix import image\nt = image.new((240, 240), color=(255, 0, 0))\nprint(t)\n\nt.draw_string(120, 20, \"hello\")\nt.draw_circle(120, 120, 60, color=(0, 250, 0), thickness = -1)\n\nt.imlib_rotation_corr(0.3)\n\nfrom maix import display\ndisplay.show(t)\n\nimport time\ntime.sleep(2)\n```\n\n- 测试文档示范可以从 wiki 里的这些 jupyter note 文档可以参考 API 设计和具体的测试。\nhttps://github.com/sipeed/sipeed_wiki/tree/main/docs/soft/maixpy3/zh/usage/vision\n- 举例来说是这样的\nhttps://github.com/sipeed/sipeed_wiki/blob/main/docs/soft/maixpy3/zh/usage/vision/maixpy3-example.ipynb\n- 对应的效果在这里\nhttps://wiki.sipeed.com/soft/maixpy3/zh/usage/vision/maixpy3-example.html\n\n会议纪要：\n暂无"}, "/soft/maixpy3/zh/develop/maix_speech.html": {"title": "", "content": "## maix-speech\n\n开发完成时间： maixpy3 0.3.6\n\n同步支持 V831 & R329\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=465855870&bvid=BV1B5411f7wR&cid=487613553&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>\n\n## 进度\n\n- 至今模型未公开，代码已合并 https://github.com/sipeed/MaixPy3/tree/master/ext_modules/_maix_speech 。"}, "/soft/maixpy3/zh/develop/r329_yolo.html": {"title": "", "content": "## R329 yolo 模型\n\n开发完成时间： maixpy3 0.4.0\n\n同步支持 V831 & R329\n\n## 进度\n\n- 至今模型未公开，代码已提交。\n\n## 相关视频\n\n<iframe src=\"//player.bilibili.com/player.html?aid=680815271&bvid=BV1kS4y157av&cid=487529883&page=1\" scrolling=\"no\" border=\"0\" frameborder=\"no\" framespacing=\"0\" allowfullscreen=\"true\"> </iframe>"}, "/soft/maixpy3/zh/develop/Edge_detection.html": {"title": "边缘检测", "content": "# 边缘检测\n\n> 2022年01月11日 以下代码由于 MaixPy3 还在施工中，此处代码仅供参考和示范，功能已在 github 和 社区供其他同学使用和参考。\n\nPytorch 使用模型(卷积/conv)实现 sobel(索贝尔) 边缘检测实现源码\n\n> 目前只能在 V831 上进行部署使用，R329 后续会更新上来\n\n## 边缘检测效果\n![](./../asserts/test.jpg)\n![](./../asserts/sobel_edge2.jpg)\n![](./../asserts/final.jpg)\n![](./../asserts/sobel_edge.jpg)\n![](./../asserts/sobel_v831.jpg)\n\n\n源码在末尾\n\n## 边缘检测原理\n边缘就是值变化剧烈的地方, 如果对值的变化求导, 则边缘部分就是导数局部最大.\n但是在图像处理时没有具体的函数让我们求导, 使用卷积运算则可以很好的近似替代\n\n如下图, 假设左上为坐标原点, 横轴为 `x`, 纵轴为`y`, 如下图左上角9个像素点, `P(x, y)`表示坐标`(x, y)`的点, 要求`P(1, 1)`处在x轴的变化率, 则只需将`P(2, 1) - P(0, 1)` 得到值为`0`, `P(1, 0)`处为`1-3 = -2,` 这个差值即变化率, 类比成导数, 我们就能知道横轴在哪些地方变化率更大.\n![](./../asserts/conv.jpg)\n上面这种方法我们可以得到横轴的变化率, 这里使用卷积核\n```\n[-1, 0, 1],\n[-2, 0, 2],\n[-1, 0, 1]\n```\n对图像进行卷积运算, 如图中的计算方法, 像素点左右权值取2, 角上的也参与计算,但是权值为1,没有左右边的权值高. 这样我们就得到了横轴的变化率图, 即边缘检测图.\n\n注意, 这里是对横轴计算了, 比较的点左右的值变化, 所以实际看到的图像会出现明显的纵轴边缘, 如下图左边\n![](./../asserts/vertical_horizontal.jpg)\n同理, 上图右边的图使用卷积核\n\n[1,2,1],\n[0,0,0],\n[-1, -2, -1]\n得到的纵轴的边缘图.\n\n注意这里用右边减左边, 如果右边的值比左边的小会是负数, 如果我们希望只检测颜色值变大(变白)则可以直接使用, 如果两个变化方向都要检测, 则可以取绝对值. 比如下图左边是没有取绝对值, 右边取了绝对值\n![](./../asserts/without_with_abs.jpg)\n得到两个方向的图后, 对其进行合并, 对每个像素平方和开方即可\n![](./../asserts/final.jpg)\n这张图左边是使用 GIMP 的 sobel 边缘检测(垂直+水平)的效果, 略微有点不同:\n![](./../asserts/sobel_edge2.jpg)\n不同的原因是使用水平和垂直的图平方和开根后, 直接用 `plt.imshow` 显示, 和 GIMP 的处理方式不同\n```python\nout = np.sqrt(np.square(out_v) + np.square(out_h))\nplt.imshow(out)\n```\n简单地直接将值规范到`[0, 255]`就和 GIMP 的图相似了(但不完全一样)\n```python\nout = np.sqrt(np.square(out_v) + np.square(out_h))\nout = out * 255.0 / out.max()\nplt.imshow(out.astype(np.uint8))\n```\n![](./../asserts/sobel_v_h.jpg)\n## 自定义卷积核来实现边缘检测\n除了上面说了使用两次卷积计算, 也可以用只计算一次的卷积核, 比如:\n```bash\n[-1, -1, -1],\n[ -1, 8, -1],\n[ -1, -1, -1]\n```\n这是对于一个通道(灰度图)来说, 如果要扩充到三个通道(RGB), 卷积核参数就是如下形式\n```bash\nconv_rgb_core_sobel = [\n                        [[-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                        ]]\n```\n经过卷积运算后, 前后图如下:\n\n![](./../asserts/sobel_edge.jpg)\n\n注意, 输入值范围如果为`[0, 255]`, 输出值则范围会变化, 以图片形式查看时需要注意加以处理, 这里使用了`plt.imshow(out)`来显示, 这个函数会自动对图像做简单的处理, 才会看起来是黑色背景\n\n## 导出成模型使用\n可以将 Net 导出成 onnx 即可在其它平台使用, 就是一个简单的卷积层\n\n部署到 V831 后的样子(使用了卷积核`[-1,-1,-1],[-1,8,-1], [-1,-1,-1],`):\n\n![](./../asserts/sobel_v831.jpg)\n\nV831 部署[源码](https://github.com/sipeed/MaixPy3/blob/master/ext_modules/_maix_nn/example/load_forward_sobel_edge_camera.py)在 github， 模型在 maixhub 上可以下载\n\n## 边缘检测源码\n\n> 这是在电脑上运行的代码，不是在开发板平台上运行的代码\n\n```python\n\n'''\n    simple sobel edge demo\n    visit: https://neucrack.com/p/377\n    @author neucrack\n    @license MIT\n'''\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 3, 3, padding=(0, 0), bias=False)\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\nnet = Net()\nconv_rgb_core_original = [\n                        [[0,0,0],[0,1,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,1,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,1,0], [0,0,0]\n                        ]]\nconv_rgb_core_sobel = [\n                        [[-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [-1,-1,-1],[-1,8,-1], [-1,    -1,    -1],\n                        ]]\nconv_rgb_core_sobel_vertical = [\n                        [[-1,0,1],[-2,0,2], [-1,    0,    1],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [-1,0,1],[-2,0,2], [-1,    0,    1],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [-1,0,1],[-2,0,2], [-1,    0,    1],\n                        ]]\nconv_rgb_core_sobel_horizontal = [\n                        [[1,2,1],[0,0,0], [-1, -2, -1],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [1,2,1],[0,0,0], [-1, -2, -1],\n                         [0,0,0],[0,0,0], [0,0,0]\n                        ],\n                        [[0,0,0],[0,0,0], [0,0,0],\n                         [0,0,0],[0,0,0], [0,0,0],\n                         [1,2,1],[0,0,0], [-1, -2, -1],\n                        ]]\ndef sobel(net, kernel):\n    sobel_kernel = np.array(kernel,    dtype='float32')\n    sobel_kernel = sobel_kernel.reshape((3,    3,    3,    3))\n    net.conv1.weight.data = torch.from_numpy(sobel_kernel)\nparams = list(net.parameters())\nimg = cv2.imread(\"out/test.jpg\")\ninput_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\ninput_tensor = (input_img.astype(np.float32) - 127.5) / 128 # to [-1, 1]\ninput_tensor = torch.Tensor(input_tensor).permute((2, 0, 1))\nprint(input_tensor.shape)\ninput_tensor = input_tensor.unsqueeze(0)\nprint(\"input shape:\", input_tensor.shape)\nsobel(net, conv_rgb_core_sobel)\nout = net(input_tensor).detach().numpy()[0].transpose([1,2,0])\nsobel(net, conv_rgb_core_sobel_vertical)\nout_v = net(input_tensor).detach().numpy()[0].transpose([1,2,0])\nsobel(net, conv_rgb_core_sobel_horizontal)\nout_h = net(input_tensor).detach().numpy()[0].transpose([1,2,0])\nprint(\"out shape: {}, tensor:{}\".format(out.shape, out))\nprint(out.shape, out.max(), out.min())\nplt.figure()\nplt.figure()\nplt.subplot(1, 5, 1)\ninput = input_tensor.numpy()[0].transpose((1,2,0))\nprint(input.max(), input.min())\nplt.imshow(input_img)\nplt.subplot(1, 5, 2)\nprint(out.max(), out.min())\n# out = np.sqrt(np.square(out))\n# out = out * 255.0 / out.max()\n# out = out.astype(np.uint8)\n# print(out.max(), out.min())\nplt.imshow(out)\nplt.subplot(1, 5, 3)\nout = np.abs(out_v)\n# out = out * 255.0 / out.max()\n# plt.imshow(out.astype(np.uint8))\nplt.imshow(out)\nplt.subplot(1, 5, 4)\nout = np.abs(out_h)\n# out = out * 255.0 / out.max()\n# plt.imshow(out.astype(np.uint8))\nplt.imshow(out)\nplt.subplot(1, 5, 5)\nout = np.sqrt(np.square(out_v) + np.square(out_h))\n# out = out * 255.0 / out.max()\n# plt.imshow(out.astype(np.uint8))\nplt.imshow(out)\nplt.show()\n```\n\n## 参考\n- [How to implement Sobel edge detection using Python from scratch](http://www.adeveloperdiary.com/data-science/computer-vision/how-to-implement-sobel-edge-detection-using-python-from-scratch/)\n- [梯度和Sobel导数](https://blog.csdn.net/lzhf1122/article/details/71752644)\n\n> 以上内容出至于：<https://neucrack.com/p/377>"}, "/soft/maixpy3/zh/usage/AI_net/yolo.html": {"title": "物品检测", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2021年12月2日|Rui|初次编写文档|----||2022年12月15日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0||2022年3月18日|Rui|在MaixSense上使用测试通过|需要将MaixPy3更新到0.4.5以上||2022年3月29日|Rui|添加MaixII-DockYolo-V220分类|模型及代码下载：[Github下载](https://github.com/Abandon-ht/Maix-II-Dock-model.git)<br>作者：小老鼠|物品检测，即目标检测，简单的来说就是可以框选出画面中的目标物体并输出坐标位置。通过不同的数据集来实现不同的目标识别，以下分别使用人脸数据集和Yolo标准数据集进行训练得出的目标检测模型。##运行效果<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=720490332&bvid=BV15Q4y1r7HV&cid=405128041&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##准备-使用的硬件为MaixII-Dock或MaixSense-获取模型文件，`MaixII-Dock`可以在[MaixHub](https://maix.sipeed.com/model/zoo/71)上获取或者烧录最新版本的系统镜像；`MaixSense`则是需要烧录最新的armbian系统镜像到内存卡上。-确认MaixPy3版本为0.4.3以上-插卡启动硬件##在MaixII-Dock上部署目标检测2022年后系统内置了模型文件`/home/model/face/yolo2_face_awnn.*`）###部署人脸目标检测网络模型classYolo:labels=[\"person\"]anchors=[1.19,1.98,2.79,4.59,4.53,8.92,8.06,5.29,10.32,10.65]m={\"param\":\"/home/model/face/yolo2_face_awnn.param\",\"bin\":\"/home/model/face/yolo2_face_awnn.bin\"}options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"output0\":(7,7,(1+4+len(labels))*5)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],}def__init__(self):frommaiximportnnfrommaix.nnimportdecoderself.model=nn.load(self.m,opt=self.options)self.decoder=decoder.Yolo2(len(self.labels),self.anchors,net_in_size=(224,224),net_out_size=(7,7))def__del__(self):delself.modeldelself.decoderprint(Yolo)[ rpyc-kernel ]( running at Thu Jan 20 13:46:59 2022 )\n<rpyc.core.protocol.Yolo object at 0xd8d7f8>###运行网络模型，进行目标检测上面已经将模型文件加载部署到系统中，下面只需要对模型文件进行解码，获取目标在画面中的位置并框选出来frommaiximportcamera,displayyolo=Yolo()print(yolo)whileTrue:img=camera.capture().resize(224,224)out=yolo.model.forward(img,quantize=True,layout=\"hwc\")boxes,probs=yolo.decoder.run(out,nms=0.3,threshold=0.5,img_size=(224,224))iflen(boxes):fori,boxinenumerate(boxes):img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],(255,0,0),1)display.show(img)else:display.show(img)##在MaixSense上部署目标检测可以用于识别labels中的物品类型。>最新版本的Armbian系统已经将模型内置到了`/home/model/`中，Tina系统目前不提供支持。###部署网络模型classYolo:labels=[\"aeroplane\",\"bicycle\",\"bird\",\"boat\",\"bottle\",\"bus\",\"car\",\"cat\",\"chair\",\"cow\",\"diningtable\",\"dog\",\"horse\",\"motorbike\",\"person\",\"pottedplant\",\"sheep\",\"sofa\",\"train\",\"tvmonitor\"]anchors=[0.4165,0.693,0.9765,1.6065,1.5855,3.122,2.821,1.8515,3.612,3.7275]m={\"bin\":\"/home/model/aipu_yolo_VOC2007.bin\"}options={\"model_type\":\"aipu\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"output0\":(7,7,(1+4+len(labels))*5)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],\"scale\":[8.031941],}def__init__(self):frommaiximportnnfrommaix.nnimportdecoderself.model=nn.load(self.m,opt=self.options)self.decoder=decoder.Yolo2(len(self.labels),self.anchors,net_in_size=(224,224),net_out_size=(7,7))def__del__(self):delself.modeldelself.decoderprint(Yolo)###运行网络模型，进行目标检测上面已经将模型文件加载部署到系统中，下面只需要对模型文件进行解码，获取目标在画面中的位置并框选出来frommaiximportcamera,displayyolo=Yolo()print(yolo)whileTrue:img=camera.capture().resize(224,224)out=yolo.model.forward(img,quantize=True,layout=\"chw\")boxes,probs=yolo.decoder.run(out,nms=0.5,threshold=0.5,img_size=(224,224))iflen(boxes):fori,boxinenumerate(boxes):class_id=probs[i][0]prob=probs[i][1][class_id]disp_str=\"{}:{:.2f}%\".format(yolo.labels[class_id],prob*100)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],(255,0,0),1)img.draw_string(box[0],box[1]+box[3],disp_str,scale=0.5,color=(0,0,255),thickness=1)display.show(img)else:display.show(img)##网络模型训练查看左边目录中的【训练AI模型】学习如何训练属于自己的目标检测模型"}, "/soft/maixpy3/zh/usage/AI_net/number_recognize.html": {"title": "数字识别", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|----||2022年1月18日|dianjixz|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月19日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0|>内容参考至Neutree的博客[使用V831AI检测数字卡片](https://neucrack.com/p/384)##背景数字识别是2021年电赛F题**智能送药小车**，节选题目部分内容![](./../asserts/number_1.jpg)**识别的数字**为：![](./../asserts/number.jpg)##运行效果<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=678372862&bvid=BV18m4y1S7qk&cid=488266454&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##准备-在[MaixHub](https://maix.sipeed.com/model/zoo/66)上获取模型文件，并将模型文件存放到U盘中-确认MaixPy3版本为0.4.0以上-使用的硬件为MaixII-Dock-内存卡内是最新版本的镜像系统-插卡启动硬件##数字识别将模型读取到python环境中！classNumber_recognition:labels=[\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"]anchors=[2.44,2.25,5.03,4.91,3.5,3.53,4.16,3.94,2.97,2.84]model={\"param\":\"/root/number_awnn.param\",\"bin\":\"/root/number_awnn.bin\"}options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"output0\":(7,7,(1+4+len(labels))*5)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],}w=options[\"inputs\"][\"input0\"][1]h=options[\"inputs\"][\"input0\"][0]def__init__(self):frommaiximportnnfrommaix.nnimportdecoderself.m=nn.load(self.model,opt=self.options)self.yolo2_decoder=decoder.Yolo2(len(self.labels),self.anchors,net_in_size=(self.w,self.h),net_out_size=(7,7))defmap_face(self,box):#将224*224空间的位置转换到240*240空间内deftran(x):returnint(x/224*240)box=list(map(tran,box))returnboxprint(Number_recognition)[ rpyc-kernel ]( running at Wed Jan 19 19:32:13 2022 )\ninit over开始数字识别frommaiximportcamera,display,imagenumber_recognition=Number_recognition()whileTrue:img=camera.capture()AI_img=img.copy().resize(224,224)out=number_recognition.m.forward(AI_img.tobytes(),quantize=True,layout=\"hwc\")boxes,probs=number_recognition.yolo2_decoder.run(out,nms=0.3,threshold=0.5,img_size=(240,240))fori,boxinenumerate(boxes):class_id=probs[i][0]prob=probs[i][1][class_id]disp_str=\"{}:{:.2f}%\".format(number_recognition.labels[class_id],prob*100)font_wh=image.get_string_size(disp_str)box=number_recognition.map_face(box)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],color=(255,0,0),thickness=2)img.draw_rectangle(box[0],box[1]-font_wh[1],box[0]+font_wh[0],box[1],color=(255,0,0))img.draw_string(box[0],box[1]-font_wh[1],disp_str,color=(255,0,0))display.show(img)"}, "/soft/maixpy3/zh/usage/AI_net/self_learn.html": {"title": "自学习", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|----||2022年1月8日|dianjixz|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0|##运行效果<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=297876630&bvid=BV1aF41137Ff&cid=568622764&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##准备-在[MaixHub](https://maix.sipeed.com/model/zoo/58)上获取模型文件和运行源码（最新版本的系统内置了模型文件`/home/model/resnet18_1000_awnn*`）-确认MaixPy3版本为0.4.3以上-使用的硬件为MaixII-Dock-内存卡内是最新版本的镜像系统-插卡启动硬件##自学习训练与保存将模型读取到python环境中！frommaiximportnnfrommaiximportcamera,displayimporttimeclassSelf_learn:model={\"param\":\"/home/model/resnet18_1000_awnn.param\",\"bin\":\"/home/model/resnet18_1000_awnn.bin\"}options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"190\":(1,1,512)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0176,0.0176,0.0176],}class_num=3#学习类别sample_num=15#学习类别总数量curr_class=0curr_sample=0def__init__(self):frommaiximportnnfrommaix.nn.app.classifierimportClassifierprint(\"--loadmodel:\",self.model)self.m=nn.load(self.model,opt=self.options)print(\"--loadok\")print(\"--loadclassifier\")self.classifier=Classifier(self.m,self.class_num,self.sample_num,512,224,224)print(\"--loadok\")[ rpyc-kernel ]( running at Thu Jan 20 09:45:15 2022 )\n-- load model: {'param': '/home/model/resnet18_1000_awnn.param', 'bin': '/home/model/resnet18_1000_awnn.bin'}\n-- load ok\n-- load classifier\n-- load ok###添加分类的类别通过摄像头拍摄物体2秒来进行添加，当画面卡住不动的时候，就需要移动到下一个类别等待摄像头启动，进行添加>一下代码只能添加3个类别，模型是没有上限的importtimefrommaiximportcamera,displayself_learn=Self_learn()forxinrange(3):t=time.time()whileTrue:if(time.time()-t)>2:img=camera.capture().resize(224,224)self_learn.classifier.add_class_img(img)display.show(img)time.sleep(2)print(\"addok!\")breakimg=camera.capture()display.show(img)add ok!###制作模型训练的数据集运行代码之后，通过每个物品给摄像头拍摄2秒，当画面卡住不动的时候就可以移动下一个物品，进行拍摄数据集importtimefrommaiximportnnfrommaiximportcamera,displayforxinrange(3):t=time.time()whileTrue:if(time.time()-t)>2:foriinrange(5):img=camera.capture().resize(224,224)self_learn.classifier.add_sample_img(img)display.show(img)time.sleep(2)breakdisplay.show(camera.capture())进行图片自学习self_learn.classifier.train()print(\"trainover\")[ rpyc-kernel ]( running at Thu Jan 20 09:31:48 2022 )训练结束后保存模型self_learn.classifier.save(\"./module.bin\")print(\"saveover\")[ rpyc-kernel ]( running at Wed Jan 19 15:43:36 2022 )开始进行分类验证importtimefrommaiximportnnfrommaiximportcamera,displaywhileTrue:img=camera.capture()AI_img=img.copy().resize(224,224)idx,distance=self_learn.classifier.predict(AI_img)msg=\"predictclass:\"+str(idx+1)+\",conf:\"+str(100-distance)print(msg)img.draw_string(10,10,msg,color=(255,0,0))display.show(img)predict class: 2, conf: 88##读取自学习模型classSelf_learn:model={\"param\":\"/home/model/resnet18_1000_awnn.param\",\"bin\":\"/home/model/resnet18_1000_awnn.bin\"}options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"190\":(1,1,512)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0176,0.0176,0.0176],}class_num=3#学习类别sample_num=15#学习类别总数量curr_class=0curr_sample=0def__init__(self):frommaiximportnnfrommaix.nn.app.classifierimportClassifierfrommaix.nn.app.classifierimportloadimportos.pathifos.path.isfile(\"./module.bin\"):print(\"--loadmodel:\",self.model)self.m=nn.load(self.model,opt=self.options)print(\"--loadok\")print(\"--loadclassifier\")self.classifier=load(self.m,\"./module.bin\")print(\"--loadok\")else:print(\"nothavemodel!\")print(\"pleaserunnn_self_learn_classifier.pygetmodel!\")print(Self_learn)[ rpyc-kernel ]( running at Wed Jan 19 15:44:46 2022 )\n-- load model: {'param': '/home/model/resnet18_1000_awnn.param', 'bin': '/home/model/resnet18_1000_awnn.bin'}\n-- load ok\n-- load classifier\n-- load ok自学习预测importtimefrommaiximportcamera,displayself_learn=Self_learn()whileTrue:img=camera.capture()AI_img=img.copy().resize(224,224)idx,distance=self_learn.classifier.predict(AI_img)msg=\"predictclass:\"+str(idx+1)+\",conf:\"+str(100-distance)#print(msg)img.draw_string(10,10,msg,color=(255,0,0))display.show(img)"}, "/soft/maixpy3/zh/usage/AI_net/face_recognize.html": {"title": "人脸识别", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|----||2022年1月8日|dianjixz|修改文档的编写方式<br>调整运行逻辑|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0|##运行效果<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=714915927&bvid=BV1MX4y1g7cE&cid=321380350&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##准备-在[MaixHub](https://maix.sipeed.com/model/zoo/68)上获取模型文件和运行源码（最新版本的系统内置了模型文件`/home/model/face_recognize/*`）-确认MaixPy3版本为0.4.0以上-使用的硬件为MaixII-Dock-内存卡内是最新版本的镜像系统-插卡启动硬件##人脸识别###读取模型文件并部署这里是最多同时检测3个人脸，需要识别更多人脸可以运行[V831的人脸识别](https://wiki.sipeed.com/news/MaixPy3/key_face_recognize.html)中的完整代码（带有按键控制的喔！）classFace_recognize:score_threshold=70#识别分数阈值input_size=(224,224,3)#输入图片尺寸input_size_fe=(128,128,3)#输入人脸数据feature_len=256#人脸数据宽度steps=[8,16,32]#channel_num=0#通道数量users=[]#初始化用户列表threshold=0.5#人脸阈值nms=0.3max_face_num=3#输出的画面中的人脸的最大个数names=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\"]#人脸标签定义model={\"param\":\"/home/model/face_recognize/model_int8.param\",\"bin\":\"/home/model/face_recognize/model_int8.bin\"}model_fe={\"param\":\"/home/model/face_recognize/fe_res18_117.param\",\"bin\":\"/home/model/face_recognize/fe_res18_117.bin\"}def__init__(self):frommaiximportnnfrommaix.nn.app.faceimportFaceRecognizeforiinrange(len(self.steps)):self.channel_num+=self.input_size[1]/self.steps[i]*(self.input_size[0]/self.steps[i])*2self.channel_num=int(self.channel_num)#统计通道数量self.options={#准备人脸输出参数\"model_type\":\"awnn\",\"inputs\":{\"input0\":self.input_size},\"outputs\":{\"output0\":(1,4,self.channel_num),\"431\":(1,2,self.channel_num),\"output2\":(1,10,self.channel_num)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],}self.options_fe={#准备特征提取参数\"model_type\":\"awnn\",\"inputs\":{\"inputs_blob\":self.input_size_fe},\"outputs\":{\"FC_blob\":(1,1,self.feature_len)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],}print(\"--loadmodel:\",self.model)self.m=nn.load(self.model,opt=self.options)print(\"--loadok\")print(\"--loadmodel:\",self.model_fe)self.m_fe=nn.load(self.model_fe,opt=self.options_fe)print(\"--loadok\")self.face_recognizer=FaceRecognize(self.m,self.m_fe,self.feature_len,self.input_size,self.threshold,self.nms,self.max_face_num)defmap_face(self,box,points):#将224*224空间的位置转换到240*240或320*240空间内frommaiximportdisplay#print(box,points)ifdisplay.width()==display.height():deftran(x):returnint(x/224*display.width())box=list(map(tran,box))deftran_p(p):returnlist(map(tran,p))points=list(map(tran_p,points))else:#168x224(320x240)>224x224(240x240)>320x240s=(224*display.height()/display.width())#168x224w,h,c=display.width()/224,display.height()/224,224/st,d=c*h,(224-s)//2#d=224-s//2==28box[0],box[1],box[2],box[3]=int(box[0]*w),int((box[1]-28)*t),int(box[2]*w),int((box[3])*t)deftran_p(p):return[int(p[0]*w),int((p[1]-d)*t)]#224-168/2=28so168/(old_h-28)=240/new_hpoints=list(map(tran_p,points))#print(box,points)returnbox,pointsdefrecognize(self,feature):#进行人脸匹配def_compare(user):#定义映射函数returnself.face_recognizer.compare(user,feature)#推测匹配分数score相关分数face_score_l=list(map(_compare,self.users))#映射特征数据在记录中的比对分数returnmax(enumerate(face_score_l),key=lambdax:x[-1])#提取出人脸分数最大值和最大值所在的位置def__del__(self):delself.face_recognizerdelself.m_fedelself.mprint(Face_recognize)[ rpyc-kernel ]( running at Thu Jan 20 14:20:31 2022 )\n-- load model: {'param': '/home/model/face_recognize/model_int8.param', 'bin': '/home/model/face_recognize/model_int8.bin'}\n-- load ok\n-- load model: {'param': '/home/model/face_recognize/fe_res18_117.param', 'bin': '/home/model/face_recognize/fe_res18_117.bin'}\n-- load ok###寻找人脸需要先确定，可以检测到对应需要识别的人脸frommaiximportcamera,image,displayface_recognizer=Face_recognize()whileTrue:img=camera.capture()#获取224*224*3的图像数据AI_img=img.copy().resize(224,224)faces=face_recognizer.face_recognizer.get_faces(AI_img.tobytes(),False)#提取人脸特征信息iffaces:forprob,box,landmarks,featureinfaces:disp_str=\"Unmarkedface\"bg_color=(255,0,0)font_color=(255,255,255)box,points=face_recognizer.map_face(box,landmarks)font_wh=image.get_string_size(disp_str)forpinpoints:img.draw_rectangle(p[0]-1,p[1]-1,p[0]+1,p[1]+1,color=bg_color)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],color=bg_color,thickness=2)img.draw_rectangle(box[0],box[1]-font_wh[1],box[0]+font_wh[0],box[1],color=bg_color,thickness=-1)img.draw_string(box[0],box[1]-font_wh[1],disp_str,color=font_color)display.show(img)###添加一张人脸运行代码之后，会直接添加检测到的所有人脸，需要单独的去添加人脸的信息，添加到人脸信息就会自动停止运行frommaiximportcamera,image,displayface_flage=1whileface_flage:img=camera.capture()#获取224*224*3的图像数据AI_img=img.copy().resize(224,224)faces=face_recognizer.face_recognizer.get_faces(AI_img.tobytes(),False)#提取人脸特征信息iffaces:forprob,box,landmarks,featureinfaces:iflen(face_recognizer.users)<len(face_recognizer.names):face_recognizer.users.append(feature)face_flage=0else:print(\"userfull\")disp_str=\"addface\"bg_color=(0,255,0)font_color=(0,0,255)box,points=face_recognizer.map_face(box,landmarks)font_wh=image.get_string_size(disp_str)forpinpoints:img.draw_rectangle(p[0]-1,p[1]-1,p[0]+1,p[1]+1,color=bg_color)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],color=bg_color,thickness=2)img.draw_rectangle(box[0],box[1]-font_wh[1],box[0]+font_wh[0],box[1],color=bg_color,thickness=-1)img.draw_string(box[0],box[1]-font_wh[1],disp_str,color=font_color)display.show(img)###识别人脸这时会将所识别的人脸和所添加的人脸信息进行对比，将对比得到相似度最高的人脸识别的正确人脸并用绿色框标记出来。frommaiximportcamera,image,displaywhileTrue:img=camera.capture()#获取224*224*3的图像数据AI_img=img.copy().resize(224,224)faces=face_recognizer.face_recognizer.get_faces(AI_img.tobytes(),False)#提取人脸特征信息iffaces:forprob,box,landmarks,featureinfaces:iflen(face_recognizer.users):#判断是否记录人脸maxIndex=face_recognizer.recognize(feature)ifmaxIndex[1]>face_recognizer.score_threshold:#判断人脸识别阈值,当分数大于阈值时认为是同一张脸,当分数小于阈值时认为是相似脸disp_str=\"{}\".format(face_recognizer.names[maxIndex[0]])bg_color=(0,255,0)font_color=(0,0,255)box,points=face_recognizer.map_face(box,landmarks)font_wh=image.get_string_size(disp_str)forpinpoints:img.draw_rectangle(p[0]-1,p[1]-1,p[0]+1,p[1]+1,color=bg_color)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],color=bg_color,thickness=2)img.draw_rectangle(box[0],box[1]-font_wh[1],box[0]+font_wh[0],box[1],color=bg_color,thickness=-1)img.draw_string(box[0],box[1]-font_wh[1],disp_str,color=font_color)else:disp_str=\"errorface\"bg_color=(255,0,0)font_color=(255,255,255)box,points=face_recognizer.map_face(box,landmarks)font_wh=image.get_string_size(disp_str)forpinpoints:img.draw_rectangle(p[0]-1,p[1]-1,p[0]+1,p[1]+1,color=bg_color)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],color=bg_color,thickness=2)img.draw_rectangle(box[0],box[1]-font_wh[1],box[0]+font_wh[0],box[1],color=bg_color,thickness=-1)img.draw_string(box[0],box[1]-font_wh[1],disp_str,color=font_color)else:#没有记录脸disp_str=\"errorface\"bg_color=(255,0,0)font_color=(255,255,255)box,points=face_recognizer.map_face(box,landmarks)font_wh=image.get_string_size(disp_str)forpinpoints:img.draw_rectangle(p[0]-1,p[1]-1,p[0]+1,p[1]+1,color=bg_color)img.draw_rectangle(box[0],box[1],box[0]+box[2],box[1]+box[3],color=bg_color,thickness=2)img.draw_rectangle(box[0],box[1]-font_wh[1],box[0]+font_wh[0],box[1],color=bg_color,thickness=-1)img.draw_string(box[0],box[1]-font_wh[1],disp_str,color=font_color)display.show(img)###删除一张人脸iflen(face_recognizer.users)>0:print(\"removeuser:\",face_recognizer.names[len(face_recognizer.users)-1])face_recognizer.users.pop()else:print(\"userempty\")###人脸保存将所添加的人脸信息保存到/root/目录下importpicklewithopen(\"/root/face_data.pickle\",'wb')asf:pickle.dump(face_recognizer.users,f)[ rpyc-kernel ]( running at Wed Jan 19 18:13:25 2022 )###加载人脸信息加载保存到/root/目录下的人脸信息importpicklewithopen(\"/root/face_data.pickle\",'rb')asf:face_recognizer.users=pickle.load(f)"}, "/soft/maixpy3/zh/usage/AI_net/Edge_detection.html": {"title": "边缘检测", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2021年12月2日|Rui|初次编写文档|----||2022年12月15日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0||2022年2月2日|DLS|修改部分代码|MaixPy3更新到0.4.6支持R329.|通过(卷积/conv)实现sobel(索贝尔)边缘检测##运行效果![](./../asserts/sobel_v831.jpg)<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=808373936&bvid=BV1F34y1q7TB&cid=487496493&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##准备-确认MaixPy3版本为0.4.3以上-使用的硬件为MaixII-Dock-内存卡内是最新版本的镜像系统-插卡启动硬件##边缘检测###部署到MaixII-Dock上-在[MaixHub](https://maix.sipeed.com/model/zoo/67)上获取模型文件和运行源码（最新0.4.7版本的系统内置了模型文件`/home/model/sobel_int8.*`）classEdge:model={\"param\":\"/home/model/sobel_int8.param\",\"bin\":\"/home/model/sobel_int8.bin\"}input_size=(224,224,3)output_size=(222,222,3)options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":input_size},\"outputs\":{\"output0\":output_size},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],}def__init__(self):frommaiximportnnprint(\"--loadmodel:\",self.model)self.model=nn.load(self.model,opt=self.options)print(\"--loadok\")def__del__(self):delself.modelprint(Edge)[ rpyc-kernel ]( running at Fri Mar 11 14:07:06 2022 )\n-- load model: {'param': '/home/model/sobel_int8.param', 'bin': '/home/model/sobel_int8.bin'}\n-- load ok###部署到MaixSense上0.4.8后系统镜像已经内置了模型文件（`/home/model/aipu_sobel.bin`）classEdge:path={\"bin\":\"/home/model/aipu_sobel.bin\"}input_size=(224,224,3)output_size=(224,224,3)options={\"model_type\":\"aipu\",\"inputs\":{\"input0\":input_size},\"outputs\":{\"output0\":output_size},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0078125,0.0078125,0.0078125],\"scale\":[0.15196067],#R329有此选项，V831没有这个选项}def__init__(self)->None:frommaiximportnnprint(\"--loadmodel:\",self.path)self.model=nn.load(self.path,opt=self.options)print(\"--loadok\")def__del__(self):delself.modelprint(Edge)###开始进行边缘检测frommaiximportcamera,displayimportnumpyasnpm=Edge()whileTrue:img=camera.capture().resize(224,224)out=m.model.forward(img,quantize=True,layout=\"hwc\")out=out.astype(np.float32).reshape(m.output_size)out=(np.ndarray.__abs__(out)*255/out.max()).astype(np.uint8)data=out.tobytes()img2=img.load(data,(222,222),mode=\"RGB\")display.show(img2)##了解更多可以查看Neutree的[博客](https://neucrack.com/p/377)，了解边缘检测开发过程"}, "/soft/maixpy3/zh/usage/AI_net/resnet.html": {"title": "1000种物品分类", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2021年12月2日|Rui|初次编写文档|----||2022年12月15日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0||2022年3月15日|Rui|修改了部分API的使用方式<br>添加了MaixSense的使用方式|一定要将MaixPy3升级到0.4.3以上|##运行效果<palign=\"center\"><iframesrc=\"//player.bilibili.com/player.html?aid=886250113&bvid=BV1ZK4y1W7DM&cid=288110985&page=1\"scrolling=\"no\"border=\"0\"frameborder=\"no\"framespacing=\"0\"allowfullscreen=\"true\"style=\"max-width:640px;max-height:480px;\"></iframe></p>##准备-确认MaixPy3版本为0.4.3以上-使用支持MaixPy3的硬件平台-确认硬件可以正常启动-确认烧录最新版本系统镜像##开始进行分类识别不同的平台使用的模型量化工具不一样，部署的方式是一样的，只是部署的参数有所不同###在MaixII-Dock上部署-20220113以后的系统镜像都内置了模型文件`/home/model/resnet18_1000_awnn.*`（可以自行[手动转换](./../../develop/resnet.md)）classResnet:m={\"param\":\"/home/model/resnet18_1000_awnn.param\",\"bin\":\"/home/model/resnet18_1000_awnn.bin\"}options={\"model_type\":\"awnn\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"output0\":(1,1,1000)},\"first_layer_conv_no_pad\":False,\"mean\":[127.5,127.5,127.5],\"norm\":[0.00784313725490196,0.00784313725490196,0.00784313725490196],}def__init__(self):frommaiximportnnself.model=nn.load(self.m,opt=self.options)def__del__(self):delself.modelprint(Resnet)[ rpyc-kernel ]( running at Wed Jan 19 16:18:45 2022 )\n<class 'rpyc.core.protocol.Resnet'>###在MaixSense上部署模型-最新的R329Armbian系统镜像中内置了模型文件`/home/model/aipu_resnet50.bin`classResnet:m={\"bin\":\"/home/model/aipu_resnet50.bin\"}options={\"model_type\":\"aipu\",\"inputs\":{\"input0\":(224,224,3)},\"outputs\":{\"output0\":(1,1,1000)},\"mean\":[127.5,127.5,127.5],\"norm\":[0.0176,0.0176,0.0176],\"scale\":[7.539542],}def__init__(self):frommaiximportnnself.model=nn.load(self.m,opt=self.options)def__del__(self):delself.modelprint(Resnet)##运行神经网络，进行分类识别importosos.chdir(\"/home/res/\")#checkoutworkdirfromclasses_labelimportlabelsfrommaiximportcamera,nn,displayresnet=Resnet()print(resnet.model)whileTrue:img=camera.capture().resize(224,224)out=resnet.model.forward(img,quantize=True)msg=\"{:.2f}:{}\".format(out.max(),labels[out.argmax()])img.draw_string(0,0,str(msg),1,(255,0,0),1)display.show(img)"}, "/soft/maixpy3/zh/usage/vision/image_example.html": {"title": "传统图像处理模块", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2022年4月13日|Ray|初次编写文档|---||2022年4月24日|Coty|添加AprilTag识别、三维坐标系以及红蓝小球追踪|---||2022年4月25日|Coty|添加AprilTag多个三维坐标显示，二维码和条形码的定位、读取信息|---||2022年4月28日|Dls|修订排版，以下功能仅在0.4.7以上版本支持|---||2022年7月04日|Dls|更新了特征检测模块（特征提取追踪）相关的内容|---|##传统视觉模块>2022年07月22日[想参与开发的点此了解【图像处理开发】传统视觉算法](../../develop/opmv_cv.md)主要分为以下几大块：-图像统计模块（获取图像信息）-颜色追踪模块（追踪颜色信息）-标记追踪模块（条码定位测距）-图像滤波模块（传统图像处理）-特征检测模块（特征提取追踪）##颜色统计模块frommaiximportimage,display,cameraROI=(80,30,15,15)whileTrue:img=camera.capture()tmp=img.get_statistics(roi=ROI)display.show(img)print(\"lMeanMedianModeSTDevMinMaxLQUQ\",tmp[0:8])print(\"aMeanMedianModeSTDevMinMaxLQUQ\",tmp[8:16])print(\"bMeanMedianModeSTDevMinMaxLQUQ\",tmp[16:24])#exampledata#openmv#\"l_mean\":42,\"l_median\":38,\"l_mode\":3,\"l_stdev\":27,\"l_min\":0,\"l_max\":100,\"l_lq\":19,\"l_uq\":65,#\"a_mean\":3,\"a_median\":1,\"a_mode\":0,\"a_stdev\":22,\"a_min\":-59,\"a_max\":81,\"a_lq\":-7,\"a_uq\":18,#\"b_mean\":5,\"b_median\":2,\"b_mode\":2,\"b_stdev\":27,\"b_min\":-88,\"b_max\":76,\"b_lq\":-12,\"b_uq\":25#maixpy3#42,37,3,27,0,98,18,66,#2,-1,-1,22,-72,83,-8,15,#7,1,-1,26,-73,76,-11,25[82, 98, 98, 19, 46, 98, 61, 98, 10, -1, -1, 15, -3, 44, -1, 20, 5, -1, -1, 9, -6, 31, -1, 13]##颜色追踪模块（2022年07月21日）以红蓝小球追踪为例识别并通过串口发送数据，基于0.5.1以后版本实现。#!/usr/bin/python3frommaiximportimage,display,camera,gpiocamera.config(size=(240,240))importserial,timeser=serial.Serial(\"/dev/ttyS1\",115200,timeout=0.2)#连接串口tmp=ser.readline()print('serialteststart...')ser.write(b\"\\r\\n\")time.sleep(1)ser.write(b\"{V831:Ready!}\\n\")set_LAB=[[(0,46,22,61,-45,74)],#red[(3,35,8,127,-128,-25)]]#blue#LAB阈值的初始化格式:[L_MIN,A_MIN,B_MIN,L_MAX,A_MAX,B_MAX]flag='1'now_time=time.time()ser.write(b\"ok\\n\")whileTrue:img=camera.capture()forjinrange(2):blobs=img.find_blobs(set_LAB[j])#在图片中查找lab阈值内的颜色色块ifblobs:foriinblobs:size=i[\"w\"]*i[\"h\"]#最大是240*240也就是57600ifsize>2000:x_start=i[\"x\"]x_end=i[\"x\"]+i[\"w\"]x_center=int((x_start+x_end)/2)#中心坐标y_start=i[\"y\"]y_end=i[\"y\"]+i[\"h\"]y_center=int((y_start+y_end)/2)m=max((x_center-i[\"w\"]*0.3),0)n=max((y_center-i[\"h\"]*0.3),0)m=min((x_center-i[\"w\"]*0.3),240)n=min((y_center-i[\"h\"]*0.3),240)mk=[int(m),int(n),20,20]if(mk[0]+20)<220and(mk[1]+20)<220:git_color=img.get_blob_color(mk,0,0)img.draw_rectangle(9,9,21,21,color=(255,255,255),thickness=1)#左上角颜色区域画出来color=(int(git_color[0]),int(git_color[1]),int(git_color[2]))img.draw_rectangle(10,10,20,20,color,thickness=-1)#将颜色填充到左上角img.draw_circle(x_center,y_center,int(i[\"h\"]/2+8),color,thickness=3)#画一个中心点在（50,50），半径为20的空心圆if(j==0):string='Red'elifj==1:string='Blue'str_size=image.get_string_size(string)img.draw_string(x_center-int(str_size[0]/2)-5,y_start-35,string,scale=1.5,color=(int(git_color[0]),int(git_color[1]),int(git_color[2])),thickness=2)ifflag=='1':flag='0'now_time=time.time()#time.asctime()display.show(img)time_t=time.time()-now_timeif(time_t>0.03):ifflag=='0':tep=\"{V831:Space_x->\"+str(x_center-120)+\"}\"#先获得数据ser.write(tep.encode(\"utf-8\"))#变成串口能发送的格式发出去flag='x'#先发X再发Y，需要等等不然32会反映不过来，但又不能阻塞elifflag=='x':flag='1'tep=\"{V831:Space_y->\"+str(y_center-120)+\"}\"#先获得数据ser.write(tep.encode(\"utf-8\"))#变成串口能发送的格式发出去**此代码配合其他MCU运行效果演示如图所示:**![演示1](../asserts/findball.gif)从演示图可以看出，MaixII-Dock成功获得了小球距离中心点的偏差，通过串口发送数据给MCU，通过PID控制舵机带动摄像头转动，使得小球重新回归中心点位。```pythonset_LAB=[[(10,35,-37,70,73,62)],#red[(10,5,-87,79,62,-28)]]#blue```对于LAB阈值的设置需要注意的是初始化的格式，**格式为:[L_MIN,A_MIN,B_MIN,L_MAX,A_MAX,B_MAX]**，此处初始化了红色和蓝色的LAB阈值。```pythonblobs=img.find_blobs(set_LAB[j])```find_blobs()函数目的是寻找对应的色块。将目标区域的信息获取之后使用**img.draw_circle()**函数将目标圈住，最后将球距离中心点的偏差值(x和y的偏差值)发送至MCU，由MCU进行后续的计算。##标记追踪模块###AprilTag标记追踪测距#!/usr/bin/python3frommaiximportdisplay,cameraf_x=(6/5.76)*240#镜头的焦距是6MM，感光cmos的长是5.76mm，240像素是屏幕的长f_y=(6/3.24)*240#镜头的焦距是6MM，感光cmos的宽是3.24mm，240像素是屏幕的宽c_x=240*0.5#屏幕分辨率的一半c_y=240*0.5#屏幕分辨率的一半whileTrue:t=camera.capture()mks=t.find_apriltags(families=16,fx=f_x,fy=f_y,cx=c_x,cy=c_y)formkinmks:x_tran=mk['x_translation']y_tran=mk['y_translation']z_tran=mk['z_translation']#家族信息fam=mk['family']#外框数据x,y,w,h,id=mk['x'],mk['y'],mk['w'],mk['h'],mk['id']#内框数据x1,y1=mk['corners'][0]#访问字典的列表x2,y2=mk['corners'][1]x3,y3=mk['corners'][2]x4,y4=mk['corners'][3]z1,z2=mk['centroid']#虚拟距离length=(x_tran*x_tran+y_tran*y_tran+z_tran*z_tran)**0.5#画外框t.draw_rectangle(x,y,x+w,y+h,color=(0,0,255),thickness=2)#打印IDt.draw_string(int(x+w*0.15),int(y+h*0.15),str(id),scale=4.0,color=(255,0,0),thickness=3)#画内框t.draw_line(x1,y1,x2,y2,color=(0,255,0),thickness=3)t.draw_line(x2,y2,x3,y3,color=(0,255,0),thickness=3)t.draw_line(x3,y3,x4,y4,color=(0,255,0),thickness=3)t.draw_line(x4,y4,x1,y1,color=(0,255,0),thickness=3)if(fam==16):t.draw_string(x,y-20,\"TAG36H11\",scale=1.0,color=(255,0,0),thickness=2)t.draw_string(x,y+h+15,str(int(length*3.0649-2))+\"cm\",scale=1.0,color=(255,0,0),thickness=2)display.show(t)**此代码运行效果演示如图所示：**![演示1](../asserts/findtag1.gif)![演示2](../asserts/findtag2.gif)在**_演示1_**中可以看到MaixII-Dock能够清楚地跟踪标签四个点的坐标并且显示出来(绿色框)，以及能够跟踪标签的最大框的坐标(蓝色框)，其实用户想要获取这些信息并不难。如果想要获取标签四个点的坐标只需要获取字典中键值为corners的列表中的值即可，而想获得外框(蓝色框)的坐标只需要获取字典中键值为x,y,w,h的的值，最后像代码中所操作的一样即可。字典的键值如下所示：'x','y','w','h','id','family','centroid','corners','x_translation','y_translation','z_translation','decision_margin','hamming','goodness','x_rotation','y_rotation','z_rotation'。在此仅介绍简单几个键值的含义。'x','y','w','h'键值返回的值分别是外框左上角x坐标和y坐标以及外框的长和宽。**特别注意的是键值'corners'返回的是一个列表，列表中的值代表着内框的四个顶点的坐标。**在**_演示2_**中可以看到两个标签距离摄像头大概是22cm左右，**那么究竟是如何测出这个距离的呢？**请接着往下看：**对于测距算法，需要知道的是：**fx，以像素为单位的相机X焦距；fy，以像素为单位的相机Y焦距；cx，图像中心image.width()/2；cy，图像中心image.height()/2；具体fx,fy,cx,cy的计算在上诉代码注释区。-**测距算法使用流程：**首先将fx，fy，cx，cy传入find_apriltags()函数内部，读取返回的x_translation，y_translation，z_translation参数。由length=(x_tran*x_tran+y_tran*y_tran+z_tran*z_tran)**0.5可以算出虚拟距离，这个虚拟距离length乘上一个比例系数K才是最终物体离CMOS的最终距离。-**比例系数K的计算：**假设我们现在将一个物体放置在离CMOS距离20cm的地方，然后直接打印出length计算的结果，此时的length假设是6.525，那么K=20/6.525=3.065。这时我们将length乘上这个比例系数K得出的结果就是真实的距离。**也就是说在第一次使用的时候需要确定比例系数K,在确定了K的大小过后，以后的使用就直接使用计算出来的比例系数K乘上计算出来的虚拟距离length，就是最后真实的距离。**（需要注意的是：最后算出的真实距离并不是标签距离摄像头的距离，而是标签距离CMOS的距离。**以MaixII-Dock为例，标配的摄像头镜头距离CMOS的距离大概是2cm，那么需要将最后算出的距离再减去2cm最后得出的结果才是标签距离摄像头的距离。**）###AprilTag获取角度信息以及三维坐标的显示#!/usr/bin/python3frommaiximportdisplay,cameraimportmathf_x=(6/5.76)*240#镜头的焦距是6MM，感光cmos的长是5.76mm，240像素是屏幕的长f_y=(6/3.24)*240#镜头的焦距是6MM，感光cmos的宽是3.24mm，240像素是屏幕的宽c_x=240*0.5#屏幕分辨率的一半c_y=240*0.5#屏幕分辨率的一半whileTrue:t=camera.capture()mks=t.find_apriltags(families=16,fx=f_x,fy=f_y,cx=c_x,cy=c_y)formkinmks:#内框数据x1,y1=mk['corners'][0]#访问字典的列表x2,y2=mk['corners'][1]x3,y3=mk['corners'][2]x4,y4=mk['corners'][3]#获取角度信息x_rol=mk['x_rotation']y_rol=mk['y_rotation']z_rol=mk['z_rotation']#画内框t.draw_line(x1,y1,x2,y2,color=(0,255,0),thickness=3)t.draw_line(x2,y2,x3,y3,color=(0,255,0),thickness=3)t.draw_line(x3,y3,x4,y4,color=(0,255,0),thickness=3)t.draw_line(x4,y4,x1,y1,color=(0,255,0),thickness=3)#显示当前角度t.draw_string(2,2,\"Xrotationis:\"+str(int(180*x_rol/3.14))+\"Angle\",scale=1.0,color=(0,0,255),thickness=1)#90°~270°正对着是180°。上下t.draw_string(2,15,\"Yrotationis:\"+str(int(180*y_rol/3.14))+\"Angle\",scale=1.0,color=(0,0,255),thickness=1)#0°~90°，270°~360°正对着是0°。左右t.draw_string(2,30,\"Zrotationis:\"+str(int(180*z_rol/3.14))+\"Angle\",scale=1.0,color=(0,0,255),thickness=1)#0°~360°正对着是0°。顺时针旋转增加#右下角画框t.draw_string(140,120,\"Space\",scale=1.0,color=(125,0,0),thickness=2)t.draw_rectangle(140,140,235,235,color=(128,128,128),thickness=2)#画出三维坐标系t.draw_line(180,200,int(180-40*math.sin(z_rol)),int(240-40*math.cos(z_rol)+40*math.cos(x_rol)),color=(255,0,0),thickness=3)t.draw_line(180,200,int(140+40*math.cos(z_rol)+40*math.cos(y_rol)),int(200-40*math.sin(z_rol)),color=(0,255,0),thickness=3)t.draw_line(180,200,int(180+40*math.sin(y_rol)),int(200-40*math.sin(x_rol)),color=(0,0,255),thickness=3)display.show(t)**此代码运行效果演示如图所示：**![三维坐标](../asserts/rotation.gif)可以看出在显示屏左上角打印出了标签的当前角度信息，那么如何获取这些信息呢？-如果想要获取标签的角度信息只需要获取字典中键值分别是x_rotation，y_rotation，z_rotation的的值即可，**需要指出的是获取的值是弧度，需要转换成角度的话需要使用弧度转角度公式**。将获取的弧度其乘上180再除以3.14即可（3.14选取了圆周率小数点后两位）。-x_rotation转为角度后取值范围是**[90°,270°]**正对着标签时显示为**180°**，在标签前后移动时变化。y_rotation转为角度后取值范围是**[0°,90°]U[270°，360°]**正对着标签时显示为**0°**，在标签左右移动时变化。z_rotation转为角度后取值范围是**[0°,360°]**正对着标签时显示为**0°**,在标签旋转时变化。**三维坐标系显示**三维坐标系生成代码如上代码中所示，将每一个轴的变化线性叠加即可获得一个不太准确的三维坐标系。-前后的变化会导致x_rotation的变化。当x_rotation变化时，X轴y的长度应该为40\\*cos(x_rol)，但是x_rotation转换为角度的范围是[90°,270°]且正对标签时为180°，画出cos图发现在定义域内180°点为极小值点，且整个取值区域内cos值都为负，我们不希望得到这样的图像。解决方法是将cos向上平移40，使得180°点cos值为0，而90°和270°为区间的最高点，是一个正数。此时终点y的结果为**200+40+40\\*cos(x_rol)**，即转至90°或270°时，X轴的长度为200，与开始点重合，实际长度为0。而对于Z轴的长度处理思路相似，但是Z轴的投影使用的是sin函数，在[90°,270°]这个区间90°是极大值点且此时sin值为正数，270°是极小值点且此时sin值为负数，正好符合实际需求，故不作处理。最后Z轴终点y的结果为**200-40\\*sin(x_rol)**。-左右的变化会导致y_rotation的变化。当y_rotation变化时，Y轴x的长度应该为40\\*cos(y_rol)，但是y_rotation转换为角度的范围是[0°,90°]U[270°，360°]，且正对标签时为0°，画出cos图发现图像被分割为了两个部分，但是不影响使用，因为在角度[270°，360°]内会随着倾角的增大而逐渐减小，所以最后终点x的结果是**180+40\\*cos(y_rol)**。而对于Z轴的长度处理思路相似，此处不做过多解释，最终Z轴终点x的结果为**180+40\\*sin(y_rol)**。-旋转的变化会导致z_rotation的变化。当z_rotation变化时，X轴x的长度应该为40\\*sin(z_rol)，但此时由于z_rotation转换而来的角度会随着逆时针的转动而增大，所以X轴x的长度应该乘以**-1**，这样才能符合实际变化，最后终点x的结果为**180-40\\*sin(z_rol)**。X轴y的长度为40\\*cos(z_rol)，但是屏幕从上而下的像素点的值是逐渐增大的，所以虽然cos值在区间内部的变化是符合我们的预期，但是依旧要乘以**-1**，最后终点y结果为**200-40\\*cos(z_rol)**。对于Y轴的x和y的长度处理思路与上诉一样，但对于x的长度需要减去一个40抵消掉初始增量，最后终点结果分别为**180-40+40\\*cos(z_rol)**和**200-40\\*sin(z_rol)**。**最后将所有计算式线性叠加即可获得最终三维坐标系。由于算式是基于正对着标签码进行计算的，所以对于非正拿标签的情况，坐标系会出现混乱**###AprilTag多个三维坐标的显示frommaiximportdisplay,cameraimportmathf_x=(6/5.76)*240f_y=(6/3.24)*240c_x=240*0.5c_y=240*0.5whileTrue:t=camera.capture()mks=t.find_apriltags(families=16,fx=f_x,fy=f_y,cx=c_x,cy=c_y)formkinmks:#内框数据x1,y1=mk['corners'][0]#访问字典的列表x2,y2=mk['corners'][1]x3,y3=mk['corners'][2]x4,y4=mk['corners'][3]x_rol=mk['x_rotation']y_rol=mk['y_rotation']z_rol=mk['z_rotation']#画内框t.draw_line(x1,y1,x2,y2,color=(0,255,0),thickness=3)t.draw_line(x2,y2,x3,y3,color=(0,255,0),thickness=3)t.draw_line(x3,y3,x4,y4,color=(0,255,0),thickness=3)t.draw_line(x4,y4,x1,y1,color=(0,255,0),thickness=3)t.draw_string(x4,y4,\"xR:\"+str(int(180*x_rol/3.14)),scale=1.0,color=(255,0,0),thickness=2)#90°~270°正对着是180°。上下t.draw_string(x4,y4+15,\"yR:\"+str(int(180*y_rol/3.14)),scale=1.0,color=(255,0,0),thickness=2)#0°~90°，270°~360°正对着是0°。左右t.draw_string(x4,y4+30,\"zR:\"+str(int(180*z_rol/3.14)),scale=1.0,color=(255,0,0),thickness=2)#0°~360°正对着是0°。顺时针旋转增加t.draw_line(x4,y4,int(x4-40*math.sin(z_rol)),int(y4+40-40*math.cos(z_rol)+40*math.cos(x_rol)),color=(255,0,0),thickness=3)t.draw_line(x4,y4,int(x4-40+40*math.cos(z_rol)+40*math.cos(y_rol)),int(y4-40*math.sin(z_rol)),color=(0,0,0),thickness=3)t.draw_line(x4,y4,int(x4+40*math.sin(y_rol)),int(y4-40*math.sin(x_rol)),color=(0,0,255),thickness=3)display.show(t)**此代码运行效果演示如图所示:**![演示1](../asserts/rotation_more.gif)在演示图中可以看到在每个标签上都打印出了一个三维坐标系，并且能够很好显示当前标签的角度信息。只需要将三维坐标系的原点定义在标签的左下角，且将三维坐标画线函数放置在for循环内部，即可出现如上图所示效果。###定位二维码及识别结果#!/usr/bin/python3frommaiximportdisplay,camerawhileTrue:t=camera.capture()mks=t.find_qrcodes()formkinmks:#外框数据X=mk['x']Y=mk['y']W=mk['w']H=mk['h']#二维码信息string=mk['payload']#内框数据x1,y1=mk['corners'][0]#访问字典的列表x2,y2=mk['corners'][1]x3,y3=mk['corners'][2]x4,y4=mk['corners'][3]#画外框t.draw_rectangle(X,Y,X+W,Y+H,color=(0,0,255),thickness=2)#打印信息t.draw_string(int(X),int(Y-35),str(string),scale=2.0,color=(255,0,0),thickness=2)#内框ID#画内框t.draw_line(x1,y1,x2,y2,color=(0,255,0),thickness=3)t.draw_line(x2,y2,x3,y3,color=(0,255,0),thickness=3)t.draw_line(x3,y3,x4,y4,color=(0,255,0),thickness=3)t.draw_line(x4,y4,x1,y1,color=(0,255,0),thickness=3)display.show(t)**此代码运行效果演示如图所示：**![演示1](../asserts/qrcode.gif)在演示中可以看到MaixII-Dock能够清楚地跟踪并且框住二维码，将二维码信息打印到屏幕上。那么如何获取二维码坐标值以及信息呢？其实非常简单。-获取外框坐标：读取**find_qrcodes()**函数返回的字典中键值为:**x,y,w,h**的值作为二维码外框的坐标信息(蓝色框)。-获取内框坐标：读取**find_qrcodes()**函数返回的字典中键值为:**corners**的列表中的值作为二维码内框的坐标(绿色框)。-获取二维码信息：读取**find_qrcodes()**函数返回的字典中键值为:**payload**的值作为二维码信息。字典的键值如下所示：'x','y','w','h','payload','version','ecc_level','mask','data_type','eci','corners'。在此仅介绍简单几个键值的含义。'x','y','w','h'键值返回的值分别是外框左上角x坐标和y坐标以及外框的长和宽。**而键值'payload'返回的值是二维码的信息，例如在二维码生成网站上输入\"Sipeed\"生成二维码，此时这个二维码的'payload'就是\"Sipeed\"。**###定位条形码及识别结果#!/usr/bin/python3frommaiximportdisplay,camerawhileTrue:t=camera.capture()mks=t.find_barcodes()formkinmks:#二维码信息string=mk['payload']TYPE=mk['type']#内框数据x1,y1=mk['corners'][0]#访问字典的列表x2,y2=mk['corners'][1]x3,y3=mk['corners'][2]x4,y4=mk['corners'][3]#画内框t.draw_line(x1,y1,x2,y2,color=(0,255,0),thickness=3)t.draw_line(x2,y2,x3,y3,color=(0,255,0),thickness=3)t.draw_line(x3,y3,x4,y4,color=(0,255,0),thickness=3)t.draw_line(x4,y4,x1,y1,color=(0,255,0),thickness=3)#打印信息t.draw_string(int(x1),int(y1-35),str(string),scale=2.0,color=(255,0,0),thickness=2)t.draw_string(int(x1),int(y1+35),str(TYPE),scale=2.0,color=(255,0,0),thickness=2)display.show(t)**此代码运行效果演示如图所示：**![演示1](../asserts/barcode.gif)在演示中可以看到MaixII-Dock能够清楚地跟踪并且框住条形码，将条形码信息以及类型打印到屏幕上。那么如何获取二维码坐标值以及相关信息呢？其实非常简单。-获取条形码坐标：读取**find_barcodes()**函数返回的字典中键值为:**corners**的列表中的值作为条形码的坐标(绿色框)。-获取条形码信息：读取**find_barcodes()**函数返回的字典中键值为:**payload**的值作为条形码信息。-获取条形码类型：读取**find_barcodes()**函数返回的字典中键值为:**type**的值作为条形码的类型**(注意：此处条形码类型选择的是CODE39，故键值为type返回的值为12，如果将条形码类型换为CODE128，则键值为type返回的值为15)**。字典的键值如下所示：'x','y','w','h','payload','rotation','type','quality','corners'。在此仅介绍简单几个键值的含义。'corners'键值返回的是一个列表，列表中的值分别条形码四个顶点的坐标。**而键值'type'返回的值是条形码的类型，'payload'返回的值是条形码的内容**。##图像滤波模块(maixpy3>=0.4.9)>为maixpy3添加opencv的函数请看这个提交[[image]Howtoadd_opencv_Canny.](https://github.com/sipeed/MaixPy3/commit/a5dd44906059e8a6204ea81c41821a411e920f16)。>为maixpy3添加openmv的函数请看这个提交[[example]usecostom_imlib_configandimageimlib_rotation_corr.](https://github.com/sipeed/MaixPy3/commit/3ac83ef6a479ceafce288d0b2a2378a92f15a286)。-lens_corr-rotation_corr-histeq-mean-Canny暂不做接口说明，但均已实现，详细请看源码。frommaiximportcamera,display,imagewhileTrue:display.show(camera.capture().Canny())##特征检测模块新添了下面两个接口，注意他们都是纯CPU运算的，没有经过AI加速。-find_template-image.cv_orb().match做个演示，如下图所示：frommaiximportimage,display,cameratmp=image.open(\"/root/template.png\")tmp=camera.capture()display.show(tmp)[ rpyc-kernel ]( running at Mon Jul  4 10:31:48 2022 )###模板匹配（find_template）采用的是ncc算法，只能匹配与模板图片大小和角度近乎一致的图案。局限性相对来说比较大，视野中的目标图案稍微比模板图片大一些或者小一些就可能匹配不成功，想要多角度多大小匹配可以尝试保存多个模板。模板匹配适应于摄像头与目标物体之间距离确定，不需要动态移动的情况，比如适应于流水线上特定物体的检测。而不适应于小车追踪一个运动的排球（因为运动的排球与摄像头的距离是动态的，摄像头看到的球大小会变化，不会与模板图片完全一样）。frommaiximportimage,display,cameratmp=image.open(\"/root/template.png\")whileTrue:img=camera.capture()res=img.find_template(tmp,0.5,step=8,search=0)iflen(res):img.draw_rectangle(res[\"x\"],res[\"y\"],res[\"x\"]+res[\"w\"],res[\"y\"]+res[\"h\"],color=(0,0,255),thickness=1)img.draw_string(res[\"x\"],res[\"y\"],str(res[\"thresh\"]))display.show(img)###orb特征提取与匹配image.cv_orb().match>ORB（OrientedFASTandRotatedBRIEF）是一种快速特征点提取和描述的算法。这个算法是由EthanRublee,VincentRabaud,KurtKonolige以及GaryR.Bradski在2011年一篇名为“ORB：AnEfficientAlternativetoSIFTorSURF”的文章中提出。ORB算法分为两部分，分别是特征点提取和特征点描述。该实现流程如下：[70b4e4e3777ed70f9b3f59724362295470d7f0e4](https://github.com/sipeed/MaixPy3/commit/70b4e4e3777ed70f9b3f59724362295470d7f0e4)提取一张图像作为目标物体特征，确定了目标特征后运行匹配函数（match）进行比较，返回识别特征点合集，如果匹配结果在预定的特征范围内，则可以认为是具备同一类特征事物的物体。感兴趣的同学可以参考该文章：[ORB特征提取算法（理论篇）](https://www.cnblogs.com/alexme/p/11345701.html)（0.5.2更新）该实现仅供学习和示意，实际表现效果帧率不错，但只适用于一些特定场景下的识别，如果想要泛化更好的效果推荐使用神经网络模型。frommaiximport*orb=image.orb()print(orb)#readtaimgsrc=Noneforiinrange(5):src=camera.capture()display.show(src)tmp=None#flagaimgimg=camera.capture()res=orb.match(src,img,limit=50)iflen(res):forpointinres['points']:#print(point)img.draw_circle(point[0],point[1],2,color=(0,255,0),thickness=1)tmp=img.copy().resize(64,64)whileTrue:img=camera.capture()res=orb.match(src,img,limit=50)#print(res['x0'],res['y0'],res['x1'],res['y1'])iflen(res):x,y=res['center']r=(res['size'][0]+res['size'][1])//4#print(x,y,res['size'],res['angle'])img.draw_circle(x,y,r,color=(255,0,0),thickness=3)forpointinres['points']:#print(point)img.draw_circle(point[0],point[1],4,color=(0,255,0),thickness=1)iflen(res['points']):img.draw_string(0,0,\"keypoints%d\"%len(res['points']))iftmp:img.draw_image(tmp)display.show(img)>以下为openmv版本实现流程，和上述是实现效果一致的，差异在匹配的后处理如何过滤一些数据，让追踪的效果更好。-find_keypoints找特征点-draw_keypoints画特征点-match_descriptor匹配的点-if(match.count()>10):满足匹配点结果-match.count(),match.theta()特征物体相对目标物体的旋转角度。主要看性能和效果了，传统视觉中orb的速度是比较快的，就算在嵌入式设备上纯CPU运算也有不错的效果。##暂未移植接口，实现已经提交。'''现已废弃，留做提醒。>2022年06月28日实验了一版opencvcv::xfeatures2d::SURF的版本后发现性能太差（2~5fps）基本没法用了，也许openmv的特征检测性能会好一些。commit28d544aa30a32ce754d7ed2ab370b2d047b98e81(HEAD->develop,sipeed/develop)Author:junhuanchen<junhuanchen@qq.com>Date:TueJun2813:48:092022+0800[cv::xfeatures2d::SURF]Giveupduetopoorperformance'''frommaiximport*test=image.cv_surf()sipeed_logo=image.open(\"/home/res/_logo.png\")b=image.open(\"/root/test_image.png\")test.match(sipeed_logo,b,dump=1)res=image.open(\"cv_surf_img_matches.jpg\")display.show(res)"}, "/soft/maixpy3/zh/usage/vision/maixpy3-example.html": {"title": "MaixPy3 基本使用示例", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|dianjixz|初次编写文档|---||2022年1月18日|dianjixz|修订文档文档|使用Jupyternotebook进行编写文档||2022年3月22日|rui|修改部分表述错误|----||2022年5月7日|dls|增加更多常用API说明|----||2022年5月11日|dls|0.4.8后image.new取代image.Image().new|支持newloadopen接口||2022年5月25日|dls|0.4.9后加入了opencv的Canny、flip，tobytes支持导出jpgbmppng等数据，修复了RGB贴图透明度混合||2022年5月31日|dls|0.5.0后找色的LAB阈值顺序发生了改变||2022年7月04日|dls|0.5.1后修复了find_blobs函数，原先采用cv实现，现改为mv实现|##\"helloworld!\"确保目标机器已经安装maixpy3软件包。使用maixpy3包在图像上显示\"helloworld!\"。frommaiximportdisplay,imagehello_img=image.new(size=(240,240),color=(255,0,0),mode=\"RGB\")#创建一张红色背景图hello_img.draw_string(30,115,\"helloworld!\",scale=1.0,color=(255,255,255),thickness=1)#在红色背景图上写下helloworlddisplay.show(hello_img)#把这张图显示出来[ rpyc-kernel ]( running at Fri Jan 14 16:48:03 2022 )##从摄像头获取图像并显示使用maixpy3包从摄像头获取图像并显示。frommaiximportcamera,display,image#引入python模块包whileTrue:img=camera.capture()#从摄像头中获取一张图像display.show(img)#将图像显示出来##在屏幕中画一条线frommaiximportimage,displayimg=image.new(size=(240,240),color=(0,0,0),mode=\"RGB\")#创建一张黑色背景图img.draw_line(0,0,100,100,color=(127,127,127),thickness=1)#画一条从（0,0）到（100,100）的白色线段display.show(img)[ rpyc-kernel ]( running at Fri Jan 14 16:47:26 2022 )##在屏幕中画一个框frommaiximportimage,displayimg=image.new(size=(240,240),color=(0,0,0),mode=\"RGB\")#创建一张黑色背景图img.draw_rectangle(10,10,60,60,color=(255,0,0),thickness=-1)#画一个从（10,10）到（60,60）的红色实心矩形img.draw_rectangle(80,160,160,200,color=(0,0,255),thickness=1)#画一个从（80,160）到（160,200）的蓝色矩形外框display.show(img)[ rpyc-kernel ]( running at Fri Jan 14 16:41:39 2022 )##在屏幕中画一个圆frommaiximportimage,displayimg=image.new(size=(240,240),color=(0,0,0),mode=\"RGB\")#创建一张黑色背景图img.draw_circle(50,50,20,color=(0,0,255),thickness=1)#画一个中心点在（50,50），半径为20的空心蓝圆img.draw_circle(150,150,20,color=(255,0,0),thickness=-1)#画一个中心点在（150,150），半径为20的实心红圆display.show(img)[ rpyc-kernel ]( running at Fri Jan 14 16:45:05 2022 )##在屏幕上显示字符显示中文需要将中文字体文件存放到开发板中，也可以直接存放到U盘中，在使用`image.load_freetype(\"path\")`加载ttf字体字库，path为字体文件的路径。frommaiximportdisplay,image#引入python模块包hello_img=image.new(size=(240,240),color=(0,0,0),mode=\"RGB\")#创建一张黑色背景图hello_img.draw_string(30,115,\"helloworld!\",scale=1.0,color=(255,255,255),thickness=1)#在黑色背景图上写下helloworlddisplay.show(hello_img)#把这张图显示出来[ rpyc-kernel ]( running at Fri Jan 14 16:49:17 2022 )##加载ttf字体与计算字符串尺寸支持中文或多国语言字体必备，使用的是python3默认的utf-8字符编码。frommaiximportimage,displayimg=image.new(color=(255,255,255))image.load_freetype(\"/home/res/sans.ttf\")s=\"二进制例程\"x,y=image.get_string_size(s,3)img.draw_string(0,120-(y+5),s,3,(255,0,0))#showleft-buttons=\"可执行文件示例\"x,y=image.get_string_size(s,2)img.draw_string(240-x,0,s,2,(0,255,0))#showright-up#waitfiximage.free_freetype()s=\"binexample\"x,y=image.get_string_size(s,1)img.draw_string(0,240-(y+5),s,1,(0,0,255))#showleft-buttondisplay.show(img)##在屏幕上画一个椭圆frommaiximportimage,displayimg=image.new(size=(240,240),color=(0,0,0),mode=\"RGB\")#创建一张黑色背景图img.draw_ellipse(120,40,20,50,90,0,360,color=(0,255,0),thickness=1)display.show(img)[ rpyc-kernel ]( running at Fri Jan 14 16:49:49 2022 )画椭圆的参数比较复杂，暂无API说明。##在一张图上截图、贴图、图像透明混合>透明图层混合目前还没能完美实现所有通道的alpha透明混合，范围在0.0,1.0之间，目前采用RGB透明混合，非RGBA融合。frommaiximportimage,displayimg=image.new(color=(255,0,0))img.draw_string(100,100,\"hello\",2,color=(0,0,255))mk=img.crop(90,90,100,50)#截一张图出来,x,y,w,himga=image.new(color=(0,255,0))imga.draw_image(mk,10,10)#imga.draw_image(imga)#pyno-allowuseselfbutlibmaixsupportimga.draw_image(mk,100,100,alpha=0.5)display.show(imga)[ rpyc-kernel ]( running at Sat May  7 11:54:46 2022 )##图像的缩放、旋转、翻转在0.4.8后才支持翻转。-resize(w,h,func)#width,height,cv::INTER_LINEAR-flip(code)#-1horizontal&vertical,1horizontal,0vertical-rotate(angle,adjust)#[-360,+360],[adjustuasge](https://blog.csdn.net/qq_40622955/article/details/119180886)frommaiximportimage,displayimg=image.new(color=(255,0,0))img.draw_string(100,100,\"KL\",2,color=(0,0,255))mk=img.crop(90,90,50,50)imga=image.new(color=(0,255,0))imga.draw_image(mk,10,10)#原图imga.draw_image(mk.rotate(-90),70,10)#顺时针90旋转imga.draw_image(mk.flip(1),130,10)#水平翻转horizontalimga.draw_image(mk.resize(80,50),10,80)#缩放到(80,50)imga.draw_image(mk.flip(0),100,80)#垂直翻转verticalimga.draw_image(mk.rotate(-135,adjust=0),10,150)#顺时针135旋转imga.draw_image(mk.rotate(+45,adjust=1),100,150)#逆时针45旋转display.show(imga)[ rpyc-kernel ]( running at Wed May 11 16:05:34 2022 )##打开图像与保存文件现在保存文件根据文件名后缀存储对应图像格式，如.jpg或.png时会编码存储图片，如果想要在cv2中使用，以下为互转的示例。```pythonfrommaiximportimage,displayimg=image.new(size=(240,240),mode=\"RGB\",color=(255,255,255))img.draw_line(0,0,240,240)img.draw_rectangle(40,120,160,200,color=(255,0,0),thickness=16)#img.draw_circle(120,120,20,color=(0,255,0))img.draw_string(40,40,\"dalaoshu\",2,color=(0,0,255))importcv2importnumpyasnpcv_img=cv2.imdecode(np.frombuffer(img.tobytes('jpg'),np.uint8),cv2.IMREAD_COLOR)cv_img=cv_img[:,:,(2,1,0)]tmp=image.load(cv_img.tobytes(),cv_img.shape)tmp.save('tmp.jpg')img_encode=cv2.imencode('.jpg',cv_img)[1]tmp=image.open(img_encode.tobytes())tmp.save('tmp.jpg')```以上操作都是在linuxdesktop分支下测试的，其他平台需要自备opencv和numpy喔。frommaiximportimage,displayimg=image.new(size=(240,240),mode=\"RGB\",color=(255,255,255))img.draw_line(0,0,240,240)img.draw_rectangle(40,120,160,200,color=(255,0,0),thickness=16)#img.draw_circle(120,120,20,color=(0,255,0))img.draw_string(40,40,\"dalaoshu\",2,color=(0,0,255))#保存图片到test.png中img.save(\"test.png\")#读取test.png中的图片tmp=image.open(\"test.png\")#0.5.0以后支持导出jpg、bmp、png、默认rgb等格式的图像数据到bytes变量中，方便向外传输图像数据。img_bytes=tmp.resize(size=(160,120)).tobytes('jpg')#>>>img_bytes=img.tobytes('jpg')#>>>img_bytes[:2]#b'\\xff\\xd8'#>>>img_bytes[-2:]#b'\\xff\\xd9'#>>>#>>>img_bytes=img.tobytes('bmp')#>>>img_bytes[:2]#b'BM'#>>>img_bytes=img.tobytes('rgb')#>>>img_bytes[:2]#b'\\x00\\x00'#>>>img_bytes=img.tobytes('png')#>>>img_bytes[:2]#b'\\x89P'#>>>#0.5.2以后image.open支持直接打开上述格式导出的bytes编码数据。tmp=image.open(img_bytes)print('tmp.png',tmp)tmp.save(\"tmp.png\")display.show(tmp)tmp.png <_maix_image.Image 0x21d3550 \" width\":160, \"height\":120, \"type\"=RGB, \"size\":57600>##【传统视觉基础】寻找色块（颜色识别）>该功能需要更新到0.5.1版本以后，填色的顺序为[(L最小值，L最大值，A最小值，A最大值，B最小值，B最大值)]。使用find_blobs的查找色块算法，查找图像中符合LAB颜色阈值的色块，可以在列表中放入多个阈值区间进行匹配，通过阈值编辑工具可将目标颜色变成白色意为提取，其他颜色全变为黑色表示忽略。如`blue=13,54,11,48,-91,-28`为什么是蓝色的lab阈值，这是因为使用LAB阈值工具可以获得两个期望颜色区间的最大和最小的LAB值，所以上图对应的下图示意图的顺序。<divalign=\"center\"><ahref=\"https://wiki.sipeed.com/threshold\"><imgsrc=\"../asserts/omv_lab.jpg\"style=\"max-width:640px;max-height:480px;\"></a></div>现已支持在线图传调试LAB阈值[en.wiki.sipeed.com/threshold](https://en.wiki.sipeed.com/threshold)，动态调试效果请看实现[https://github.com/junhuanchen/thresholding-filter-browser-html](https://github.com/junhuanchen/thresholding-filter-browser-html)）。>Lab颜色空间中的L分量用于表示像素的亮度，取值范围是[0,100],表示从纯黑到纯白；a表示从红色到绿色的范围，取值范围是[127,-128]；b表示从黄色到蓝色的范围，取值范围是[127,-128]。frommaiximportimage,display,camerablue=[(34,100,-38,3,-128,-28)]#0.5.0以后蓝色的lab阈值，0.4.9之前为[(13,11,-91,54,48,-28)]whileTrue:img=camera.capture()blobs=img.find_blobs(blue,merge=True)#在图片中查找lab阈值内的颜色色块merge合并小框。ifblobs:foriinblobs:img.draw_rectangle(i[\"x\"],i[\"y\"],i[\"x\"]+i[\"w\"],i[\"y\"]+i[\"h\"],color=(0,0,255),thickness=1)#将找到的颜色区域画出来display.show(img)##【传统视觉基础】寻线寻迹找线算法是面对小车寻线而开发的一个算法。主要的流程是，灰度化->自适应阈值处理->形态学运算->图像最小二乘法。完成算法流程后返回一条线的信息，让小车能够根据识别出来线的信息进行运动。这个接口比较适合做图像寻线。对应实现https://github.com/sipeed/MaixPy3/blob/release/ext_modules/_maix_image/_maix_vision.cpp#L513-L612frommaiximportimage,display,cameraimporttimewhileTrue:img=camera.capture()line=img.find_line()img.draw_line(line[\"rect\"][0],line[\"rect\"][1],line[\"rect\"][2],line[\"rect\"][3],color=(255,255,255),thickness=1)img.draw_line(line[\"rect\"][2],line[\"rect\"][3],line[\"rect\"][4],line[\"rect\"][5],color=(255,255,255),thickness=1)img.draw_line(line[\"rect\"][4],line[\"rect\"][5],line[\"rect\"][6],line[\"rect\"][7],color=(255,255,255),thickness=1)img.draw_line(line[\"rect\"][6],line[\"rect\"][7],line[\"rect\"][0],line[\"rect\"][1],color=(255,255,255),thickness=1)img.draw_circle(line[\"cx\"],line[\"cy\"],4,color=(255,255,255),thickness=1)display.show(img)##【传统视觉基础】获取区域的颜色颜色统计算法，统计感兴趣区域最多的颜色并返回。frommaiximportimage,display,cameraimporttimewhileTrue:img=camera.capture()colors=img.get_blob_color((100,100,10,10),0,0)img.draw_rectangle(100,100,110,110,color=(255,0,0),thickness=1)#将找到的颜色区域画出来img.draw_rectangle(9,9,21,21,color=(255,255,255),thickness=1)#将找到的颜色区域画出来img.draw_rectangle(10,10,20,20,color=(int(colors[0]),int(colors[1]),int(colors[2])),thickness=-1)#将找到的颜色区域画出来display.show(img)##【传统视觉基础】识别二维码（过时）这是标准python3库的libzbar实现，查找画面中出现的二维码，但功能比较有限制，比如不能输出定位框，现已被image实现替代，往下看传统视觉章节可知。frommaiximportcamera,display,zbarwhileTrue:img=camera.capture()result=zbar.scan_codes([\"qrcode\",\"code39\"],img)display.show(img.draw_string(10,10,str(result),2.0,(255,0,0)))"}, "/soft/maixpy3/zh/usage/vision/back-information.html": {"title": "传统视觉背景知识", "content": "|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|dianjixz|初次编写文档|---||2022年1月18日|dianjixz|修订文档文档|使用Jupyternotebook进行编写文档|##图像传感器-摄像头![image-9.png](attachment:image-9.png)[摄像头](https://baike.baidu.com/item/%E6%91%84%E5%83%8F%E5%A4%B4/321263?fr=aladdin)（CAMERA或WEBCAM）又称为电脑相机、电脑眼、电子眼等，是一种视频输入设备，被广泛的运用于视频会议、远程医疗及实时监控等方面。普通的人也可以彼此通过摄像头在网络进行有影像、有声音的交谈和沟通。另外，人们还可以将其用于当前各种流行的数码影像、影音处理等。摄像头的具体内容太多，在此就不做过多解释了。在本文档中，图像传感器就是摄像头，主要用来获取图像数据。##图像描述摄像头的成像背景请参考[摄像头成像的基本原理及结构](https://www.sohu.com/a/141710704_712214)。像素和分辨率是图像描述的基本参数。像素是图像的基本组成单位，分辨率是用来描述像素数量。![image-7.png](attachment:image-7.png)##颜色空间这里只对使用到的颜色空间做简单的介绍。更多请参考：[计算机视觉及色彩空间RGB,HSV,HLS,Lab,LMS,XYZ,CMYK](https://blog.51cto.com/u_15353042/3751269)###RGB三原色在光的世界里，白色光并不是纯净的光。牛顿用三菱镜把光分解成了七种颜色的光。而现代研究表明，光的各种颜色可以由RGB三原色组成。![image-4.png](attachment:image-4.png)###LAB颜色模型[Lab](https://baike.baidu.com/item/Lab%E9%A2%9C%E8%89%B2%E6%A8%A1%E5%9E%8B/3944053?fr=aladdin)模式是根据CommissionInternationalEclairage（CIE）在1931年所制定的一种测定颜色的国际标准建立的。于1976年被改进，并且命名的一种色彩模式。Lab颜色模型是一种设备无关的颜色模型，也是一种基于生理特征的颜色模型。Lab颜色模型由三个要素组成，一个要素是亮度（L），a和b是两个颜色通道。a包括的颜色是从深绿色（低亮度值）到灰色（中亮度值）再到亮粉红色（高亮度值）；b是从亮蓝色（低亮度值）到灰色（中亮度值）再到黄色（高亮度值）。因此，这种颜色混合后将产生具有明亮效果的色彩。对于lab颜色模型的两个优点。基于生理特征的颜色，与设备无关。这两个优点能够很方便的应用在计算机视觉模型的颜色提取上，机器视觉算法的颜色分割主要是基于该模型进行的。![image-6.png](attachment:image-6.png)详细的LAB阈值获取等待有工具后就写。##图像的坐标系统图像的坐标和现实的坐标有些不一样，相对图像来说，左上角为图像坐标的原点，横纵轴依然是X轴Y轴，不同的是，Y轴是反方向的。相应的宽就是图像X轴的长度，高是图像的Y轴的长度。在视觉工具使用中要注意图像坐标系统。![image-8.png](attachment:image-8.png)"}, "/soft/maixpy3/zh/usage/hardware/PWM.html": {"title": "Linux PWM 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|外设文档通过测试的平台有MaixII-Dock,使用的是MaixPy30.4.0|MaixPy3把Linux系统中的PWM的使用方法进行封装和简化使用，让用户使用起来更加的简单##使用方式PWM的使用方式和GPIO的类似，需要根据管脚定义图确定所使用的PWM通道序号，下面以MaixII-Dock为例子讲述如何使用MaixPy3中的PWM###准备查看开发板的管脚定义图，选择PWM输出通道序号，PWM-x是指PWM的输出通道序号<imgsrc=\"./../asserts/M2Dock_pin.jpg\"width=450px><imgsrc=\"./../asserts/R329-pin.jpg\"height=450px>这里选择使用PWM-8,假设硬件都有这个资源，如果没有，就根据实际情况修改一下。###开始先将PWM-8实例化，设置周期和占空比，最后是使能PWM，就可以输出了，具体代码看下面frommaiximportpwmimporttimetest_pwm=pwm.PWM(8)test_pwm.export()test_pwm.period=20000000#表示pwm的周期，单位nstest_pwm.duty_cycle=500000#表示占空比，单位nstest_pwm.enable=True#表示是否使能pwmfortinrange(3):foriinrange(500000,15000000,+100000):test_pwm.duty_cycle=itime.sleep(0.05)foriinrange(15000000,500000,-100000):test_pwm.duty_cycle=itime.sleep(0.05)[ rpyc-kernel ]( running at Mon Jan 17 16:58:42 2022 )别的开发板使用方式是同样的流程，包括树莓派>注明：>建议小白使用在对应开发管脚定义图上的对应PWM通道，别的通道需要自行查看[LinuxPWM](https://www.baidu.com/s?ie=UTF-8&wd=Linux%20PWM)使用方法。##PWM用途1.呼吸灯![PWM](./../asserts/pwm.gif)2.PWM可以作为电机驱动控制信号，驱动舵机。但是外接电机的时候需要做好电源隔离，不要直接将舵机接到开发板上，舵机产生的反向电流会把开发板上的芯片给击穿。![](./../asserts/PWM_1.gif)##了解更多更多关于LinuxPWM的可以查看大老鼠写的[博客](https://www.cnblogs.com/juwan/p/14343977.html)"}, "/soft/maixpy3/zh/usage/hardware/event.html": {"title": "输入事件设备", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|外设文档通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0|输入事件是Linux系统中都存在的一种特殊设备（/dev/event/input），可以通过事件来检测外接的鼠标、键盘等设备是否发生变化，如果检测键盘输入了什么进行了什么样的操作，一样可以通过输入事件来获取。##使用方法###准备接上事件设备，例如树莓派，接上键盘鼠标等事件检测设备。对于MaixII-Dock，开发板上的按键是两个事件设备![111](./../asserts/M2Dock_pin.jpg)###代码下面是在MaixII-Dock上运行的代码，用于检测两个按键的输入。如果外接别的设备，这需要修改`event.InputDevice()`中的参数。frommaiximporteventfromselectimportselectdefcheck_key():importostmp=\"/dev/input/by-path/\"ifos.path.exists(tmp):foriinos.listdir(tmp):ifi.find(\"kbd\")!=-1:returntmp+ireturn\"/dev/input/event0\"count=0dev=event.InputDevice(check_key())whileTrue:r,w,x=select([dev],[],[],0)#ifr==0orset0willread()raiseBlockingIOErrorifr:fordataindev.read():print(data)ifdata.code==0x02:print('presskeyS1')ifdata.code==0x03:print('presskeyS2')ifdata.value==1anddata.code!=0:count+=1print('presssum:',count)[ rpyc-kernel ]( running at Mon Jan 17 18:30:27 2022 )\npress key S1\npress sum: 1\npress key S1\npress key S2\npress sum: 2\npress key S2\npress key S1\npress sum: 3\npress key S1\npress key S2\npress sum: 4\npress key S2\npress key S1\npress sum: 5\npress key S1\npress key S2\npress sum: 6\npress key S2运行代码之后，按下按键则会有内容打印出来通过`/dev/input/event0`进行事件设备的选择，可以通过`os.system(\"ls/dev/input/\")`进行查看接入了多少事件设备>详细请看[python-evdev](https://python-evdev.readthedocs.io/)或了解底层linuxevdev输入子系统。"}, "/soft/maixpy3/zh/usage/hardware/I2C.html": {"title": "Linux I2C 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档|测试了MaixII-DockMaixPy3IDE0.4.0|##简介I2C是一种串行通信总线，在总线上可以挂多个主设备，多个从设备，可以和单片机、I2C传感器等进行通讯>目前MaixPy3所封装的I2C只能设置为主机模式##准备使用之前需要将硬件连接好，将SDL和SDA的两个数据线连接到开发板上的对应的SDA和SDL的管脚上。通过查看开发板上的**管脚定义图**，确定所接上的I2C总线的序号，一般是以I2C-x或者是SDL.x、SDA.x的形式标示，x为总线序号。<imgsrc=\"./../asserts/M2Dock_pin.jpg\"width=450px><imgsrc=\"./../asserts/R329-pin.jpg\"height=450px>>使用MaixSense的I2C的需要外接一个4.7k的电阻，用来过滤数据##连接**扫描设备**查看总线上的设备地址(返回十进制的数据)frommaiximporti2cprint(i2c.scan())[ rpyc-kernel ]( running at Wed Jan  5 17:22:45 2022 )\n[38]**实例化设备**`/dev/i2c-x`是指所使用的I2C总线序号frommaiximporti2ci2c=i2c.I2C('/dev/i2c-2',0x26)[ rpyc-kernel ]( running at Wed Jan  5 17:22:49 2022 )**读取设备寄存器信息**frommaiximporti2ci2c=i2c.I2C('/dev/i2c-2',0x26)print(i2c.read(0x1,1))[ rpyc-kernel ]( running at Wed Jan  5 17:24:48 2022 )\nbytearray(b'\\x13')##了解更多MaixPy3中的i2c是基于libi2c进行封装的，想要了解更多可以查看[m2docki2c的开发记录](https://wiki.sipeed.com/soft/maixpy3/zh/usage/hardware/back/I2C_1.html)，也可以查阅[libi2c](https://github.com/sipeed/MaixPy3/tree/master/ext_modules/libi2c)的实现。"}, "/soft/maixpy3/zh/usage/hardware/ADC.html": {"title": "ADC 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|外设文档通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0||2022年1月19日|dalaoshu|修订具体描述|由于ADC相对SOCLinux来说是一个特殊的功能，在MaixPy3的设计里不通用。|ADC通讯协议，目前只是针对MaixII-Dock进行开发的，其他芯片或平台需要仔细阅读数据手册来确认是否支持。##使用方法###准备查看对应开发板上的管脚定义图或者是文件，如MaixII-Dock管脚定义图所示，将ADC设备接入到GPADC0的管脚上![111](./../asserts/M2Dock_pin.jpg)###代码根据数据手册可知V831数据地址0x05070080处有一个12bit（0-4095）的adc引脚，但该引脚默认被当做adc-key使用，使得一个引脚可以支持多个按键事件。定义MaixII-DockADC模块classv83x_ADC():def__init__(self,addr=b\"0x05070080\")->None:self.addr=addrself.path=\"/sys/class/sunxi_dump/dump\"self.file=open(self.path,\"wb+\")self.last=self.value()def__del__(self):try:ifself.file:self.file.close()delself.fileexceptExceptionase:passdefvalue(self):self.file.write(b\"0x05070080\")self.file.seek(0)returnint(self.file.read()[:-1],16)v83x_ADC=v83x_ADC()[dls][ rpyc-kernel ]( running at Wed Jul 13 19:03:06 2022 )使用ADC进行是设备通讯importtimefrommaiximportdisplay,imagev831_adc0=v83x_ADCwhileTrue:time.sleep(0.1)tmp=image.Image().new((240,240),(0x2c,0x3e,0x50),\"RGB\")val=v831_adc0.value()#print(val)img=image.Image().open('/home/res/logo.png')tmp.draw_image(img,50,40,alpha=1).draw_string(20,200,\"adc0:\"+str(val),1,(0xbd,0xc3,0xc7))display.show(tmp)###运行效果**旋钮控制**![](./../asserts/adc-1.gif)**触控检测**![](./../asserts/adc-2.gif)**光照测量**![](./../asserts/adc-3.gif)##了解更多[什么是ADC](https://baike.baidu.com/item/%E6%A8%A1%E6%8B%9F%E6%95%B0%E5%AD%97%E8%BD%AC%E6%8D%A2%E5%99%A8/5382125?fr=aladdin)关于[LinuxADC](https://www.baidu.com/s?ie=utf-8&f=8&rsv_bp=1&tn=baidu&wd=linux%20ADC&oq=AD%2526lt%253B&rsv_pq=e7716f6c0000714c&rsv_t=628f6V5N5NUB2ky3bv1AhbIkN%2FFaocfP4Kb9JFMQmgvAQFoNlb%2Fv3y7fEwE&rqlang=cn&rsv_enter=1&rsv_dl=tb&rsv_sug3=9&rsv_sug1=5&rsv_sug7=100&rsv_sug2=0&rsv_btype=t&inputT=3109&rsv_sug4=3697)"}, "/soft/maixpy3/zh/usage/hardware/watchdog.html": {"title": "Watchdog 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|目前测试的是MaixII-DockMaixPy30.4.0，其他平台理论可用，但未测试。|看门狗主要用于保护系统正常运行，作用原理为，看门狗启动后，程序中必须定时执行一个喂狗的操作，当系统受到干扰不能正常运行时，喂狗操作也不能定时执行，此时看门狗将产生内部复位，使系统重新开始工作。##代码由于目前还在开发状态，以下代码为封装前代码，后续会有对应封装importarrayimportfcntlimportstructIO_WRITE=0x40000000IO_READ=0x80000000IO_READ_WRITE=0xC0000000IO_SIZE_INT=0x00040000IO_SIZE_40=0x00280000IO_TYPE_WATCHDOG=ord('W')<<8WDR_INT=IO_READ|IO_SIZE_INT|IO_TYPE_WATCHDOGWDR_40=IO_READ|IO_SIZE_40|IO_TYPE_WATCHDOGWDWR_INT=IO_READ_WRITE|IO_SIZE_INT|IO_TYPE_WATCHDOGWDIOC_GETSUPPORT=0|WDR_40WDIOC_GETSTATUS=1|WDR_INTWDIOC_GETBOOTSTATUS=2|WDR_INTWDIOC_GETTEMP=3|WDR_INTWDIOC_SETOPTIONS=4|WDWR_INTWDIOC_KEEPALIVE=5|WDR_INTWDIOC_SETTIMEOUT=6|WDWR_INTWDIOC_GETTIMEOUT=7|WDR_INTWDIOC_SETPRETIMEOUT=8|WDWR_INTWDIOC_GETPRETIMEOUT=9|WDR_INTWDIOC_GETTIMELEFT=10|WDR_INTWDIOF_OVERHEAT=0x0001WDIOF_FANFAULT=0x0002WDIOF_EXTERN1=0x0004WDIOF_EXTERN2=0x0008WDIOF_POWERUNDER=0x0010WDIOF_CARDRESET=0x0020WDIOF_POWEROVER=0x0040WDIOF_SETTIMEOUT=0x0080WDIOF_MAGICCLOSE=0x0100WDIOF_PRETIMEOUT=0x0200WDIOF_ALARMONLY=0x0400WDIOF_KEEPALIVEPING=0x8000WDIOS_DISABLECARD=0x0001WDIOS_ENABLECARD=0x0002WDIOS_TEMPPANIC=0x0004WATCHDOG_DEVICE='/dev/watchdog'WATCHDOG_STOP='V'WATCHDOG_START='S'classWatchdog:def__init__(self,stop=True):super(Watchdog,self).__init__()self.fd=open(WATCHDOG_DEVICE,'w')self.stop=stopifstop:self.Stop()def__del__(self)->None:#forv831try:self.fd.close()importosos.system(\"echoV>\"+WATCHDOG_DEVICE)exceptExceptionase:passdefStop(self):self.fd.write(WATCHDOG_STOP)defStart(self):self.stop=Falseself.fd.write(WATCHDOG_START)def_IoctlInt(self,cmd,write_value=0):buf=array.array('I',[write_value])start_temporary=self.stopifstart_temporary:self.Start()fcntl.ioctl(self.fd,cmd,buf,True)ifstart_temporary:self.Stop()returnbuf[0]defGetBootStatus(self):returnself._IoctlInt(WDIOC_GETBOOTSTATUS)defGetTemp(self):returnself._IoctlInt(WDIOC_GETTEMP)defSetOptions(self,options):returnself._IoctlInt(WDIOC_SETOPTIONS,options)defKeepAlive(self):ifself.stop:self.Start()returnself._IoctlInt(WDIOC_KEEPALIVE)defSetTimeout(self,timeout):timeout=int(timeout)iftimeout<=0:raiseValueError('timeout<=0')returnself._IoctlInt(WDIOC_SETTIMEOUT,timeout)defGetTimeout(self):returnself._IoctlInt(WDIOC_GETTIMEOUT)defSetPreTimeout(self,timeout):timeout=int(timeout)iftimeout<=0ortimeout>16:raiseValueError('0<timeout<=16')returnself._IoctlInt(WDIOC_SETPRETIMEOUT,timeout)defGetPreTimeout(self):returnself._IoctlInt(WDIOC_GETPRETIMEOUT)defGetTimeLeft(self):returnself._IoctlInt(WDIOC_GETTIMELEFT)#实例化看门狗设置喂狗时间1st=Watchdog()t.SetTimeout(1)importtime#进行喂狗操作0.5s喂一次t.KeepAlive()time.sleep(0.5)t.KeepAlive()time.sleep(0.5)t.__del__()delt#实例化看门狗设置喂狗时间2st=Watchdog()t.SetTimeout(2)time.sleep(2)delt当喂狗失败之后，系统会自动昂重启运行效果如下：![](./../asserts/Watchdog.gif)##了解更多查看[大佬鼠博客](https://www.cnblogs.com/juwan/p/14870346.html)中的关于看门狗的介绍"}, "/soft/maixpy3/zh/usage/hardware/UART.html": {"title": "Linux UART 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|外设文档通过测试的平台有MaixII-Dock,使用的是MaixPy30.4.0|##UART使用教程在Linux系统中，串口是以设备的形式存在（/dev/ttyS*），所使用的方式和原来的单片机方式有所不同。这是系统标准的UART通讯，和Linux系统中的串口操作相似。下面以MaixII-Dock为例子，来简单的简述一下如何使用UART。###准备通过查看开发板的管脚定义图，确定需要使用的UART通道。下面的代码是使用MaixII-Dock的UART-1通道>对于MaixII-Dock，不要使用UART-0通道来进行串口通讯。这个串口是直连芯片，会有一些其他数据吞吐<imgsrc=\"./../asserts/M2Dock_pin.jpg\"width=450px><imgsrc=\"./../asserts/R329-pin.jpg\"height=450px>将MaixII-Dock上UART-1TX和UART-1RX短接即可进行串口通讯测试###开始运行下列代码，即可进行串口通讯，别的开发板用法同理importserialser=serial.Serial(\"/dev/ttyS1\",115200)#连接串口print('serialteststart...')ser.write(b\"HelloWrold!!!\\n\")#输入需要通讯的内容foriinrange(3):ser.setDTR(True)ser.setRTS(True)tmp=ser.readline()print(tmp)ser.write(tmp)ser.setDTR(False)ser.setRTS(False)[ rpyc-kernel ]( running at Mon Jan 17 17:12:46 2022 )\nserial test start ...\nb'Hello Wrold !!!\\n'\nb'Hello Wrold !!!\\n'\nb'Hello Wrold !!!\\n'代码中的`/dev/ttyS1`是指串口通道1，不同的开发板，串口的表示方式不一样，请自行查看对应开发板的串口表达方式这是标准Python的串口库，更多的使用查看[Pythonserial](https://pypi.org/project/pyserial/)##UART用途这是操作系统的标准URAT，可以和单片机进行串口通讯，也可以对带有串口协议的设备、外设通讯。由于太多零基础的同学在使用，我这里就介绍一下主流的传输方式和使用方法。###传递ASCII字符串。如示例代码的`ser.write(b\"HelloWrold!!!\\n\")`意思就是传递bytes字符串，通常出现在json或python代码作为协议传输的场合，相比其他协议简单易懂，这里用下述代码为例。try:a=0b=1.2c=b\"hello\"send=b\"%d,%f,%s\\n\"%(a,b,c)#假设串口TxRx接在一起。#ser.write(send)#recv=ser.readline()#读取以\\n结尾字符串。recv=sendresult=recv.replace(b\"\\n\",b\"\").split(b',')exceptExceptionase:print(e)result=[]foriinresult:print(i)a=int(result[0])b=float(result[1])c=result[2]print(a,b,c)b'0'\nb'1.200000'\nb'hello'\n0 1.2 b'hello'###传递HexString字符串。前一种可读性好，但实际传输效率低，因为内容都是可读的ASCII字符，而实际场景下比较常用的modbusymodem等与其他芯片沟通的协议。如果是其他语言，如C语言这类底层语言，是可以通过sprintf函数转换int到char*字符串的，如`sprintf(send,\"%d,%f,%s\\n\",0,1.2,\"hello)`这和前者是一样的。但还有一种形式，如下C代码：```cstructdata{uint8_thead;uint8_tlen;uint16_tretain_0;uint32_tid;floatdecision;uint16_tretain_1;uint8_tsum;uint8_tend;}upload_data={0x55,sizeof(structdata),0,1234,5678.9,3,4,0x0A};write(self->dev_ttyS,(uint8_t*)&upload_data,upload_data.len);```那么要如何在Python中接收或发送这样的数据给C语言呢？这里我们就要使用Python的标准库`importstruct`来解决这个问题。只需要知道两个函数struct.pack()和struct.unpack()对应封包和解包。>sizeof(structdata)=1+1+2+4+4+2+1+1=16bytes详细的数据类型定义（`>BBHdfHBB`）请查阅官方文档[格式字符](https://docs.python.org/zh-cn/3/library/struct.html#format-characters)即可了解。defhex_str_to_bytes(hexString:str)->bytes:String=hexString.replace(\"-\",\"\")returnbytes.fromhex(String)defbytes_to_hex_str(String:bytes)->str:hexString=String.hex()importrereturn'-'.join(re.findall(r'.{2}',hexString))importstructupload_data=struct.pack(\">BBHdfHBB\",0x55,16,0,1234,5678.9,3,4,0x0a)print('serial.write',upload_data)print('hexstring',bytes_to_hex_str(upload_data))print('bytes',hex_str_to_bytes(bytes_to_hex_str(upload_data)))data=struct.unpack(\">BBHdfHBB\",upload_data)print('data',data)foriindata:print(type(i),i)serial.write b'U\\x10\\x00\\x00@\\x93H\\x00\\x00\\x00\\x00\\x00E\\xb1w3\\x00\\x03\\x04\\n'\nhexstring 55-10-00-00-40-93-48-00-00-00-00-00-45-b1-77-33-00-03-04-0a\nbytes b'U\\x10\\x00\\x00@\\x93H\\x00\\x00\\x00\\x00\\x00E\\xb1w3\\x00\\x03\\x04\\n'\ndata (85, 16, 0, 1234.0, 5678.89990234375, 3, 4, 10)\n<class 'int'> 85\n<class 'int'> 16\n<class 'int'> 0\n<class 'float'> 1234.0\n<class 'float'> 5678.89990234375\n<class 'int'> 3\n<class 'int'> 4\n<class 'int'> 10这里`struct.pack(\">`的>和<就引出了大小端的问题，不同芯片可能会有不同的大小端，主要影响int或float的转换方向问题，如0x09ABCDEF会被另一个芯片当成0xEFCDAB09可以在[这个大小端网站体验](https://www.toolhelper.cn/Digit/LittleBigEndianConvert)效果。例如上述案例数据大佬鼠我可能会写成以下Python接收代码：defchecksum(data):sum=0foriinrange(len(data)):sum+=data[i]#print(i,data[i],sum)returnsum&0xffimportserialimportstructser=serial.Serial(\"/dev/ttyUSB0\",115200)#连接串口'''structapriltag_data{uint8_thead;uint8_tlen;uint8_tretain_0;uint8_tretain_1;uint32_ttm;uint32_tid;floatdecision_margin;floatcenter[2];floatpoints[4][2];floatrotation[3][3];uint8_tretain_2;uint8_tretain_3;uint8_tsum;uint8_tend;}upload_data={0x55,sizeof(structapriltag_data),0,0,gs831_get_ms(),0,0,{0,0},{{0,0},{0,0},{0,0},{0,0}},{{0,0},{0,0},{0,0},{0,0},{0,0},{0,0},{0,0},{0,0},{0,0}},0,0,0,0x0A};'''data,sum=b'',0whileTrue:dat=ser.read(1)ifdat[0]==0x55:data=datdat=ser.read(1)#datlendata+=dat#print(len(data),data)dat=ser.readline()#print(data[1],len(dat),dat)ifdata[1]-2==len(dat):data+=datsum=checksum(data[:-2])#print(len(data),data)ifsum==data[-2]:res=struct.unpack(\">BBBBIIffffffffffffffffffffBBbB\",data)#print(res[4:-4])print(\"%02.03f%02.03f%02.03f%02.03f%02.03f%02.03f%02.03f%02.03f%02.03f\"%(res[17:-4]))continue和对应的C发送代码：```Cstructapriltag_data{uint8_thead;uint8_tlen;uint8_tretain_0;uint8_tretain_1;uint32_ttm;uint32_tid;floatdecision_margin;floatcenter[2];floatpoints[4][2];floatrotation[3][3];uint8_tretain_2;uint8_tretain_3;uint8_tsum;uint8_tend;}upload_data={0x55,sizeof(structapriltag_data),0,0,gs831_get_ms(),0,0,{0,0},{{0,0},{0,0},{0,0},{0,0}},{{0,0,0},{0,0,0},{0,0,0}},0,0,0,0x0A};upload_data.id=det->id;upload_data.decision_margin=det->decision_margin;upload_data.center[0]=det->c[0];upload_data.center[1]=det->c[1];upload_data.points[0][0]=det->p[0][0];upload_data.points[0][1]=det->p[0][1];upload_data.points[1][0]=det->p[1][0];upload_data.points[1][1]=det->p[1][1];upload_data.points[2][0]=det->p[2][0];upload_data.points[2][1]=det->p[2][1];upload_data.points[3][0]=det->p[3][0];upload_data.points[3][1]=det->p[3][1];upload_data.rotation[0][0]=R(0,0);upload_data.rotation[0][1]=R(0,1);upload_data.rotation[0][2]=R(0,2);upload_data.rotation[1][0]=R(1,0);upload_data.rotation[1][1]=R(1,1);upload_data.rotation[1][2]=R(1,2);upload_data.rotation[2][0]=R(2,0);upload_data.rotation[2][1]=R(2,1);upload_data.rotation[2][2]=R(2,2);uint8_t*ptr=(uint8_t*)&upload_data;for(inti=0;i<upload_data.len-2;i++)upload_data.sum+=ptr[i];write(gs831->dev_ttyS,ptr,upload_data.len);```当然方法千千万万，按照自己的实际情况和需要写就好，没有绝对正确的代码，相比之前第一种最好写，但效率不高，如果你的场景不在乎效率，那就随意一些吧。"}, "/soft/maixpy3/zh/usage/hardware/SPI.html": {"title": "Linux SPI 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|---||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|外设文档通过测试的平台有MaixII-Dock，使用的是MaixPy30.4.0|##使用教程在Linux系统中，SPI是以设备的形式存在（/dev/spidevX.X），所使用的方式和原来的单片机方式有所不同。下面以MaixII-Dock为例子，来简单的简述一下如何使用SPI。###准备通过查看开发板的管脚定义图，确定自己使用的SPI通道序号，片选序号。###开始![MaixII-Dock管脚图](./../asserts/M2Dock_pin.jpg)以MaixII-Dock为例。查看MaixII-Dock管脚图，只引出了一个SPI通道，使用的是SPI-1，片选0。使用代码为frommaiximportspispi=spi.SpiDev()spi.open(1,0)spi.bits_per_word=8spi.max_speed_hz=1spi.mode=0b11importtimeforiinrange(3):time.sleep(0.1)to_send=[0x01,0x02,0x01]print(spi.xfer2(to_send,800000))[ rpyc-kernel ]( running at Mon Jan 17 17:13:45 2022 )\n[1, 2, 1]\n[1, 2, 1]\n[1, 2, 1]通过短接SPI的MOSI和MISO进行通讯测试![](./../asserts/SPI.jpg)这里所使用的是标准Python中的spidev库，更多的使用方法可以查看[Pythonspidev](https://www.baidu.com/s?ie=utf-8&wd=Python%20spidev)##了解更多SPI通信协议的[原理](https://zhuanlan.zhihu.com/p/139903418)关于MaixII-DockSPI更多详情可以查看大佬鼠的博文[为AWV831配置spidev模块，使用py-spidev进行用户层的SPI通信。](https://www.cnblogs.com/juwan/p/14341406.html)"}, "/soft/maixpy3/zh/usage/hardware/GPIO.html": {"title": "Linux GPIO 的使用", "content": "---|更新时间|负责人|内容|备注||---|---|---|---||2022年1月4日|Rui|初次编写文档|----||2022年1月8日|Rui|修改文档的编写方式|使用Jupyternotebook进行编写文档||2022年1月18日|Rui|修改文档，增加效果图|通过测试的平台有MaixII-Dock、树莓派、maixsense，使用的是MaixPy30.4.0||2022年3月18日|Rui|增加GPIO模式设置方式，添加MaixSense|---||2022年3月25日|Rui|修改GPIO的一些使用方式和主要事项|---|##简介GPIO,全称General-PurposeInput/Output（通用输入输出），是一种软件运行期间能够动态配置和控制的通用引脚。所有的GPIO在上电后的初始状态都是输入模式，可以控制电平的输出和获取外设的电平变化。##在？点个灯？点灯就是通过控制GPIO进行高低电平的输出，使用之前需要加载对应板子的GPIO配置。通过查看开发板管脚定义图，确定所使用的IO管脚号，例如下面使用的是MaixII-Dock的GPIOPH6管脚，则需要使用gpio.gpio(6,\"H\",1)，进行定义，完整代码如下：importtimefrommaiximportgpioled=gpio.gpio(6,\"H\",1)print(led.source)whileTrue:led.set_value(0)print(led.get_value())time.sleep(0.5)led.set_value(1)print(led.get_value())time.sleep(0.5)led.release()[ rpyc-kernel ]( running at Wed Jan  5 17:30:51 2022 )\nGPIO chip 1 bank H line 6\n0\n1\n0\n1\n0\n1接上一个LED灯，运行效果如下![GPIO](./../asserts/GPIO.gif)>没有LED灯的，可以通过直接操作GPIOPH14来控制板载LED<imgsrc=\"./../asserts/M2Dock_pin.jpg\"width=450px><imgsrc=\"./../asserts/R329-pin.jpg\"height=450px>>目前MaixSense只有PH组的可以使用，后续会修复剩下的GPIO。（2022.04.11）##弄个按键将GPIO设置成输入模式，才能获取板子的电平变化。需要在GPIO实例化的时候添加多一个参数，来设置GPIO的模式。```pythonkey=gpio.gpio(6,\"H\",1,line_mode=2)```>2为设置成输入模式，该参数默认为1，即输出模式GPIO使用结束之后需要将其释放，否则会处于占用状态，通过gpio.release()释放GPIO或者使用一个类封装起来，完整代码如下：importtimeclassBUTTON:def__init__(self,line,bank,chip=1,mode=2):frommaiximportgpioself.button=gpio.gpio(line,bank,chip,mode)defis_pressed(self):ifself.button.get_value()!=1:returnTruedef__del__(self):self.button.release()key=BUTTON(6,\"H\")print(key.button.source)[ rpyc-kernel ]( running at Fri Mar 25 14:47:36 2022 )\nGPIO chip 1 bank H line 6检测按键是否按下了whileTrue:ifkey.is_pressed():print(\"pressed!!\")[ rpyc-kernel ]( running at Fri Mar 25 14:47:40 2022 )\npressed!!\npressed!!\npressed!!\npressed!!##了解更多Linux系统中，使用GPIO作为电平的输出口，我们需要关注[GPIO_line、GPIO_bank和GPIO_chip](https://www.baidu.com/s?ie=utf-8&wd=GPIO_line%20%20GPIO_bank%20GPIO_chip)，这个三个参数。以上面的例程为例子来说，使用MaixII-Dock上的GPIO6，通过查看开发板上引出口的丝印，可以知道GPIO6绑定在PH6的这个管脚上。通过查看[V831_PIN功能介绍](https://thoughts.teambition.com/share/600659e9823a3d004a4e1c7a#slate-title)得知，V831只能使用gpiochip1。这时我们知道了三个参数分别为6、H和1。这时可以使用以下的代码进行GPIO的实例化frommaiximportgpioimporttimeled=gpio.gpio(6,\"H\",1)print(led.source)foriinrange(3):led.set_value(0)print(led.get_value())time.sleep(0.5)led.set_value(1)print(led.get_value())time.sleep(0.5)led.release()[ rpyc-kernel ]( running at Wed Jan  5 17:31:00 2022 )\nGPIO chip 1 bank H line 6\n0\n1\n0\n1\n0\n1>如果gpio_chip值选择错误了，就会报`lineoffsetoutofrange`错误>更多的关于Linux_gpio的使用可以查看[大佬鼠](https://www.cnblogs.com/juwan/p/14336100.html)的博客"}}